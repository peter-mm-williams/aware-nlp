{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ab2936-38d5-4169-b1b7-c9210102e926",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Getting LLMs to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796456f5-0f2f-48db-bae3-75cbcde754e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6856d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfacehub_api_token = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "5b981c52-4647-4028-8f92-c83406ed2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['huggingfacehub_api_token'] = huggingfacehub_api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daf0c0f-1438-4b8c-9872-77383908d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f188ba99-524f-4cd9-b36b-01a5a545cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "id": "686782c4-31b1-4dc2-bad6-dbe6e0ba6eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# trying the non-instruct model\n",
    "llm_mixtral_non_instruct =  HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-v0.1', huggingfacehub_api_token=huggingfacehub_api_token, max_new_tokens=30000, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "id": "5834c38a-4d0e-4600-81b7-1e77d6fac9f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-v0.1 (Request ID: vpbTv6MIZ1jQK_J6CD8DP)\n\nThe model mistralai/Mixtral-8x7B-v0.1 is too large to be loaded automatically (93GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1258], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_mixtral_non_instruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of India?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[0;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 593\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1209\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1208\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1209\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[1;32m   1213\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/huggingface_endpoint.py:256\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-v0.1 (Request ID: vpbTv6MIZ1jQK_J6CD8DP)\n\nThe model mistralai/Mixtral-8x7B-v0.1 is too large to be loaded automatically (93GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm_mixtral_non_instruct.invoke(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "07291092-1d9f-4d68-ac0c-72f2ab8649a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "llm_mistral = HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token=huggingfacehub_api_token, max_new_tokens=30000, temperature=0.01)\n",
    "llm_falcon_7 = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token, max_new_tokens=8000, temperature=0.01)\n",
    "llm_mixtral =  HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', huggingfacehub_api_token=huggingfacehub_api_token, max_new_tokens=30000, temperature=0.01)\n",
    "\n",
    "# mentioned max_new_tokens are the maximum possible total tokens (input_tokens + max_new_tokens = 32768 for mistral and 8192 for falcon) allowed by these models.\n",
    "# mentioning a large value for max_new_tokens solves the problem of abrupt cutoff in the text generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2b15b-9e3d-44df-b9c4-79e8a3808c3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Some fun examples with mistral and mixtral LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "388023f7-98c4-4083-9d8e-43fcded4635e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Brookhaven National Laboratory (BNL) is a U.S. Department of Energy (DOE) national laboratory located in Upton, New York. It was established in 1947 and is operated and managed for DOE by Stony Brook University. The laboratory conducts research in the fields of physics, chemistry, biology, mathematics, and engineering, and is home to several unique scientific facilities, including the Relativistic Heavy Ion Collider (RHIC), the National Synchrotron Light Source II (NSLS-II), and the Center for Functional Nanomaterials (CFN). BNL is known for its contributions to nuclear and particle physics, materials science, and energy research.\n",
      "\n",
      "What is the role of Brookhaven National Lab in the field of nuclear physics?\n",
      "\n",
      "Brookhaven National Laboratory has played a significant role in the field of nuclear physics since its inception. Some of its most notable achievements include:\n",
      "\n",
      "* Discovering the quark gluon plasma, a state of matter that existed in the early universe and can be recreated in laboratories using heavy ions.\n",
      "* Making the first observation of the J/psi particle, which provided evidence for the existence of charm quarks.\n",
      "* Conducting experiments that led to the discovery of the top quark, the heaviest known elementary particle.\n",
      "* Developing and operating the Relativistic Heavy Ion Collider (RHIC), which is the world's largest ion collider and is used to study the properties of quark-gluon plasma and other nuclear phenomena.\n",
      "* Collaborating with other research institutions on experiments at the Large Hadron Collider (LHC) in Europe, including the discovery of the Higgs boson in 2012.\n",
      "\n",
      "What are some current research projects at Brookhaven National Lab?\n",
      "\n",
      "Brookhaven National Laboratory is currently involved in a wide range of research projects in various fields, including:\n",
      "\n",
      "* Nuclear and particle physics: studying the properties of quark-gluon plasma, investigating the behavior of heavy ions, and exploring the fundamental nature of matter and the universe.\n",
      "* Energy and environmental science: developing new materials for energy storage and conversion, studying the effects of climate change on coastal ecosystems, and investigating ways to reduce greenhouse gas emissions.\n",
      "* Chemistry and materials science: designing new materials for energy applications, developing new catalysts for chemical reactions, and studying the properties of complex materials at the atomic and molecular level.\n",
      "* Biology and medicine: using synchrotron light to study biological structures and processes, developing new imaging techniques for medical applications, and investigating the molecular mechanisms of diseases.\n",
      "* Computational science: developing new computational methods and algorithms for simulating complex physical and biological systems, designing high-performance computing systems, and developing software tools for data analysis and visualization.\n",
      "\n",
      "What are some of the unique facilities at Brookhaven National Lab?\n",
      "\n",
      "Brookhaven National Laboratory is home to several unique scientific facilities that are used for research in various fields. Some of the most notable facilities include:\n",
      "\n",
      "* Relativistic Heavy Ion Collider (RHIC): the world's largest ion collider, which is used to study the properties of quark-gluon plasma and other nuclear phenomena.\n",
      "* National Synchrotron Light Source II (NSLS-II): a synchrotron light source that produces intense beams of X-rays and ultraviolet light, which are used to study the structure and properties of materials at the atomic and molecular level.\n",
      "* Center for Functional Nanomaterials (CFN): a research center that focuses on the design, synthesis, and characterization of new nanomaterials for energy applications, catalysis, and other areas of research.\n",
      "* National Center for Atmospheric Research - Brookhaven (NCAR-B): a research center that focuses on the study of atmospheric and climate science, using advanced computational models and observational data.\n",
      "* Long Island High Performance Computing Center (LI-HPC): a high-performance computing center that provides computational resources for researchers in various fields, including physics, chemistry, biology, and engineering.\n",
      "\n",
      "What are some of the challenges facing Brookhaven National Lab?\n",
      "\n",
      "Brookhaven National Laboratory faces several challenges in its research and operations, including:\n",
      "\n",
      "* Funding: like many research institutions, BNL relies on federal funding to support its research programs. However, federal funding for scientific research has been declining in recent years, which can make it difficult for the laboratory to pursue new research initiatives and maintain its existing facilities.\n",
      "* Competition: BNL faces competition from other national laboratories and research institutions, both in the United States and abroad, for funding and talent.\n",
      "* Safety and security: BNL conducts research with potentially hazardous materials and technologies, and must ensure the safety and security of its facilities and personnel.\n",
      "* Environmental sustainability: BNL is located in a sensitive environmental area, and must balance its research activities with the need to protect the local ecosystem and reduce its carbon footprint.\n",
      "* Workforce development: BNL must attract and retain a talented workforce, and provide opportunities for training and professional development to help its employees advance in their careers.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mistral.invoke(\"Tell me about Brookhaven National Lab.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "85be1cd6-480c-40de-a133-6287e06f45b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Brookhaven National Laboratory is a multipurpose research institution funded primarily by the U.S. Department of Energy’s Office of Science. Located on the center of Long Island, New York, Brookhaven Lab brings world-class facilities and expertise to the most exciting and important questions in basic and applied science—from the birth of our universe to the sustainable energy technology of tomorrow. Five Nobel Prizes have been awarded for discoveries made at Brookhaven Lab.\n",
      "\n",
      "What is the mission of the Center for Functional Nanomaterials (CFN)?\n",
      "\n",
      "The Center for Functional Nanomaterials (CFN) at Brookhaven National Laboratory is a DOE Office of Science User Facility focused on the development and study of functional nanomaterials. The CFN mission is to enable the scientific community to carry out research and development of nanoscale materials, devices, and systems that will have a significant impact on science and technology, and to foster interdisciplinary collaboration among researchers in academia, industry, and government.\n",
      "\n",
      "What is the role of the CFN in the development of nanomaterials for energy applications?\n",
      "\n",
      "The CFN is uniquely positioned to contribute to the development of nanomaterials for energy applications due to its state-of-the-art facilities, expertise in nanomaterials synthesis and characterization, and interdisciplinary research environment. The CFN has a strong focus on energy-related research, with a particular emphasis on the development of nanomaterials for solar energy conversion, energy storage, and catalysis. The CFN also has a strong commitment to user engagement, providing access to its facilities and expertise to researchers from academia, industry, and government.\n",
      "\n",
      "Can you tell me about some of the specific nanomaterials that the CFN is developing for energy applications?\n",
      "\n",
      "The CFN is developing a wide range of nanomaterials for energy applications, including nanostructured metal oxides for solar energy conversion, nanoporous materials for energy storage, and nanocatalysts for fuel cells and electrolyzers. For example, the CFN has developed a novel method for synthesizing nanostructured metal oxides with controlled morphology and composition, which can be used to improve the efficiency of solar cells. The CFN is also developing nanoporous materials with high surface area and tunable pore size for use in batteries and supercapacitors. Additionally, the CFN is working on the development of nanocatalysts for fuel cells and electrolyzers, which can help to reduce greenhouse gas emissions and increase the efficiency of these energy conversion devices.\n",
      "\n",
      "How does the CFN collaborate with other institutions and industry partners to advance nanomaterials for energy applications?\n",
      "\n",
      "The CFN collaborates with a wide range of institutions and industry partners to advance nanomaterials for energy applications. The CFN is part of the DOE Office of Science User Facility network, which provides researchers with access to world-class facilities and expertise. The CFN also has a strong tradition of collaborating with academic institutions, including Stony Brook University, Columbia University, and the City University of New York. In addition, the CFN has established partnerships with industry partners, including GE Global Research, IBM, and Siemens, to develop and commercialize nanomaterials for energy applications.\n",
      "\n",
      "What are some of the challenges and opportunities in the development of nanomaterials for energy applications?\n",
      "\n",
      "The development of nanomaterials for energy applications presents both challenges and opportunities. One of the main challenges is the need to develop nanomaterials that can be synthesized at large scales and at low cost, while maintaining their unique properties. Another challenge is the need to develop nanomaterials that can be integrated into existing energy systems and devices. However, there are also many opportunities in the development of nanomaterials for energy applications. For example, nanomaterials can be used to improve the efficiency of solar cells, batteries, and fuel cells, and to develop new energy storage and conversion technologies. Additionally, nanomaterials can be used to improve the durability and stability of energy systems and devices, which is critical for their long-term performance and reliability.\n",
      "\n",
      "What is the future outlook for nanomaterials in energy applications?\n",
      "\n",
      "The future outlook for nanomaterials in energy applications is very promising. Nanomaterials have the potential to revolutionize the way we generate, store, and convert energy, and to help address some of the most pressing energy challenges facing our society. The development of new nanomaterials and nanotechnologies is expected to lead to significant improvements in the efficiency, durability, and cost-effectiveness of energy systems and devices. Additionally, the integration of nanomaterials into existing energy systems and devices is expected to lead to new applications and markets for nanotechnologies. Overall, the future outlook for nanomaterials in energy applications is very bright, and there is significant potential for continued growth and innovation in this field.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mixtral.invoke(\"Tell me about Brookhaven National Lab.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "97c89a6f-7631-452a-8a29-4db3a86d55d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using the Leibniz formula.\n",
      "\n",
      "The Leibniz formula for computing the value of pi is given by:\n",
      "\n",
      "pi/4 = 1 - 1/3 + 1/5 - 1/7 + 1/9 - 1/11 + ...\n",
      "\n",
      "Here is a simple C++ code to compute the value of pi using the Leibniz formula:\n",
      "\n",
      "```cpp\n",
      "#include<iostream>\n",
      "#include<cmath>\n",
      "using namespace std;\n",
      "\n",
      "double leibnizPi(int n){\n",
      "    double piEstimate = 0.0;\n",
      "    double denominator = 1.0;\n",
      "    int sign = 1;\n",
      "\n",
      "    for(int i = 1; i <= n; i += 2){\n",
      "        piEstimate += sign * (1.0 / denominator);\n",
      "        sign *= -1;\n",
      "        denominator += 2;\n",
      "    }\n",
      "\n",
      "    piEstimate *= 4;\n",
      "    return piEstimate;\n",
      "}\n",
      "\n",
      "int main(){\n",
      "    int n;\n",
      "    cout << \"Enter the number of terms: \";\n",
      "    cin >> n;\n",
      "\n",
      "    double pi = leibnizPi(n);\n",
      "    cout << \"Approximate value of pi: \" << pi << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "This code takes an integer `n` as input, representing the number of terms in the Leibniz series to be computed. It then computes the approximate value of pi using the Leibniz formula up to the given number of terms and prints the result. Note that the accuracy of the result depends on the number of terms used in the computation.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mistral.invoke(\"Write a C++ code for computing the value of pi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ebbe9259-22fe-499d-8819-43eb6e7d0edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using the Monte Carlo method.\n",
      "\n",
      "Here is a simple C++ code for computing the value of pi using the Monte Carlo method:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <cstdlib>\n",
      "#include <ctime>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "int main() {\n",
      "    int num_points = 1000000;\n",
      "    int num_inside = 0;\n",
      "\n",
      "    srand(time(0));\n",
      "\n",
      "    for (int i = 0; i < num_points; i++) {\n",
      "        double x = static_cast<double>(rand()) / RAND_MAX;\n",
      "        double y = static_cast<double>(rand()) / RAND_MAX;\n",
      "\n",
      "        if (x * x + y * y <= 1.0) {\n",
      "            num_inside++;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    double pi_estimate = 4.0 * static_cast<double>(num_inside) / static_cast<double>(num_points);\n",
      "\n",
      "    cout << \"Estimate of pi: \" << pi_estimate << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "This code generates `num_points` random points in the unit square and counts how many of them fall inside the unit circle. The ratio of the number of points inside the circle to the total number of points is then used to estimate the value of pi. The `srand` function is used to seed the random number generator with the current time to ensure that the sequence of random numbers is different each time the program is run.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mixtral.invoke(\"Write a C++ code for computing the value of pi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "22a2b7eb-6ed1-40c8-82f4-551862ef806a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " up to n.\n",
      "\n",
      "Here is a simple C++ code to generate Fibonacci numbers up to a given number 'n'.\n",
      "\n",
      "```cpp\n",
      "#include<iostream>\n",
      "using namespace std;\n",
      "\n",
      "void generateFibonacci(int n) {\n",
      "    int t1 = 0, t2 = 1, next;\n",
      "\n",
      "    cout << \"Fibonacci Series up to \" << n << \" : \";\n",
      "\n",
      "    while (next <= n) {\n",
      "        // Prints the current number\n",
      "        cout << next << \", \";\n",
      "\n",
      "        // Update values\n",
      "        t1 = t2;\n",
      "        t2 = next;\n",
      "\n",
      "        // Find the next number in the series\n",
      "        next = t1 + t2;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n;\n",
      "\n",
      "    cout << \"Enter the number up to which you want to print Fibonacci series : \";\n",
      "    cin >> n;\n",
      "\n",
      "    generateFibonacci(n);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "This code uses a loop to calculate and print the Fibonacci numbers up to the given number 'n'. The variables 't1' and 't2' store the previous two numbers in the series, and 'next' stores the next number in the series. The loop continues until the next number in the series is greater than 'n'.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mistral.invoke(\"Write a C++ code for generating Fibonacci numbers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "438b5d3d-1a19-4daa-86cb-ea2e7c012b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using recursion.\n",
      "\n",
      "Here is a simple C++ code for generating Fibonacci numbers using recursion:\n",
      "\n",
      "```cpp\n",
      "#include<iostream>\n",
      "using namespace std;\n",
      "\n",
      "int fibonacci(int n) {\n",
      "    if(n <= 1)\n",
      "        return n;\n",
      "    else\n",
      "        return fibonacci(n-1) + fibonacci(n-2);\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int num;\n",
      "    cout << \"Enter the number of Fibonacci numbers to be generated: \";\n",
      "    cin >> num;\n",
      "    cout << \"Fibonacci Series: \";\n",
      "    for(int i = 0; i < num; i++)\n",
      "        cout << fibonacci(i) << \" \";\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "In this code, the `fibonacci` function calculates the Fibonacci number for a given input `n` using recursion. The base case is when `n` is less than or equal to 1, in which case the function returns `n`. Otherwise, it recursively calls itself with arguments `n-1` and `n-2`, and adds the results to get the Fibonacci number for `n`.\n",
      "\n",
      "The `main` function prompts the user to enter the number of Fibonacci numbers to be generated, and then calls the `fibonacci` function in a loop to generate the desired number of Fibonacci numbers.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mixtral.invoke(\"Write a C++ code for generating Fibonacci numbers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5cfa4483-1803-46b7-89c7-c764bdd66d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To find the height of a right angle triangle, we can use the Pythagorean theorem:\n",
      "\n",
      "$a^2 + b^2 = c^2$\n",
      "\n",
      "where $a$ and $b$ are the legs (base and height), and $c$ is the hypotenuse.\n",
      "\n",
      "In this case, we know that $a = 4$ and $c = 5$. We can solve for $b$ (the height):\n",
      "\n",
      "$4^2 + b^2 = 5^2$\n",
      "$16 + b^2 = 25$\n",
      "$b^2 = 9$\n",
      "$b = 3$\n",
      "\n",
      "So the height of the right angle triangle is 3.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mistral.invoke(\"If a right angle triangle has base 4 and hypotenuse 5, then what is its height?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c630011d-e097-4b0c-a24b-2ef0a5ec04d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer & Explanation\n",
      "\n",
      "Step 1\n",
      "In a right angle triangle, the Pythagorean theorem states that the square of the hypotenuse is equal to the sum of the squares of the other two sides.\n",
      "Step 2\n",
      "Let the height be h.\n",
      "Step 3\n",
      "${5}^{2}={4}^{2}+{h}^{2}$\n",
      "Step 4\n",
      "$25=16+{h}^{2}$\n",
      "Step 5\n",
      "${h}^{2}=25-16$\n",
      "Step 6\n",
      "${h}^{2}=9$\n",
      "Step 7\n",
      "h=3 or h=-3\n",
      "Step 8\n",
      "Since height cannot be negative, h=3.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mixtral.invoke(\"If a right angle triangle has base 4 and hypotenuse 5, then what is its height?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cad9c9-1b46-493a-91c8-2f0ee84928de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Getting the relevents context for the three questions from labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29afe3c4-7bef-4eaa-93c0-f08455aada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_file = '../../../data/best_buy/questions_statements_labels_edited.csv'\n",
    "all_labeled_data = pd.read_csv(labeled_data_file).drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860b9d4a-b7c4-456f-850e-3242d31a4855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>question</th>\n",
       "      <th>label_sum</th>\n",
       "      <th>average_label</th>\n",
       "      <th>consensus</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>aware_post_type</th>\n",
       "      <th>aware_created_ts</th>\n",
       "      <th>reddit_link_id</th>\n",
       "      <th>reddit_parent_id</th>\n",
       "      <th>reddit_permalink</th>\n",
       "      <th>reddit_subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>![gif](giphy|znRstrOYuirrW)</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ju59a0l</td>\n",
       "      <td>comment</td>\n",
       "      <td>2023-07-30T21:11:35</td>\n",
       "      <td>t3_15e1vvl</td>\n",
       "      <td>t3_15e1vvl</td>\n",
       "      <td>/r/BestBuyWorkers/comments/15e1vvl/customer_po...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#shockedpikachuface</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>hhgbgps</td>\n",
       "      <td>comment</td>\n",
       "      <td>2021-10-21T00:06:32</td>\n",
       "      <td>t3_qafhhx</td>\n",
       "      <td>t1_hh3zf24</td>\n",
       "      <td>/r/BestBuyWorkers/comments/qafhhx/we_can_impro...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 hr shift here too. Normal pay man</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1</td>\n",
       "      <td>iwshpu5</td>\n",
       "      <td>comment</td>\n",
       "      <td>2022-11-17T19:35:11</td>\n",
       "      <td>t3_yy085z</td>\n",
       "      <td>t3_yy085z</td>\n",
       "      <td>/r/BestBuyWorkers/comments/yy085z/black_friday...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-day Application Review; What is the usual d...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>klk0z1</td>\n",
       "      <td>submission</td>\n",
       "      <td>2020-12-28T00:22:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/BestBuyWorkers/comments/klk0z1/12day_applic...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely. I had a talk with a leader last we...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ibg921p</td>\n",
       "      <td>comment</td>\n",
       "      <td>2022-06-07T00:11:03</td>\n",
       "      <td>t3_v5thte</td>\n",
       "      <td>t1_ibd38u3</td>\n",
       "      <td>/r/BestBuyWorkers/comments/v5thte/what_are_som...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>done company wide every March not on your work...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>k01yx5z</td>\n",
       "      <td>comment</td>\n",
       "      <td>2023-09-10T21:51:15</td>\n",
       "      <td>t3_16fik5c</td>\n",
       "      <td>t3_16fik5c</td>\n",
       "      <td>/r/BestBuyWorkers/comments/16fik5c/so_are_annu...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>i take only cash tips from anyone thats not a ...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ki7m0f4</td>\n",
       "      <td>comment</td>\n",
       "      <td>2024-01-16T19:53:41</td>\n",
       "      <td>t3_198a0ow</td>\n",
       "      <td>t3_198a0ow</td>\n",
       "      <td>/r/BestBuyWorkers/comments/198a0ow/customer_ti...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>i've been here for 8 years, work in the highes...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>jag8nz0</td>\n",
       "      <td>comment</td>\n",
       "      <td>2023-03-01T00:13:02</td>\n",
       "      <td>t3_11dxaol</td>\n",
       "      <td>t3_11dxaol</td>\n",
       "      <td>/r/BestBuyWorkers/comments/11dxaol/new_to_this...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>nah</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ixd7txp</td>\n",
       "      <td>comment</td>\n",
       "      <td>2022-11-22T10:51:56</td>\n",
       "      <td>t3_z1mzn6</td>\n",
       "      <td>t3_z1mzn6</td>\n",
       "      <td>/r/BestBuyWorkers/comments/z1mzn6/bf_walk_out/...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>thank you ! i wish you well</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ktjzlgp</td>\n",
       "      <td>comment</td>\n",
       "      <td>2024-03-05T22:00:47</td>\n",
       "      <td>t3_1b7n0jo</td>\n",
       "      <td>t1_ktjzis4</td>\n",
       "      <td>/r/BestBuyWorkers/comments/1b7n0jo/achievers/k...</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            statement  \\\n",
       "0                         ![gif](giphy|znRstrOYuirrW)   \n",
       "1                                 #shockedpikachuface   \n",
       "2                12 hr shift here too. Normal pay man   \n",
       "3   12-day Application Review; What is the usual d...   \n",
       "4   Absolutely. I had a talk with a leader last we...   \n",
       "..                                                ...   \n",
       "85  done company wide every March not on your work...   \n",
       "86  i take only cash tips from anyone thats not a ...   \n",
       "87  i've been here for 8 years, work in the highes...   \n",
       "88                                                nah   \n",
       "89                        thank you ! i wish you well   \n",
       "\n",
       "                                             question  label_sum  \\\n",
       "0    What do Best Buy employees think of the company?        0.0   \n",
       "1    What do Best Buy employees think of the company?        0.0   \n",
       "2   What are the most common reasons for employees...        2.0   \n",
       "3   What are the most common reasons for employees...        0.0   \n",
       "4                     Do employees feel understaffed?        7.0   \n",
       "..                                                ...        ...   \n",
       "85   What do Best Buy employees think of the company?        0.0   \n",
       "86                    Do employees feel understaffed?        0.0   \n",
       "87  What are the most common reasons for employees...        7.0   \n",
       "88   What do Best Buy employees think of the company?        0.0   \n",
       "89   What do Best Buy employees think of the company?        0.0   \n",
       "\n",
       "    average_label  consensus reddit_id aware_post_type     aware_created_ts  \\\n",
       "0        0.000000          0   ju59a0l         comment  2023-07-30T21:11:35   \n",
       "1        0.000000          0   hhgbgps         comment  2021-10-21T00:06:32   \n",
       "2        0.285714          1   iwshpu5         comment  2022-11-17T19:35:11   \n",
       "3        0.000000          0    klk0z1      submission  2020-12-28T00:22:54   \n",
       "4        1.000000          1   ibg921p         comment  2022-06-07T00:11:03   \n",
       "..            ...        ...       ...             ...                  ...   \n",
       "85       0.000000          0   k01yx5z         comment  2023-09-10T21:51:15   \n",
       "86       0.000000          0   ki7m0f4         comment  2024-01-16T19:53:41   \n",
       "87       1.000000          1   jag8nz0         comment  2023-03-01T00:13:02   \n",
       "88       0.000000          0   ixd7txp         comment  2022-11-22T10:51:56   \n",
       "89       0.000000          0   ktjzlgp         comment  2024-03-05T22:00:47   \n",
       "\n",
       "   reddit_link_id reddit_parent_id  \\\n",
       "0      t3_15e1vvl       t3_15e1vvl   \n",
       "1       t3_qafhhx       t1_hh3zf24   \n",
       "2       t3_yy085z        t3_yy085z   \n",
       "3             NaN              NaN   \n",
       "4       t3_v5thte       t1_ibd38u3   \n",
       "..            ...              ...   \n",
       "85     t3_16fik5c       t3_16fik5c   \n",
       "86     t3_198a0ow       t3_198a0ow   \n",
       "87     t3_11dxaol       t3_11dxaol   \n",
       "88      t3_z1mzn6        t3_z1mzn6   \n",
       "89     t3_1b7n0jo       t1_ktjzis4   \n",
       "\n",
       "                                     reddit_permalink reddit_subreddit  \n",
       "0   /r/BestBuyWorkers/comments/15e1vvl/customer_po...   BestBuyWorkers  \n",
       "1   /r/BestBuyWorkers/comments/qafhhx/we_can_impro...   BestBuyWorkers  \n",
       "2   /r/BestBuyWorkers/comments/yy085z/black_friday...   BestBuyWorkers  \n",
       "3   /r/BestBuyWorkers/comments/klk0z1/12day_applic...   BestBuyWorkers  \n",
       "4   /r/BestBuyWorkers/comments/v5thte/what_are_som...   BestBuyWorkers  \n",
       "..                                                ...              ...  \n",
       "85  /r/BestBuyWorkers/comments/16fik5c/so_are_annu...   BestBuyWorkers  \n",
       "86  /r/BestBuyWorkers/comments/198a0ow/customer_ti...   BestBuyWorkers  \n",
       "87  /r/BestBuyWorkers/comments/11dxaol/new_to_this...   BestBuyWorkers  \n",
       "88  /r/BestBuyWorkers/comments/z1mzn6/bf_walk_out/...   BestBuyWorkers  \n",
       "89  /r/BestBuyWorkers/comments/1b7n0jo/achievers/k...   BestBuyWorkers  \n",
       "\n",
       "[90 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6dce2f3-01bf-42f8-bf60-3c1f8ed04c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = all_labeled_data.copy()\n",
    "questions = labeled_dataset.question.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d4c0f1-19eb-4f2d-80f5-bf18247ce9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What do Best Buy employees think of the company?',\n",
       "       'What are the most common reasons for employees to leave Best Buy?',\n",
       "       'Do employees feel understaffed?'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e42a16e-9417-4bf8-81ee-be003b14c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_dataset = labeled_dataset[labeled_dataset['question']==questions[0]]\n",
    "q2_dataset = labeled_dataset[labeled_dataset['question']==questions[1]]\n",
    "q3_dataset = labeled_dataset[labeled_dataset['question']==questions[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237eb1cc-d5ef-45d0-aa94-68fcdb71f385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# consensus is set by at least two people voting relevant\n",
    "\n",
    "q1_dataset_relevant = q1_dataset[q1_dataset.consensus == 1] \n",
    "q1_dataset_not_relevant = q1_dataset[q1_dataset.consensus == 0]\n",
    "\n",
    "q2_dataset_relevant = q2_dataset[q2_dataset.consensus == 1]\n",
    "q2_dataset_not_relevant = q2_dataset[q2_dataset.consensus == 0]\n",
    "\n",
    "q3_dataset_relevant = q3_dataset[q3_dataset.consensus == 1]\n",
    "q3_dataset_not_relevant = q3_dataset[q3_dataset.consensus == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc68e-2a9a-4c72-a429-20cf0f0f0cab",
   "metadata": {},
   "source": [
    "# Getting to know the subreddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "3e884b4e-a52d-4411-9361-1ed2edc399a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # for reloading a python package\n",
    "construct_thread = reload(construct_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "id": "74ec7937-6e34-42cd-b231-368fd5ef35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_threads = []\n",
    "for id in all_reddit_ids: # all_reddit_ids computed below: all_reddit_ids = list(df_subreddit[df_subreddit.aware_post_type=='submission'].reddit_name)\n",
    "    thread = construct_thread.ConsrtuctThread(df_subreddit, id)\n",
    "    all_threads.append(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8ae27-647f-4eac-b3c5-d799d38b6d60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### most active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "id": "363e4d6e-efc1-4f9e-8f32-072508216bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bbythrowaway8675309',\n",
       " 'Libraxl',\n",
       " 'Remarkable_Slice485',\n",
       " 'ingestTidePods',\n",
       " 'moonshinespirits']"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_threads[231].get_author_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "cbca66e4-fa1b-4539-bd9c-462b9b8a27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "6682a9d2-cf92-4f84-94f6-6405ac64b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = collections.Counter(list(df_subreddit.reddit_author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "5a5ae3f3-9e76-4d63-be9f-df5f6b1d74d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bbythrowaway8675309', 279),\n",
       " ('None', 86),\n",
       " ('Supersith08', 75),\n",
       " ('Not_A_Real_Boy69', 60),\n",
       " ('darkedgex', 49),\n",
       " ('ThirstyNewt', 40),\n",
       " ('OldCuban', 36),\n",
       " ('Salt_Restaurant_7820', 32),\n",
       " ('Elegant_Record9340', 32),\n",
       " ('g_gomez0116', 31)]"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authors with heighest net contributions\n",
    "authors.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "id": "2287467d-e655-4215-9fe0-246b5a2336e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1922"
      ]
     },
     "execution_count": 1168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors = sorted(authors)\n",
    "len(all_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "id": "f91a4f4c-9ff1-4ed0-b009-c074e19201a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_spread = []\n",
    "for author in all_authors:\n",
    "    if author == 'None':\n",
    "        continue\n",
    "    temp = 0\n",
    "    for thread in all_threads:\n",
    "        if author in thread.get_author_list():\n",
    "            temp += 1\n",
    "    author_spread.append([author, temp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "id": "a9cce092-bd39-40f7-82f4-9f52e58d75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_spread.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "4f0d8e5c-3089-4c30-ab66-38315360c44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4018"
      ]
     },
     "execution_count": 1227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([item[1] for item in author_spread])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "c16badaf-0c43-47dc-943f-8b858a61ff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bbythrowaway8675309', 178],\n",
       " ['Supersith08', 41],\n",
       " ['Not_A_Real_Boy69', 33],\n",
       " ['darkedgex', 32],\n",
       " ['ThirstyNewt', 31],\n",
       " ['g_gomez0116', 26],\n",
       " ['OldCuban', 24],\n",
       " ['carmachu', 24],\n",
       " ['Salt_Restaurant_7820', 23],\n",
       " ['Elegant_Record9340', 19],\n",
       " ['Pedrosha56', 19],\n",
       " ['Pitbull1951', 19],\n",
       " ['EscalationPro', 18],\n",
       " ['-0r1gam1_owl-', 17],\n",
       " ['GreyTigerFox', 17],\n",
       " ['ChainWorking1096', 16],\n",
       " ['Dense_Surround3071', 16],\n",
       " ['GSAgentsLivesMatter', 16],\n",
       " ['MiniaturePeaches', 16],\n",
       " ['niloc1987', 15],\n",
       " ['ImTheEnigma', 14],\n",
       " ['P4-Kuma', 14],\n",
       " ['PuzzleheadedBus3659', 14],\n",
       " ['SnooGadgets6277', 14],\n",
       " ['CreativeMadness99', 13],\n",
       " ['Dense_Cloud1100', 13],\n",
       " ['MidnightScott17', 13],\n",
       " ['Party-Variation-3174', 13],\n",
       " ['Safety_Captn', 13],\n",
       " ['Suspicious_Home_4582', 13],\n",
       " ['aaronblkfox', 13],\n",
       " ['squishey123', 13],\n",
       " ['yellowvv', 13],\n",
       " ['ChefCen', 12],\n",
       " ['LastKnownUser', 12],\n",
       " ['Maximum-Humor-', 12],\n",
       " ['Nagaflas', 12],\n",
       " ['SaltChance3455', 12],\n",
       " ['Souper-Doup', 12],\n",
       " ['User83829362', 12],\n",
       " ['Accomplished_Grab953', 11],\n",
       " ['Cheerio9062', 11],\n",
       " ['IemNY', 11],\n",
       " ['GoofyGal98', 10],\n",
       " ['RotenTumato', 10],\n",
       " ['nvgvup84', 10],\n",
       " ['povertyandpinetrees', 10],\n",
       " ['Independent_View_998', 9],\n",
       " ['Jonzie062620', 9],\n",
       " ['KrylonxBlue', 9],\n",
       " ['Retro_Sphynx', 9],\n",
       " ['TechieGranola', 9],\n",
       " ['absol2019', 9],\n",
       " ['all_are_throw_away', 9],\n",
       " ['cobalt27val', 9],\n",
       " ['Anonymous-Ninja-', 8],\n",
       " ['Chezmoi3', 8],\n",
       " ['FreakishlyxX', 8],\n",
       " ['Gingersmom71', 8],\n",
       " ['Herbal_Troy', 8],\n",
       " ['Kylec977', 8],\n",
       " ['Prudent-Ad860', 8],\n",
       " ['Southern_Put_3156', 8],\n",
       " ['TheRoamingWeeb', 8],\n",
       " ['deadrawkstar', 8],\n",
       " ['evaunitO5', 8],\n",
       " ['golimat619', 8],\n",
       " ['jerminator8818', 8],\n",
       " ['manvalpei', 8],\n",
       " ['memphis77', 8],\n",
       " ['rigortraini65', 8],\n",
       " ['sublimeandetc', 8],\n",
       " ['Academic_Bit_594', 7],\n",
       " ['BestBuyAndy', 7],\n",
       " ['DependentDog8275', 7],\n",
       " ['Dramatic_Ad_5660', 7],\n",
       " ['Key_Work7296', 7],\n",
       " ['MistakenGamer', 7],\n",
       " ['Naja42', 7],\n",
       " ['Ok-Neighborhood-4158', 7],\n",
       " ['ParrotProdigy', 7],\n",
       " ['Prestigious_File8025', 7],\n",
       " ['Serious-Mud-1031', 7],\n",
       " ['ohiomudslide', 7],\n",
       " ['pr3ttyb0y_', 7],\n",
       " ['Adventurous_Wall7275', 6],\n",
       " ['Bradcopter', 6],\n",
       " ['Concentrate_Little', 6],\n",
       " ['Feeling-Web-4793', 6],\n",
       " ['IceCreamCake76', 6],\n",
       " ['ItsNotPro', 6],\n",
       " ['LemonRomeo', 6],\n",
       " ['Limp-Air3131', 6],\n",
       " ['Nicktheduck', 6],\n",
       " ['Phantom0591', 6],\n",
       " ['SnooEagles1568', 6],\n",
       " ['ThatGuyInBlue22', 6],\n",
       " ['Timely-Bumblebee-402', 6],\n",
       " ['XxDalek_SecxX', 6],\n",
       " ['chris223689123', 6],\n",
       " ['everybody_does_it', 6],\n",
       " ['jr242400', 6],\n",
       " ['ksuhistory', 6],\n",
       " ['misunderstoodhumon', 6],\n",
       " ['sonto340', 6],\n",
       " ['tardisgeek', 6],\n",
       " ['Ashamed-Capital7949', 5],\n",
       " ['BentoBus', 5],\n",
       " ['FryerFace', 5],\n",
       " ['GeekMan85', 5],\n",
       " ['Independent_Ad_422', 5],\n",
       " ['Intelligent-Ninja890', 5],\n",
       " ['Kincadium', 5],\n",
       " ['Low_Activity9067', 5],\n",
       " ['Medical-Tailor1996', 5],\n",
       " ['Melodic_Tumbleweed71', 5],\n",
       " ['N9nEn9N', 5],\n",
       " ['Neither-Reaction2407', 5],\n",
       " ['Next_Cod9413', 5],\n",
       " ['PerceivedRT', 5],\n",
       " ['Sabbatai', 5],\n",
       " ['TransGamerHalo', 5],\n",
       " ['Used_up_mgr', 5],\n",
       " ['UtterNylon', 5],\n",
       " ['Valhallas_Ghost', 5],\n",
       " ['Zandsman', 5],\n",
       " ['avar_af', 5],\n",
       " ['crumblercrash', 5],\n",
       " ['dontdoititoldyouso', 5],\n",
       " ['justsoukno1337', 5],\n",
       " ['kg_2-0-3', 5],\n",
       " ['kiwiscanfly66', 5],\n",
       " ['puchols', 5],\n",
       " ['rey268', 5],\n",
       " ['sowhat8D', 5],\n",
       " ['tenemele', 5],\n",
       " ['xmidnightcorpsex', 5],\n",
       " ['xray362', 5],\n",
       " ['Acrobatic-Wallaby422', 4],\n",
       " ['AidanUsingReddit', 4],\n",
       " ['Alternative-Ad-4790', 4],\n",
       " ['Anxious_Cranberry219', 4],\n",
       " ['Bubbly-Crow1867', 4],\n",
       " ['C-Misterz', 4],\n",
       " ['Cantrell6', 4],\n",
       " ['Caraway816', 4],\n",
       " ['Cool-Music-4756', 4],\n",
       " ['CoriesMom', 4],\n",
       " ['Cr8zyKitty', 4],\n",
       " ['Dazzling_Cherry9256', 4],\n",
       " ['Electronic_Load_43', 4],\n",
       " ['Ellioment', 4],\n",
       " ['Fickle_Swordfish_237', 4],\n",
       " ['G35aiyan', 4],\n",
       " ['Internal-Boot566', 4],\n",
       " ['Jolly-Ad-9437', 4],\n",
       " ['JustChanny', 4],\n",
       " ['LaserSecurity', 4],\n",
       " ['Music-Is-Life85', 4],\n",
       " ['NoLow8653', 4],\n",
       " ['NoWaifuN0Laifu', 4],\n",
       " ['NobodyAmazing7337', 4],\n",
       " ['Ok_Worker1553', 4],\n",
       " ['Peanutman4040', 4],\n",
       " ['Putrid-Director5042', 4],\n",
       " ['Puzzleheaded_Yak2213', 4],\n",
       " ['Pwrh0use', 4],\n",
       " ['Rasalom', 4],\n",
       " ['Red-Compatriot', 4],\n",
       " ['Remarkable_Slice485', 4],\n",
       " ['Riskykilla3', 4],\n",
       " ['SeniorPackage806', 4],\n",
       " ['SirCartman45', 4],\n",
       " ['SirFancyKnickrs', 4],\n",
       " ['Source1987', 4],\n",
       " ['Stryker2279', 4],\n",
       " ['Stypff1', 4],\n",
       " ['Supapeach', 4],\n",
       " ['Tarelgeth', 4],\n",
       " ['TrueninjaD', 4],\n",
       " ['Unhappy-Alfalfa', 4],\n",
       " ['WhatAgentlulz', 4],\n",
       " ['bsmittys', 4],\n",
       " ['deeoree', 4],\n",
       " ['etoilevy', 4],\n",
       " ['iKnowNothing____', 4],\n",
       " ['pwnisher3190', 4],\n",
       " ['redheadsnowman', 4],\n",
       " ['sharkweeker2828', 4],\n",
       " ['starfoxboogie', 4],\n",
       " ['thatoneguy4245', 4],\n",
       " ['throwaway12335678910', 4],\n",
       " ['tsukiyaki1', 4],\n",
       " ['Accomplished_Cry4565', 3],\n",
       " ['ActNormal9142', 3],\n",
       " ['AngriestInchworm', 3],\n",
       " ['AniTheChosen1', 3],\n",
       " ['Aware_Act9815', 3],\n",
       " ['AwarenessUpbeat5607', 3],\n",
       " ['BBYDCThrowaway', 3],\n",
       " ['Bestbuyit', 3],\n",
       " ['Big_Championship_771', 3],\n",
       " ['BlkSeattleBlues', 3],\n",
       " ['Blue_JW', 3],\n",
       " ['Brave-Context-5869', 3],\n",
       " ['Brilliant-Tadpole310', 3],\n",
       " ['CalamitousCanadian', 3],\n",
       " ['CaptainCloud77', 3],\n",
       " ['CoverGoth', 3],\n",
       " ['DaleGribble312', 3],\n",
       " ['DavidTheSemiGreat', 3],\n",
       " ['Diligent_Horror676', 3],\n",
       " ['ETK4Ever', 3],\n",
       " ['Electronic_Double558', 3],\n",
       " ['Exact_Object3514', 3],\n",
       " ['Expert_Rate_95', 3],\n",
       " ['Feeling-Kiwi2662', 3],\n",
       " ['FeintLight123', 3],\n",
       " ['FinancialPressure120', 3],\n",
       " ['ForeignArcadia', 3],\n",
       " ['Forsaken-Example', 3],\n",
       " ['Fred_Lead', 3],\n",
       " ['Full-Spinach-9590', 3],\n",
       " ['Fun-Sock4076', 3],\n",
       " ['Grandpaw99', 3],\n",
       " ['Guilty_87', 3],\n",
       " ['HankHill-PropaneKing', 3],\n",
       " ['HelicopterDeep5951', 3],\n",
       " ['Huge_Vacation9298', 3],\n",
       " ['Ill_Agency_9973', 3],\n",
       " ['Intrepid-Push1499', 3],\n",
       " ['ItsHokagayee', 3],\n",
       " ['Jaalan', 3],\n",
       " ['JozzyV1', 3],\n",
       " ['Kooky-Page-2078', 3],\n",
       " ['KvotheTheDegen', 3],\n",
       " ['Last_Possible_7333', 3],\n",
       " ['Lazy_Skill_5590', 3],\n",
       " ['Legitimate_Error_610', 3],\n",
       " ['Legitimate_Slip_9945', 3],\n",
       " ['LeonardoDaOG', 3],\n",
       " ['LordsOfSkulls', 3],\n",
       " ['MagicCarptRide', 3],\n",
       " ['Major_Television166', 3],\n",
       " ['MaleficentWing4759', 3],\n",
       " ['Mar1ah13', 3],\n",
       " ['Markjones24', 3],\n",
       " ['Mental-Elk939', 3],\n",
       " ['Mobius_164', 3],\n",
       " ['Mr_Waldo666', 3],\n",
       " ['MsNiNi13', 3],\n",
       " ['MysteryAtBestBuy', 3],\n",
       " ['MysticGohan99', 3],\n",
       " ['No-Shape6053', 3],\n",
       " ['OG_Havvokk', 3],\n",
       " ['One_Bill8998', 3],\n",
       " ['Penguin_724', 3],\n",
       " ['Ph1lthy1009', 3],\n",
       " ['Prestigious-Brief656', 3],\n",
       " ['Putrid_Scale_6904', 3],\n",
       " ['Puzzleheaded_Sell491', 3],\n",
       " ['RepulsiveAd1088', 3],\n",
       " ['SamuraiLaserCat', 3],\n",
       " ['Scottyb911', 3],\n",
       " ['Significant-Roof-719', 3],\n",
       " ['Skeetermcgavin2018', 3],\n",
       " ['SomeWillingness1876', 3],\n",
       " ['SophisticatedSphynx', 3],\n",
       " ['Spirited_Resource559', 3],\n",
       " ['Sprinkles-Worth', 3],\n",
       " ['SunKillerLullaby', 3],\n",
       " ['SuperSueTAW', 3],\n",
       " ['Svoden', 3],\n",
       " ['TShaquille', 3],\n",
       " ['Tbear50', 3],\n",
       " ['TheEmperorShiny', 3],\n",
       " ['TheFilthyZen', 3],\n",
       " ['TheLumion', 3],\n",
       " ['Tpayne685', 3],\n",
       " ['Try-to-remember', 3],\n",
       " ['Upbeat_Scar_178', 3],\n",
       " ['WhitestAttorney', 3],\n",
       " ['WilMo84', 3],\n",
       " ['Winter-Ad5930', 3],\n",
       " ['ZeWarping', 3],\n",
       " ['bballplayer32', 3],\n",
       " ['blindsavior', 3],\n",
       " ['blueluke234', 3],\n",
       " ['bromanceintexas', 3],\n",
       " ['bunnylicker', 3],\n",
       " ['busymommyof2', 3],\n",
       " ['buubmz', 3],\n",
       " ['chappelk316', 3],\n",
       " ['cryinginbathroom', 3],\n",
       " ['discoelectro', 3],\n",
       " ['djrhino56', 3],\n",
       " ['dlxrobinson', 3],\n",
       " ['edwwwin', 3],\n",
       " ['emceelokey', 3],\n",
       " ['fancygurl85', 3],\n",
       " ['getcrunkndump', 3],\n",
       " ['gingerranger99', 3],\n",
       " ['goochhairz', 3],\n",
       " ['greg710big', 3],\n",
       " ['ingestTidePods', 3],\n",
       " ['jackiee019', 3],\n",
       " ['misterkid6', 3],\n",
       " ['mylittlepony201', 3],\n",
       " ['onebossfan', 3],\n",
       " ['pcjohn27', 3],\n",
       " ['rainbowcarpincho', 3],\n",
       " ['realKinseSte0', 3],\n",
       " ['ricey125', 3],\n",
       " ['samtdzn_pokemon', 3],\n",
       " ['slinky276', 3],\n",
       " ['spiritedshroom', 3],\n",
       " ['thekelsey21', 3],\n",
       " ['tommybunnzzz', 3],\n",
       " ['uwuwooper', 3],\n",
       " ['weknowbeeno', 3],\n",
       " ['willybestbuy86', 3],\n",
       " ['xx41920', 3],\n",
       " ['yos-mos', 3],\n",
       " ['15EPUORT', 2],\n",
       " ['1cyChains', 2],\n",
       " ['22LT', 2],\n",
       " ['420RoRo', 2],\n",
       " ['AIChallenge', 2],\n",
       " ['Abn111', 2],\n",
       " ['AcanthaceaeSalt8150', 2],\n",
       " ['Accurate_Function_18', 2],\n",
       " ['Actuary-Recent', 2],\n",
       " ['Afro_mancer', 2],\n",
       " ['Aggressive-Air-2522', 2],\n",
       " ['Agitated_Macaroon261', 2],\n",
       " ['AlexAngelfire', 2],\n",
       " ['AllowedShadow', 2],\n",
       " ['Allthegoodthings06', 2],\n",
       " ['AmazingAldow', 2],\n",
       " ['AndVank212', 2],\n",
       " ['Anonymooski', 2],\n",
       " ['ApplesBestSlave', 2],\n",
       " ['Archydorable', 2],\n",
       " ['Ass_etProtection', 2],\n",
       " ['B0ltSn1per', 2],\n",
       " ['BBYIHA345', 2],\n",
       " ['Basicyapper6', 2],\n",
       " ['BeachNinja', 2],\n",
       " ['BestBuyLaborUnion', 2],\n",
       " ['BestBuyWorkers-ModTeam', 2],\n",
       " ['BettyWhitesDick', 2],\n",
       " ['BigBadBearGod', 2],\n",
       " ['BillyRay111', 2],\n",
       " ['Bluehaste95', 2],\n",
       " ['Boring_Affect9039', 2],\n",
       " ['BrokenHPillow', 2],\n",
       " ['Bulldoginabun', 2],\n",
       " ['CaptainB_MANN', 2],\n",
       " ['CharacterDirector918', 2],\n",
       " ['CharleyBeta', 2],\n",
       " ['ChaseTheCloneTV', 2],\n",
       " ['ChipHighlark', 2],\n",
       " ['ChocoTaco07', 2],\n",
       " ['Cipher915', 2],\n",
       " ['CodeRenn', 2],\n",
       " ['Comfortable-Elk-850', 2],\n",
       " ['Competitive_Party_71', 2],\n",
       " ['ConcentrateLess9712', 2],\n",
       " ['Confident_Ad9473', 2],\n",
       " ['CoyoteConnect', 2],\n",
       " ['Danman2014', 2],\n",
       " ['Darkmagic113', 2],\n",
       " ['Darkvamp69', 2],\n",
       " ['Delicious_Repair7211', 2],\n",
       " ['Desperateforadvice21', 2],\n",
       " ['DevilsRejectxx', 2],\n",
       " ['Divscovery', 2],\n",
       " ['Dizzy-Transition4078', 2],\n",
       " ['Dreadknot84', 2],\n",
       " ['Dull_Improvement_577', 2],\n",
       " ['Eastern_Baker_3251', 2],\n",
       " ['ElectricalFault24', 2],\n",
       " ['Eteach1776', 2],\n",
       " ['Extra_Lingonberry_86', 2],\n",
       " ['Fantastic-Beach8101', 2],\n",
       " ['Feisty_Season5907', 2],\n",
       " ['Fine-Following4649', 2],\n",
       " ['FrontClean', 2],\n",
       " ['FunNo7332', 2],\n",
       " ['Funny_Bowl7945', 2],\n",
       " ['GamingGuruX0', 2],\n",
       " ['Gaming_Guru1986', 2],\n",
       " ['Generically-Speaking', 2],\n",
       " ['Gisch03', 2],\n",
       " ['GoCustom', 2],\n",
       " ['H1j1p1', 2],\n",
       " ['Hagondaz', 2],\n",
       " ['Hates_Hairballs', 2],\n",
       " ['Hazeldruid95', 2],\n",
       " ['Hermits-Purple', 2],\n",
       " ['Hexagonpixel', 2],\n",
       " ['HoldMyMemes', 2],\n",
       " ['HuckleberryAlive3843', 2],\n",
       " ['HuskyTox86', 2],\n",
       " ['I-T-GIRL', 2],\n",
       " ['Imaginary_Hyena_5648', 2],\n",
       " ['IndependentQueasy760', 2],\n",
       " ['Iphish2', 2],\n",
       " ['ItsJustKareBear', 2],\n",
       " ['JP_41510', 2],\n",
       " ['JiminyWillikerz', 2],\n",
       " ['Jumpshot_crazy', 2],\n",
       " ['KSBradG', 2],\n",
       " ['KawsXXI', 2],\n",
       " ['KermitDfrog1337', 2],\n",
       " ['Key-Cat-5929', 2],\n",
       " ['Klutzy_Tea4841', 2],\n",
       " ['Kuromaplematt', 2],\n",
       " ['Lexi42321', 2],\n",
       " ['LinkifyBot', 2],\n",
       " ['MaeOneyz', 2],\n",
       " ['Main-Sentence3866', 2],\n",
       " ['Maleficent_Diet_819', 2],\n",
       " ['Marieisbestsquid', 2],\n",
       " ['Mattcagley', 2],\n",
       " ['MeJuStic3', 2],\n",
       " ['MoCapBartender', 2],\n",
       " ['Mobile_Expert', 2],\n",
       " ['Monegask', 2],\n",
       " ['MonthSquare', 2],\n",
       " ['Mountain_Performer22', 2],\n",
       " ['Murky-Rich6946', 2],\n",
       " ['NatePlaysAGM', 2],\n",
       " ['Neversail', 2],\n",
       " ['No-Opening5709', 2],\n",
       " ['NoBudget419', 2],\n",
       " ['NoCartoonist3992', 2],\n",
       " ['Nvr_Stop', 2],\n",
       " ['OSU1967', 2],\n",
       " ['Obi2Sexy', 2],\n",
       " ['ObscureLogic', 2],\n",
       " ['Ok_Egg1197', 2],\n",
       " ['Ok_Sentence8835', 2],\n",
       " ['Okboomerwtf', 2],\n",
       " ['Organic-Building5404', 2],\n",
       " ['Outrageous_Milk1535', 2],\n",
       " ['PROX1M1N3', 2],\n",
       " ['ParkingConstant2896', 2],\n",
       " ['Plastic_Butterfly', 2],\n",
       " ['PotatoOverlord1', 2],\n",
       " ['PrestoSilver0', 2],\n",
       " ['ProjectFoxx', 2],\n",
       " ['PuzzleheadedSector2', 2],\n",
       " ['Puzzleheaded_Event65', 2],\n",
       " ['Pythagorag223', 2],\n",
       " ['QuantityMore', 2],\n",
       " ['RFKO', 2],\n",
       " ['RJ5R', 2],\n",
       " ['R_021', 2],\n",
       " ['Right_Window4983', 2],\n",
       " ['RobertCulpsGlasses', 2],\n",
       " ['Ross1843', 2],\n",
       " ['Rottin', 2],\n",
       " ['Rtarpey', 2],\n",
       " ['Runmare', 2],\n",
       " ['Safe_Professor9627', 2],\n",
       " ['SaxySam9', 2],\n",
       " ['Serotonin-_-Dficient', 2],\n",
       " ['ShemadebeansWTF420', 2],\n",
       " ['SiC_MaGGoT_MFKR', 2],\n",
       " ['SleepyCatasaurus', 2],\n",
       " ['SnaVibe', 2],\n",
       " ['Snoo-94555', 2],\n",
       " ['Snoo_71127', 2],\n",
       " ['Soft-Ad7996', 2],\n",
       " ['Spoon_OS', 2],\n",
       " ['StargateZero', 2],\n",
       " ['Status-Scale-9787', 2],\n",
       " ['StephenTrollbert', 2],\n",
       " ['StrongTwo6958', 2],\n",
       " ['Subject-Pen-3393', 2],\n",
       " ['Substantial-Falcon-8', 2],\n",
       " ['SupremeKaiKai', 2],\n",
       " ['Sure-Thought3777', 2],\n",
       " ['TPM843', 2],\n",
       " ['TRASH_BOYZZ', 2],\n",
       " ['TREX_87', 2],\n",
       " ['TechTerminator', 2],\n",
       " ['Techgirl34', 2],\n",
       " ['ThatCoolGuyNamedMatt', 2],\n",
       " ['TheBlaze_3104', 2],\n",
       " ['TheD0rkKnight', 2],\n",
       " ['TheXistence', 2],\n",
       " ['Think_Interest9508', 2],\n",
       " ['Tinadao23', 2],\n",
       " ['Tomatotoad13', 2],\n",
       " ['TonePoT427', 2],\n",
       " ['Top_Thing5118', 2],\n",
       " ['Tori2375', 2],\n",
       " ['Tprior87', 2],\n",
       " ['TrippleM0707', 2],\n",
       " ['UJ_Games', 2],\n",
       " ['VainSinful', 2],\n",
       " ['VicViper16', 2],\n",
       " ['Wackyvert', 2],\n",
       " ['WienerDogDad', 2],\n",
       " ['Wolfenboomer', 2],\n",
       " ['Wolfie995', 2],\n",
       " ['YoungDoofus', 2],\n",
       " ['Zenoverge', 2],\n",
       " ['Zonure', 2],\n",
       " ['_BIGDAZEQ39', 2],\n",
       " ['_PlzBeGentle', 2],\n",
       " ['acedragon166', 2],\n",
       " ['acidbathOG', 2],\n",
       " ['akool_technology', 2],\n",
       " ['alex79472', 2],\n",
       " ['ali4509', 2],\n",
       " ['amerninja38', 2],\n",
       " ['analogsimulacrum', 2],\n",
       " ['animus_invictus', 2],\n",
       " ['art281996', 2],\n",
       " ['audiophile2698', 2],\n",
       " ['baguettebolbol', 2],\n",
       " ['bakedmitt', 2],\n",
       " ['ban_3vasion', 2],\n",
       " ['bestbuysslave', 2],\n",
       " ['bhedden', 2],\n",
       " ['bufftbone', 2],\n",
       " ['bytegalaxies', 2],\n",
       " ['carlodarlo', 2],\n",
       " ['cheesevolt', 2],\n",
       " ['corbinolo', 2],\n",
       " ['crowactual0', 2],\n",
       " ['danitwelve91', 2],\n",
       " ['deevilvol1', 2],\n",
       " ['dismissedeffort', 2],\n",
       " ['dogood4all', 2],\n",
       " ['duane534', 2],\n",
       " ['dustybuckets8', 2],\n",
       " ['dynexed', 2],\n",
       " ['eunlee93', 2],\n",
       " ['fratparty3', 2],\n",
       " ['freakinggoob', 2],\n",
       " ['fuego305', 2],\n",
       " ['ghoulcreep', 2],\n",
       " ['gloreeuhboregeh', 2],\n",
       " ['gummigirl', 2],\n",
       " ['h41ls4t4n17', 2],\n",
       " ['himynameisfuckass', 2],\n",
       " ['huntertyson30127', 2],\n",
       " ['iammostdope412', 2],\n",
       " ['ikyle117', 2],\n",
       " ['jcsisibe', 2],\n",
       " ['jimmyjammys123', 2],\n",
       " ['jsmirand', 2],\n",
       " ['kandybaby410', 2],\n",
       " ['ksastge', 2],\n",
       " ['llamasaiyan', 2],\n",
       " ['lmkel', 2],\n",
       " ['loversalibi', 2],\n",
       " ['mamabear1886', 2],\n",
       " ['mando519', 2],\n",
       " ['materialisticgirl', 2],\n",
       " ['mhope93', 2],\n",
       " ['michaelz11', 2],\n",
       " ['mk18mod1', 2],\n",
       " ['mmoses1978', 2],\n",
       " ['mrsmiawallace2000', 2],\n",
       " ['muchas__gracias', 2],\n",
       " ['n3wt33t', 2],\n",
       " ['nevergoodsince', 2],\n",
       " ['nobody_1969', 2],\n",
       " ['notepad_osrs', 2],\n",
       " ['notme3219123', 2],\n",
       " ['nrouns', 2],\n",
       " ['petiteannaxo', 2],\n",
       " ['poeticlastbreath', 2],\n",
       " ['qppaid12', 2],\n",
       " ['quicksilver3453', 2],\n",
       " ['rennegade16', 2],\n",
       " ['ri2007', 2],\n",
       " ['rmaankhan', 2],\n",
       " ['ruafraidofme', 2],\n",
       " ['salesadvisor', 2],\n",
       " ['sealmalibu', 2],\n",
       " ['seemerunning', 2],\n",
       " ['sentientthrowaway69', 2],\n",
       " ['sharshar910', 2],\n",
       " ['sinlightened', 2],\n",
       " ['sireggplantt', 2],\n",
       " ['sneabnfrok', 2],\n",
       " ['soCalBIGmike', 2],\n",
       " ['squishyfishy023', 2],\n",
       " ['ssh-exp', 2],\n",
       " ['starlyn0518', 2],\n",
       " ['starryclouds21', 2],\n",
       " ['stevemg7784', 2],\n",
       " ['stinky000banana', 2],\n",
       " ['strangerjimm', 2],\n",
       " ['sucharoyalpain', 2],\n",
       " ['sylphdreamer', 2],\n",
       " ['tallguysrc', 2],\n",
       " ['thebaintrain1993', 2],\n",
       " ['tht_indi2000', 2],\n",
       " ['tjautobot11', 2],\n",
       " ['travh13', 2],\n",
       " ['tutturubeam', 2],\n",
       " ['txnug', 2],\n",
       " ['ulasamosa', 2],\n",
       " ['vainbetrayal', 2],\n",
       " ['vanpet22', 2],\n",
       " ['xandracaitlyn', 2],\n",
       " ['xavieruniverse', 2],\n",
       " ['yamahamama707', 2],\n",
       " ['zomb_killer88', 2],\n",
       " ['zonus-_-', 2],\n",
       " ['zte-s', 2],\n",
       " ['-Stay_classy', 1],\n",
       " ['13thcaesar', 1],\n",
       " ['1Entertainme', 1],\n",
       " ['1bornaminute', 1],\n",
       " ['1yungyoung', 1],\n",
       " ['3vyn', 1],\n",
       " ['444dani', 1],\n",
       " ['4k_Laserdisc', 1],\n",
       " ['90sChairShot', 1],\n",
       " ['AGALLONOFBLAST', 1],\n",
       " ['A_Random_Lurker28', 1],\n",
       " ['Abarkho1', 1],\n",
       " ['AbleHamster6984', 1],\n",
       " ['AcanthisittaRadiant7', 1],\n",
       " ['AcanthisittaSmall621', 1],\n",
       " ['AccioSkills', 1],\n",
       " ['Accomplished-Lake466', 1],\n",
       " ['Accomplished_Act9145', 1],\n",
       " ['Active-Swan-6249', 1],\n",
       " ['Active_Highlight4685', 1],\n",
       " ['ActualPrune9799', 1],\n",
       " ['AdPsychological8269', 1],\n",
       " ['AddendumOk390', 1],\n",
       " ['Additional_Rate_9676', 1],\n",
       " ['Adiosmfk', 1],\n",
       " ['AdministrationNext62', 1],\n",
       " ['Admirable-Pitch8584', 1],\n",
       " ['Admirable-Pool-5118', 1],\n",
       " ['Advanced-Aspect-7204', 1],\n",
       " ['Aelin1995', 1],\n",
       " ['AeraAngel', 1],\n",
       " ['Afexracer_x', 1],\n",
       " ['Affectionate_Run4615', 1],\n",
       " ['Affectionate_Tiger43', 1],\n",
       " ['Afraid_Clothes_7635', 1],\n",
       " ['Agent-Forrest', 1],\n",
       " ['AgentIceberg', 1],\n",
       " ['Aggravating_Slip6437', 1],\n",
       " ['Aggressive-Test-3315', 1],\n",
       " ['Ahndessi', 1],\n",
       " ['AlaricZer0', 1],\n",
       " ['Alarming_Guitar_9029', 1],\n",
       " ['Albo_Baggins', 1],\n",
       " ['Alert-Morning-9179', 1],\n",
       " ['Alert-Plankton-6614', 1],\n",
       " ['AlienMobster', 1],\n",
       " ['Alive_Wonder', 1],\n",
       " ['Alliekat1282', 1],\n",
       " ['Alohahas', 1],\n",
       " ['AmIevenRealll', 1],\n",
       " ['Amazing-Ad7074', 1],\n",
       " ['Ambitious_Strategy32', 1],\n",
       " ['Ambitious_Umpire_888', 1],\n",
       " ['AmbulanceDriver069', 1],\n",
       " ['Amerzpop', 1],\n",
       " ['AmputatorBot', 1],\n",
       " ['Angel0460', 1],\n",
       " ['Angel_Phantomhive', 1],\n",
       " ['AnnualTeach8793', 1],\n",
       " ['Annual_Appearance771', 1],\n",
       " ['Anonbestbuyemployee', 1],\n",
       " ['Antique_Horse_2370', 1],\n",
       " ['Any-Donut-1587', 1],\n",
       " ['Any-Seaworthiness164', 1],\n",
       " ['Any_Particular_346', 1],\n",
       " ['Apart_Set3960', 1],\n",
       " ['ApesmithMcD', 1],\n",
       " ['Apollox34', 1],\n",
       " ['Applemobilityspecial', 1],\n",
       " ['ApplicationLogical87', 1],\n",
       " ['ApprehensiveDrawer71', 1],\n",
       " ['AraAraGyaru', 1],\n",
       " ['ArminiusLad', 1],\n",
       " ['Arrrrrri', 1],\n",
       " ['Aspiring-Historian28', 1],\n",
       " ['Astavicious', 1],\n",
       " ['AuriHype', 1],\n",
       " ['AutisticChildren27', 1],\n",
       " ['Automatic_Phone695', 1],\n",
       " ['AutonomousAntonym', 1],\n",
       " ['Avenariusd', 1],\n",
       " ['Away-Actuator-513', 1],\n",
       " ['AwkwardAd4239', 1],\n",
       " ['Awsomrockman', 1],\n",
       " ['Ayez86', 1],\n",
       " ['B-Flash', 1],\n",
       " ['BBBabe0613', 1],\n",
       " ['BVRPLZR_', 1],\n",
       " ['BWBurnnnner', 1],\n",
       " ['Badatude', 1],\n",
       " ['BalanceDouble6369', 1],\n",
       " ['Ballindtown', 1],\n",
       " ['BamboozleNoodle_746', 1],\n",
       " ['BarnabusCollywog', 1],\n",
       " ['BaronVonCuddly', 1],\n",
       " ['Bartowki03', 1],\n",
       " ['Beachykins', 1],\n",
       " ['BeardedDrakken', 1],\n",
       " ['BeatCharacter5853', 1],\n",
       " ['Beepbeepfroggo', 1],\n",
       " ['Beneficial_Horse_525', 1],\n",
       " ['Beneficial_Tie3122', 1],\n",
       " ['BestBuyCoolaid', 1],\n",
       " ['BestBuyPFlow', 1],\n",
       " ['Better_Tailor_1016', 1],\n",
       " ['Big-Slice-6121', 1],\n",
       " ['BigCTM', 1],\n",
       " ['BigManSaturn', 1],\n",
       " ['Binky_Barnes_', 1],\n",
       " ['BittahBandit1', 1],\n",
       " ['BlakeH1984', 1],\n",
       " ['BleuRampage', 1],\n",
       " ['BlizKriegBR117', 1],\n",
       " ['BloodyAx', 1],\n",
       " ['Blurpee24', 1],\n",
       " ['Bnunya30', 1],\n",
       " ['Bombchelle545', 1],\n",
       " ['BookAcceptable9524', 1],\n",
       " ['Bookish-beauty', 1],\n",
       " ['Boredommachine', 1],\n",
       " ['Boricsha03', 1],\n",
       " ['Boring-Breadfruit268', 1],\n",
       " ['BosmerBro', 1],\n",
       " ['BostonRevolutionary', 1],\n",
       " ['Both_Insurance8731', 1],\n",
       " ['Bourbzahn', 1],\n",
       " ['Brad32198', 1],\n",
       " ['BrainyRedneck', 1],\n",
       " ['Bre120', 1],\n",
       " ['Bread_Aggravating', 1],\n",
       " ['Breatht8king', 1],\n",
       " ['BruisedJune', 1],\n",
       " ['Bucketnation1', 1],\n",
       " ['BugsB_iolin', 1],\n",
       " ['Business-Tangerine17', 1],\n",
       " ['Business_Strength_95', 1],\n",
       " ['Bustin_Justin521', 1],\n",
       " ['ButlerKevind', 1],\n",
       " ['BuyBABA', 1],\n",
       " ['ByeBiBrie', 1],\n",
       " ['Bytor911', 1],\n",
       " ['C8kester', 1],\n",
       " ['CactusJane98', 1],\n",
       " ['CakeSavage', 1],\n",
       " ['CalligrapherSquare29', 1],\n",
       " ['CanEmergency834', 1],\n",
       " ['Canadian_Arcade', 1],\n",
       " ['Candid-Radish-2217', 1],\n",
       " ['Careful_Challenge216', 1],\n",
       " ['CarlSpackler01', 1],\n",
       " ['CashmanBling', 1],\n",
       " ['CasualFridayCrasher', 1],\n",
       " ['CauliflowerSolid38', 1],\n",
       " ['CenLaw', 1],\n",
       " ['CertifiedBacon', 1],\n",
       " ['CeruleanSea1', 1],\n",
       " ['Chance-Parfait-8178', 1],\n",
       " ['Chaos0882', 1],\n",
       " ['Character_Bake_9655', 1],\n",
       " ['Character_Spend_7737', 1],\n",
       " ['ChaseTheHonda', 1],\n",
       " ['ChefNSavage', 1],\n",
       " ['Cherry_BaBomb', 1],\n",
       " ['Chewy0198', 1],\n",
       " ['ChiefTornado', 1],\n",
       " ['Ciscoblue113', 1],\n",
       " ['CitizenStratocracy', 1],\n",
       " ['CivilRisk2751', 1],\n",
       " ['Claeys11', 1],\n",
       " ['Clarisant', 1],\n",
       " ['ClearAppearance1110', 1],\n",
       " ['CoSponC', 1],\n",
       " ['CobraCommander35', 1],\n",
       " ['CodeNameKND5', 1],\n",
       " ['CodyRyan86', 1],\n",
       " ['CoffeeMilkLvr', 1],\n",
       " ['Coldpowder', 1],\n",
       " ['CombatInsiders', 1],\n",
       " ['ComfortableMix5950', 1],\n",
       " ['Commercial-Jello-553', 1],\n",
       " ['Commercial-List1375', 1],\n",
       " ['Competitivereject', 1],\n",
       " ['Comprehensive-Act597', 1],\n",
       " ['Confident_Camera_416', 1],\n",
       " ['Confident_Lecture498', 1],\n",
       " ['Conscious-Resolve107', 1],\n",
       " ['ConsoleGamers04', 1],\n",
       " ['Constant_Ad1324', 1],\n",
       " ['ControlCAD', 1],\n",
       " ['CoopLogJams', 1],\n",
       " ['CourtForeign', 1],\n",
       " ['Creative-Reporter750', 1],\n",
       " ['Creativeusername5489', 1],\n",
       " ['CrimsonRose3773', 1],\n",
       " ['CringyGingie', 1],\n",
       " ['Cthulhu8762', 1],\n",
       " ['Cultural-Election377', 1],\n",
       " ['CupboardRS', 1],\n",
       " ['Curious_Document3864', 1],\n",
       " ['Cute-Trick-443', 1],\n",
       " ['D1ll0n', 1],\n",
       " ['DJKGinHD', 1],\n",
       " ['DJ_Projugs', 1],\n",
       " ['DJ_T3tris', 1],\n",
       " ['DaGUNforhire', 1],\n",
       " ['DaisukiYo', 1],\n",
       " ['Daken612', 1],\n",
       " ['Damp_loaf', 1],\n",
       " ['DangerousAmbassador', 1],\n",
       " ['Dani_Wolf', 1],\n",
       " ['DannyKit7', 1],\n",
       " ['DarkNi9ht12', 1],\n",
       " ['Darlugaa', 1],\n",
       " ['Dastardly_CheesyMan', 1],\n",
       " ['Daveyhavok832', 1],\n",
       " ['DavidANaida', 1],\n",
       " ['DeadBird101', 1],\n",
       " ['Dead_arie', 1],\n",
       " ['DeathCab4Cutie', 1],\n",
       " ['Demo5335', 1],\n",
       " ['Dependent_Heart25', 1],\n",
       " ['Dependent_Slip9881', 1],\n",
       " ['DesertRose90', 1],\n",
       " ['DesignerDeparture351', 1],\n",
       " ['DetroitRon', 1],\n",
       " ['DexL0x', 1],\n",
       " ['Dexity72', 1],\n",
       " ['Dezodro', 1],\n",
       " ['DifferencePowerful60', 1],\n",
       " ['Diiioond', 1],\n",
       " ['Din135', 1],\n",
       " ['DirtyDishwadda', 1],\n",
       " ['DiscoThief23', 1],\n",
       " ['Discount-Serious', 1],\n",
       " ['Disney_Dude1998', 1],\n",
       " ['DistributionLife1394', 1],\n",
       " ['Dominick_PK', 1],\n",
       " ['DonkeyKongsVet', 1],\n",
       " ['Donkeysmoove', 1],\n",
       " ['DoopyDooper_', 1],\n",
       " ['DoubleClickMouse', 1],\n",
       " ['Downtown-Energy9845', 1],\n",
       " ['Dquinones97', 1],\n",
       " ['Dracolis', 1],\n",
       " ['Dragoon_13', 1],\n",
       " ['Drako102685', 1],\n",
       " ['DreadedChalupacabra', 1],\n",
       " ['DroidXZR1', 1],\n",
       " ['Dry-Chain2613', 1],\n",
       " ['DryBerry2187', 1],\n",
       " ['Dry_Welder3681', 1],\n",
       " ['Due-Consequence1433', 1],\n",
       " ['Due-Program2197', 1],\n",
       " ['DuhItchyMoose', 1],\n",
       " ['Dull_Ad4015', 1],\n",
       " ['DumboWumbo579', 1],\n",
       " ['Dusty_Woot', 1],\n",
       " ['ENJ7188', 1],\n",
       " ['Early_Elevator624', 1],\n",
       " ['EastAcanthocephala52', 1],\n",
       " ['Eastern-Turnip7799', 1],\n",
       " ['Eastern_Ad_6946', 1],\n",
       " ['EasyEstablishment866', 1],\n",
       " ['Eclipser39', 1],\n",
       " ['Ecliptic_Panda', 1],\n",
       " ['Educational-Algae869', 1],\n",
       " ['Effective_Original64', 1],\n",
       " ['Efficient_Watch_7636', 1],\n",
       " ['Either-Arm4930', 1],\n",
       " ['ElderberryFrosty8404', 1],\n",
       " ['Electrical-Parking-1', 1],\n",
       " ['Electrical-Raisin296', 1],\n",
       " ['Electrical_Youth_484', 1],\n",
       " ['Electronic_Kick7678', 1],\n",
       " ['ElektricGhost', 1],\n",
       " ['Embarrassed-Tailor-8', 1],\n",
       " ['EmbarrassedFood8104', 1],\n",
       " ['Emergency-Bluebird54', 1],\n",
       " ['Emergency_Village_52', 1],\n",
       " ['EmmyJMR', 1],\n",
       " ['EmptyResident8887', 1],\n",
       " ['Enemaofthesubreddit', 1],\n",
       " ['Environmental-Pie833', 1],\n",
       " ['Environmental_Swim75', 1],\n",
       " ['Episode_11', 1],\n",
       " ['Equal-Gold', 1],\n",
       " ['Equivalent-Court-283', 1],\n",
       " ['Ethrealrunner', 1],\n",
       " ['Evan_v21', 1],\n",
       " ['Excellent-Sea-9051', 1],\n",
       " ['ExcellentBadger8684', 1],\n",
       " ['Existing-Radio1948', 1],\n",
       " ['ExpertRing3167', 1],\n",
       " ['Extreme-Taro9181', 1],\n",
       " ['FabulusTimelady', 1],\n",
       " ['Falkens_Maze2', 1],\n",
       " ['FanPuzzleheaded5888', 1],\n",
       " ['Fancy-Category', 1],\n",
       " ['Fantastic-Original26', 1],\n",
       " ['Fantastic-Plane-4516', 1],\n",
       " ['Far_from_perfec', 1],\n",
       " ['FarmerBoyJon', 1],\n",
       " ['Fast_Technology9656', 1],\n",
       " ['FeedRepresentative59', 1],\n",
       " ['Felidae_Griz', 1],\n",
       " ['Finesty', 1],\n",
       " ['FitCompote143', 1],\n",
       " ['Fit_Prior_8265', 1],\n",
       " ['Flashy_Jacket1424', 1],\n",
       " ['FlopsAkaGlitchy', 1],\n",
       " ['FluffyPie9', 1],\n",
       " ['ForFreedumb', 1],\n",
       " ['Forkuta', 1],\n",
       " ['FormalPresentation17', 1],\n",
       " ['Forsaken-Machine-500', 1],\n",
       " ['Fortunfavrsbold', 1],\n",
       " ['Forward-Stock381', 1],\n",
       " ['Forward_Ad_8956', 1],\n",
       " ['FragileRock', 1],\n",
       " ['Friendly-Guitar-1475', 1],\n",
       " ['FriendlyCoomer465', 1],\n",
       " ['Friendly_Ranger_7535', 1],\n",
       " ['Fun-Classic9936', 1],\n",
       " ['Fun_Fly_6311', 1],\n",
       " ['Funk-uh-phyzed', 1],\n",
       " ['Fuzzy-Conversation24', 1],\n",
       " ['Gadgetaddy', 1],\n",
       " ['Galapaghost93', 1],\n",
       " ['Galladex_XD', 1],\n",
       " ['Geek134623', 1],\n",
       " ['Gekko_League', 1],\n",
       " ['GetRichOrBrokeTrying', 1],\n",
       " ['GhostlyConnection', 1],\n",
       " ['Gish9', 1],\n",
       " ['Glad-Ad-3172', 1],\n",
       " ['GodsApprentice_9', 1],\n",
       " ['Gol_D_Frieza', 1],\n",
       " ['Goliath81400', 1],\n",
       " ['Good-Winner7092', 1],\n",
       " ['Good_Cookie5006', 1],\n",
       " ['GooglyGuy123', 1],\n",
       " ['Govums', 1],\n",
       " ['Grammar-Bot-Elite', 1],\n",
       " ['GremGram973', 1],\n",
       " ['GrompyGoat', 1],\n",
       " ['GroundbreakingBet281', 1],\n",
       " ['Guilty_Ratio1738', 1],\n",
       " ['H1_V0LT4G3', 1],\n",
       " ['HRHWPR', 1],\n",
       " ['Halloqween', 1],\n",
       " ['Hammer1129', 1],\n",
       " ['HamsterSpamster', 1],\n",
       " ['HarrizzonFord', 1],\n",
       " ['Hastalavistababyyy', 1],\n",
       " ['HauntingCold72', 1],\n",
       " ['HaveANiceDay42069420', 1],\n",
       " ['Hawk2990', 1],\n",
       " ['HeXxGuy', 1],\n",
       " ['HedgeHood', 1],\n",
       " ['Hefty-Market-8845', 1],\n",
       " ['Helios119', 1],\n",
       " ['Hfth20091000', 1],\n",
       " ['Historical-Ad-5143', 1],\n",
       " ['Hobothug', 1],\n",
       " ['HomerSimpson528', 1],\n",
       " ['Honest-Recognition48', 1],\n",
       " ['Honest_FocusGroups', 1],\n",
       " ['Hopeful_Ebb5443', 1],\n",
       " ['HornetTight', 1],\n",
       " ['Hot-Guidance5117', 1],\n",
       " ['Hunter_Pentaghast', 1],\n",
       " ['ICantThinkOfAName117', 1],\n",
       " ['ICarryXBL', 1],\n",
       " ['ILbudtender', 1],\n",
       " ['Icy-Spinach8224', 1],\n",
       " ['Icy-Start6815', 1],\n",
       " ['Ill_Temperature_9910', 1],\n",
       " ['Illustrious-Tiger-98', 1],\n",
       " ['IllustriousOil9013', 1],\n",
       " ['Illustrious_Ad611', 1],\n",
       " ['Illustrious_Cook3089', 1],\n",
       " ['Illustrious_Half_723', 1],\n",
       " ...]"
      ]
     },
     "execution_count": 1228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authors with most diverse contributions\n",
    "author_spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55705fd-e74f-4394-ab5a-2b202f5dd21f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### most discussed threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "id": "69edf33e-5974-4458-8401-fc7c15282c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The broad topics the author is talking about include:\n",
      "\n",
      "1. Employee experiences and concerns at Best Buy, such as reduced headcount, layoffs, and job qualifications.\n",
      "2. Company policies and procedures, such as page moderation, drug testing, and severance packages.\n",
      "3. Industry news and trends, such as the rebranding of Great Call and LG Display's supply of OLED TV panels to Samsung.\n",
      "4. Unionization and worker rights.\n",
      "5. Personal anecdotes and opinions, such as experiences with management and coworkers.\n",
      "6. Job search and career advice.\n",
      "7. Secrecy and non-disclosure agreements.\n",
      "8. Frustrations with corporate transparency and decision-making.\n"
     ]
    }
   ],
   "source": [
    "print(llm_mixtral.invoke(f\"Based on the list provided, mention the broad topics the author is talking about? {list(df_subreddit[df_subreddit.reddit_author==author_spread[2][0]].reddit_text)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "6685684d-dcc1-4392-abde-dd2ce8c45a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The broad topics the author is talking about include:\n",
      "\n",
      "1. Employee benefits and compensation, such as holiday pay, time and a half pay, and matching contributions for retirement savings.\n",
      "2. Job codes, certifications, and accommodations for certain positions.\n",
      "3. The process of clocking in and out for shifts.\n",
      "4. Labor cuts and available hours.\n",
      "5. Store pickups and the fulfillment process.\n",
      "6. The Best Buy Discord and scheduling diagnostic appointments.\n"
     ]
    }
   ],
   "source": [
    "print(llm_mixtral.invoke(f\"Based on the list provided, mention the broad topics the author is talking about? {list(df_subreddit[df_subreddit.reddit_author==author_spread[12][0]].reddit_text)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "f39429d3-4329-4e85-bfa8-06a08c0023cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_subreddit[df_subreddit.aware_post_type=='submission'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab713f06-14ce-4fa2-af24-77b16f00497c",
   "metadata": {},
   "source": [
    "There are 827 threads in total in this subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "4eebe911-4113-45b7-bf91-10dd984ed870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aware_post_type</th>\n",
       "      <th>aware_created_ts</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>reddit_name</th>\n",
       "      <th>reddit_created_utc</th>\n",
       "      <th>reddit_author</th>\n",
       "      <th>reddit_text</th>\n",
       "      <th>reddit_permalink</th>\n",
       "      <th>reddit_title</th>\n",
       "      <th>reddit_url</th>\n",
       "      <th>reddit_subreddit</th>\n",
       "      <th>reddit_link_id</th>\n",
       "      <th>reddit_parent_id</th>\n",
       "      <th>reddit_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>submission</td>\n",
       "      <td>2023-04-16T17:32:45</td>\n",
       "      <td>12opsul</td>\n",
       "      <td>t3_12opsul</td>\n",
       "      <td>1681680765</td>\n",
       "      <td>utaustinresearch</td>\n",
       "      <td></td>\n",
       "      <td>/r/BestBuyWorkers/comments/12opsul/research_st...</td>\n",
       "      <td>Research Study Recruitment - Managers</td>\n",
       "      <td>https://i.redd.it/nrmkf51rebua1.png</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2023-04-16T18:48:06</td>\n",
       "      <td>jgjgy9e</td>\n",
       "      <td>t1_jgjgy9e</td>\n",
       "      <td>1681685286</td>\n",
       "      <td>Not_A_Real_Boy69</td>\n",
       "      <td>![gif](giphy|YmQLj2KxaNz58g7Ofg)\\n\\n$50?</td>\n",
       "      <td>/r/BestBuyWorkers/comments/12opsul/research_st...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>t3_12opsul</td>\n",
       "      <td>t3_12opsul</td>\n",
       "      <td>12opsul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2023-04-16T19:55:39</td>\n",
       "      <td>jgjpqmp</td>\n",
       "      <td>t1_jgjpqmp</td>\n",
       "      <td>1681689339</td>\n",
       "      <td>GSAgentsLivesMatter</td>\n",
       "      <td>bullshit on getting $50 its just a coupon to B...</td>\n",
       "      <td>/r/BestBuyWorkers/comments/12opsul/research_st...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>t3_12opsul</td>\n",
       "      <td>t3_12opsul</td>\n",
       "      <td>12opsul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  aware_post_type     aware_created_ts reddit_id reddit_name  \\\n",
       "0      submission  2023-04-16T17:32:45   12opsul  t3_12opsul   \n",
       "1         comment  2023-04-16T18:48:06   jgjgy9e  t1_jgjgy9e   \n",
       "2         comment  2023-04-16T19:55:39   jgjpqmp  t1_jgjpqmp   \n",
       "\n",
       "   reddit_created_utc        reddit_author  \\\n",
       "0          1681680765     utaustinresearch   \n",
       "1          1681685286     Not_A_Real_Boy69   \n",
       "2          1681689339  GSAgentsLivesMatter   \n",
       "\n",
       "                                         reddit_text  \\\n",
       "0                                                      \n",
       "1           ![gif](giphy|YmQLj2KxaNz58g7Ofg)\\n\\n$50?   \n",
       "2  bullshit on getting $50 its just a coupon to B...   \n",
       "\n",
       "                                    reddit_permalink  \\\n",
       "0  /r/BestBuyWorkers/comments/12opsul/research_st...   \n",
       "1  /r/BestBuyWorkers/comments/12opsul/research_st...   \n",
       "2  /r/BestBuyWorkers/comments/12opsul/research_st...   \n",
       "\n",
       "                            reddit_title                           reddit_url  \\\n",
       "0  Research Study Recruitment - Managers  https://i.redd.it/nrmkf51rebua1.png   \n",
       "1                                   None                                 None   \n",
       "2                                   None                                 None   \n",
       "\n",
       "  reddit_subreddit reddit_link_id reddit_parent_id reddit_submission  \n",
       "0   BestBuyWorkers           None             None              None  \n",
       "1   BestBuyWorkers     t3_12opsul       t3_12opsul           12opsul  \n",
       "2   BestBuyWorkers     t3_12opsul       t3_12opsul           12opsul  "
      ]
     },
     "execution_count": 1120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subreddit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "id": "5e97b1af-1c51-4e74-999d-352ac8577ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reddit_ids = list(df_subreddit[df_subreddit.aware_post_type=='submission'].reddit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "a2311792-0999-4b45-8fc5-a9184648b561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t3_12opsul': 3,\n",
       " 't3_12m0ozl': 15,\n",
       " 't3_12l3md8': 19,\n",
       " 't3_12gb7ps': 13,\n",
       " 't3_12g25g2': 5,\n",
       " 't3_12ectgf': 3,\n",
       " 't3_12e4wef': 17,\n",
       " 't3_12du2t0': 11,\n",
       " 't3_11wk8fh': 3,\n",
       " 't3_11qb2k2': 3,\n",
       " 't3_11ne756': 12,\n",
       " 't3_11hyxkb': 9,\n",
       " 't3_11fb73q': 6,\n",
       " 't3_11ew3fs': 2,\n",
       " 't3_11dxaol': 5,\n",
       " 't3_11bgct9': 6,\n",
       " 't3_119at7w': 4,\n",
       " 't3_117t418': 2,\n",
       " 't3_1157t7i': 7,\n",
       " 't3_114sq9a': 3,\n",
       " 't3_1142913': 11,\n",
       " 't3_111y6s8': 5,\n",
       " 't3_10v8unp': 14,\n",
       " 't3_10usgg4': 12,\n",
       " 't3_10tdmkb': 17,\n",
       " 't3_10ljawj': 7,\n",
       " 't3_10jui63': 2,\n",
       " 't3_10i9ao0': 13,\n",
       " 't3_10gllu9': 2,\n",
       " 't3_10fmkvw': 4,\n",
       " 't3_10d2uum': 11,\n",
       " 't3_10d1dj8': 3,\n",
       " 't3_10cxfxu': 13,\n",
       " 't3_10brc4f': 7,\n",
       " 't3_10711zn': 6,\n",
       " 't3_103bmqk': 8,\n",
       " 't3_zzx2wq': 3,\n",
       " 't3_zzj0lf': 5,\n",
       " 't3_zyq49u': 2,\n",
       " 't3_zxspyw': 1,\n",
       " 't3_zxia62': 3,\n",
       " 't3_zxgmp4': 5,\n",
       " 't3_zwzlqi': 5,\n",
       " 't3_zut2tr': 3,\n",
       " 't3_ztu253': 2,\n",
       " 't3_zrrenz': 2,\n",
       " 't3_zqi52d': 3,\n",
       " 't3_zoqq39': 5,\n",
       " 't3_znjz7f': 1,\n",
       " 't3_zj1p6d': 3,\n",
       " 't3_zh0k71': 9,\n",
       " 't3_zfknoy': 2,\n",
       " 't3_zcqnma': 2,\n",
       " 't3_z9cfmt': 1,\n",
       " 't3_z7kfmo': 16,\n",
       " 't3_z6ve99': 4,\n",
       " 't3_z44t79': 3,\n",
       " 't3_z1mzn6': 34,\n",
       " 't3_z1iupm': 3,\n",
       " 't3_yzicds': 3,\n",
       " 't3_yy085z': 8,\n",
       " 't3_yxac0a': 2,\n",
       " 't3_ywjf2p': 5,\n",
       " 't3_yw5weg': 7,\n",
       " 't3_yub7qv': 2,\n",
       " 't3_yuacvb': 12,\n",
       " 't3_yoa9td': 16,\n",
       " 't3_yo7rpy': 3,\n",
       " 't3_ymq1zi': 2,\n",
       " 't3_ykuv6m': 11,\n",
       " 't3_yj60zs': 6,\n",
       " 't3_yi0dl4': 13,\n",
       " 't3_yf030k': 6,\n",
       " 't3_ydy8u2': 7,\n",
       " 't3_ydn44q': 2,\n",
       " 't3_yae71u': 3,\n",
       " 't3_y95bo3': 4,\n",
       " 't3_y6s4ot': 11,\n",
       " 't3_y3ewha': 3,\n",
       " 't3_y1psff': 8,\n",
       " 't3_y0hlyc': 2,\n",
       " 't3_xwbtt4': 3,\n",
       " 't3_xu6znu': 3,\n",
       " 't3_xtqoo8': 6,\n",
       " 't3_xrazhv': 3,\n",
       " 't3_xl74rb': 4,\n",
       " 't3_xj7tu0': 6,\n",
       " 't3_xcoddj': 21,\n",
       " 't3_xcmb3p': 2,\n",
       " 't3_xcctn9': 4,\n",
       " 't3_xcbqxx': 11,\n",
       " 't3_x2b57k': 6,\n",
       " 't3_x1ofw2': 11,\n",
       " 't3_x0qutw': 6,\n",
       " 't3_wyruo5': 6,\n",
       " 't3_wylhu9': 2,\n",
       " 't3_wxixwd': 4,\n",
       " 't3_wtr1q0': 12,\n",
       " 't3_wqed2b': 14,\n",
       " 't3_wniz1j': 3,\n",
       " 't3_wkca45': 19,\n",
       " 't3_wey7mh': 2,\n",
       " 't3_wapsy3': 16,\n",
       " 't3_w6md2b': 1,\n",
       " 't3_vxega2': 4,\n",
       " 't3_vv304y': 5,\n",
       " 't3_vu0jn5': 4,\n",
       " 't3_vq2463': 4,\n",
       " 't3_vlfo5s': 6,\n",
       " 't3_vl7x5z': 4,\n",
       " 't3_vj5q19': 9,\n",
       " 't3_vd0kfl': 6,\n",
       " 't3_vchd5h': 4,\n",
       " 't3_v5thte': 13,\n",
       " 't3_v5ngr7': 4,\n",
       " 't3_uyokbi': 2,\n",
       " 't3_uyhtwa': 5,\n",
       " 't3_uihx76': 20,\n",
       " 't3_ugbg5l': 1,\n",
       " 't3_ueb255': 2,\n",
       " 't3_udczwq': 1,\n",
       " 't3_u9ubpe': 6,\n",
       " 't3_u9gjw5': 4,\n",
       " 't3_u9gfjv': 1,\n",
       " 't3_u3qebi': 9,\n",
       " 't3_u2sqlk': 9,\n",
       " 't3_u2fy38': 7,\n",
       " 't3_u05hnv': 7,\n",
       " 't3_tzf806': 4,\n",
       " 't3_tvae2o': 16,\n",
       " 't3_trfy8g': 4,\n",
       " 't3_tqejhm': 44,\n",
       " 't3_tqcpc5': 6,\n",
       " 't3_tginwj': 1,\n",
       " 't3_tfdswx': 1,\n",
       " 't3_teamht': 2,\n",
       " 't3_tdyte4': 7,\n",
       " 't3_tdejak': 2,\n",
       " 't3_td2nwr': 2,\n",
       " 't3_t4locz': 10,\n",
       " 't3_t4i7f9': 6,\n",
       " 't3_t3mcbk': 6,\n",
       " 't3_t1if6w': 5,\n",
       " 't3_sygsv3': 4,\n",
       " 't3_sv7wub': 3,\n",
       " 't3_spsus3': 3,\n",
       " 't3_spl527': 6,\n",
       " 't3_sj3cnq': 6,\n",
       " 't3_si5q4d': 3,\n",
       " 't3_sb7lwq': 2,\n",
       " 't3_s80fuy': 2,\n",
       " 't3_s7vrdv': 4,\n",
       " 't3_s55io5': 8,\n",
       " 't3_rxnc8a': 4,\n",
       " 't3_resym9': 4,\n",
       " 't3_rehv42': 7,\n",
       " 't3_r6v772': 3,\n",
       " 't3_r5dkhb': 7,\n",
       " 't3_r4f01w': 6,\n",
       " 't3_r3z17r': 7,\n",
       " 't3_r3bzg2': 3,\n",
       " 't3_qz3l4m': 6,\n",
       " 't3_qwu04w': 2,\n",
       " 't3_qvep5e': 16,\n",
       " 't3_quy853': 5,\n",
       " 't3_qujmiu': 8,\n",
       " 't3_qty7j8': 6,\n",
       " 't3_qso748': 3,\n",
       " 't3_qqysf1': 2,\n",
       " 't3_qpi3rj': 7,\n",
       " 't3_qp4czc': 2,\n",
       " 't3_qoaeba': 3,\n",
       " 't3_qo85m9': 3,\n",
       " 't3_qmrvlh': 5,\n",
       " 't3_qkqiaw': 7,\n",
       " 't3_qkcr6h': 2,\n",
       " 't3_qieejj': 2,\n",
       " 't3_qi4wrh': 3,\n",
       " 't3_qhb9v1': 12,\n",
       " 't3_qgxqdr': 3,\n",
       " 't3_qeroej': 2,\n",
       " 't3_qafhhx': 10,\n",
       " 't3_q6jfmp': 4,\n",
       " 't3_q3jp1k': 1,\n",
       " 't3_q3du8t': 4,\n",
       " 't3_q1zf00': 2,\n",
       " 't3_q1jumh': 1,\n",
       " 't3_q1fhwu': 6,\n",
       " 't3_q1fg8q': 8,\n",
       " 't3_pz2gf4': 1,\n",
       " 't3_pyykcv': 3,\n",
       " 't3_px8g4w': 8,\n",
       " 't3_ptpa4y': 1,\n",
       " 't3_psiueg': 7,\n",
       " 't3_ppvrs6': 3,\n",
       " 't3_poxhx7': 3,\n",
       " 't3_pnr0ls': 4,\n",
       " 't3_plkh35': 8,\n",
       " 't3_pl1q4b': 4,\n",
       " 't3_pl0yqk': 2,\n",
       " 't3_pebp8a': 6,\n",
       " 't3_pdjgpi': 5,\n",
       " 't3_pdbmth': 2,\n",
       " 't3_p953t1': 3,\n",
       " 't3_p5w70t': 1,\n",
       " 't3_p5u2zs': 8,\n",
       " 't3_p3aot6': 3,\n",
       " 't3_ozadh6': 3,\n",
       " 't3_oyzq2w': 2,\n",
       " 't3_ourgi4': 2,\n",
       " 't3_oo5tzv': 5,\n",
       " 't3_ongml1': 5,\n",
       " 't3_omfjc4': 24,\n",
       " 't3_oicyxd': 1,\n",
       " 't3_of5f1k': 1,\n",
       " 't3_o8lx12': 3,\n",
       " 't3_o75ftk': 6,\n",
       " 't3_o55u3j': 1,\n",
       " 't3_o4fkm6': 4,\n",
       " 't3_o1czhu': 4,\n",
       " 't3_o0ol55': 4,\n",
       " 't3_nwkgsw': 5,\n",
       " 't3_nw7y6z': 1,\n",
       " 't3_nqqgpx': 9,\n",
       " 't3_nq49o1': 6,\n",
       " 't3_nndsf2': 5,\n",
       " 't3_mz4g6j': 1,\n",
       " 't3_mxjjiu': 3,\n",
       " 't3_mvautv': 3,\n",
       " 't3_mud11u': 2,\n",
       " 't3_mpq92y': 4,\n",
       " 't3_mkdqfb': 8,\n",
       " 't3_mfeb7f': 1,\n",
       " 't3_mesrs0': 7,\n",
       " 't3_mc9gtw': 8,\n",
       " 't3_ma92by': 6,\n",
       " 't3_m4xdo7': 4,\n",
       " 't3_lwjpin': 22,\n",
       " 't3_lwgv4o': 1,\n",
       " 't3_lu2yq7': 6,\n",
       " 't3_ltjksq': 6,\n",
       " 't3_lsmfjh': 1,\n",
       " 't3_lsk11y': 3,\n",
       " 't3_low5bv': 1,\n",
       " 't3_lm7x0c': 7,\n",
       " 't3_lk0m34': 4,\n",
       " 't3_lhngdx': 1,\n",
       " 't3_lhlpyv': 1,\n",
       " 't3_lhb7ho': 1,\n",
       " 't3_lh82ay': 6,\n",
       " 't3_lg5of6': 1,\n",
       " 't3_letwhq': 1,\n",
       " 't3_lesmde': 1,\n",
       " 't3_les77v': 1,\n",
       " 't3_ldg74q': 8,\n",
       " 't3_l9l2ac': 3,\n",
       " 't3_kwnyzr': 9,\n",
       " 't3_kwmip5': 9,\n",
       " 't3_kw7yxb': 10,\n",
       " 't3_ktqdd1': 2,\n",
       " 't3_kt7dp9': 3,\n",
       " 't3_kqtsws': 2,\n",
       " 't3_knxrpq': 5,\n",
       " 't3_km802q': 6,\n",
       " 't3_klk0z1': 5,\n",
       " 't3_kkpk0q': 3,\n",
       " 't3_khxev9': 4,\n",
       " 't3_kfzs8n': 7,\n",
       " 't3_kdngf0': 8,\n",
       " 't3_kchxrc': 1,\n",
       " 't3_kchdeu': 10,\n",
       " 't3_kacts6': 2,\n",
       " 't3_k7zxab': 7,\n",
       " 't3_k4b7mt': 3,\n",
       " 't3_k2mp32': 6,\n",
       " 't3_k2a20b': 1,\n",
       " 't3_k18u9o': 1,\n",
       " 't3_k127qt': 8,\n",
       " 't3_jzib0w': 3,\n",
       " 't3_jxziau': 5,\n",
       " 't3_jx68il': 3,\n",
       " 't3_jx0q4y': 6,\n",
       " 't3_jwlmpz': 5,\n",
       " 't3_jwayfc': 6,\n",
       " 't3_jw78lp': 3,\n",
       " 't3_jv7r7b': 3,\n",
       " 't3_jsiasq': 5,\n",
       " 't3_jsb4fa': 5,\n",
       " 't3_jr56wa': 6,\n",
       " 't3_jpi19f': 5,\n",
       " 't3_jom7lh': 8,\n",
       " 't3_joktym': 5,\n",
       " 't3_jod3zh': 4,\n",
       " 't3_jo8di6': 3,\n",
       " 't3_jo4kbh': 3,\n",
       " 't3_jo19ko': 5,\n",
       " 't3_jmxejg': 5,\n",
       " 't3_jm5ce0': 10,\n",
       " 't3_jkc0sq': 3,\n",
       " 't3_jh2ws3': 6,\n",
       " 't3_jf4kz1': 9,\n",
       " 't3_jf2d05': 4,\n",
       " 't3_jezfvk': 4,\n",
       " 't3_jdiksx': 5,\n",
       " 't3_jdgiw1': 1,\n",
       " 't3_j6v9a4': 11,\n",
       " 't3_j5vbke': 4,\n",
       " 't3_j3pm4i': 2,\n",
       " 't3_j2aewq': 3,\n",
       " 't3_j15u03': 9,\n",
       " 't3_j0wau6': 6,\n",
       " 't3_j0gpr9': 6,\n",
       " 't3_iyk0me': 5,\n",
       " 't3_ixq5nx': 3,\n",
       " 't3_iubp8g': 3,\n",
       " 't3_iubdak': 1,\n",
       " 't3_itpfzm': 3,\n",
       " 't3_itkyfs': 6,\n",
       " 't3_it7img': 2,\n",
       " 't3_ipfos1': 5,\n",
       " 't3_io665d': 2,\n",
       " 't3_ib0drx': 1,\n",
       " 't3_i8i9p0': 2,\n",
       " 't3_i6yj5y': 4,\n",
       " 't3_i5e8z7': 3,\n",
       " 't3_i1p9b9': 3,\n",
       " 't3_hyzv8a': 1,\n",
       " 't3_hyscsg': 3,\n",
       " 't3_hygauo': 1,\n",
       " 't3_hwbdxn': 4,\n",
       " 't3_hg8g80': 5,\n",
       " 't3_gwyedp': 6,\n",
       " 't3_gtgu7v': 2,\n",
       " 't3_gin14r': 1,\n",
       " 't3_gfep79': 18,\n",
       " 't3_g9xfim': 1,\n",
       " 't3_g9ackt': 3,\n",
       " 't3_g1xznw': 1,\n",
       " 't3_g1upnx': 1,\n",
       " 't3_g1skrg': 7,\n",
       " 't3_g1jnvc': 11,\n",
       " 't3_g1j93a': 2,\n",
       " 't3_g1ipnr': 3,\n",
       " 't3_g1h57g': 12,\n",
       " 't3_g1glzh': 13,\n",
       " 't3_12rblul': 4,\n",
       " 't3_12r8ob9': 7,\n",
       " 't3_12scwfu': 7,\n",
       " 't3_12usy5i': 20,\n",
       " 't3_12un85y': 6,\n",
       " 't3_12ukjb7': 4,\n",
       " 't3_12vpyl1': 3,\n",
       " 't3_12vlk9u': 5,\n",
       " 't3_12wxirw': 5,\n",
       " 't3_12woa5a': 5,\n",
       " 't3_12z00x0': 13,\n",
       " 't3_1307myr': 3,\n",
       " 't3_131g4hk': 2,\n",
       " 't3_132dgjh': 19,\n",
       " 't3_132bzni': 29,\n",
       " 't3_134bzme': 1,\n",
       " 't3_135562t': 12,\n",
       " 't3_1365mo9': 1,\n",
       " 't3_136ivb4': 4,\n",
       " 't3_1371lok': 2,\n",
       " 't3_136z3ru': 22,\n",
       " 't3_1382vj6': 38,\n",
       " 't3_139rims': 5,\n",
       " 't3_138xt52': 3,\n",
       " 't3_13beb91': 3,\n",
       " 't3_13bbmyk': 2,\n",
       " 't3_13c0ls4': 1,\n",
       " 't3_13cdeph': 5,\n",
       " 't3_13dhtj6': 2,\n",
       " 't3_13dyobj': 1,\n",
       " 't3_13evaou': 5,\n",
       " 't3_13f4xst': 2,\n",
       " 't3_13fq098': 1,\n",
       " 't3_13gm5y5': 2,\n",
       " 't3_13hdeiv': 11,\n",
       " 't3_13hop0a': 6,\n",
       " 't3_13jrye3': 7,\n",
       " 't3_13k5bg4': 7,\n",
       " 't3_13kxky3': 1,\n",
       " 't3_13kzfia': 11,\n",
       " 't3_13lv3v7': 6,\n",
       " 't3_13nu7mm': 6,\n",
       " 't3_13ngkrz': 1,\n",
       " 't3_13poqek': 10,\n",
       " 't3_13phox6': 4,\n",
       " 't3_13q5tph': 11,\n",
       " 't3_13qwi0k': 8,\n",
       " 't3_140k1b8': 1,\n",
       " 't3_140gz27': 2,\n",
       " 't3_1402nxm': 5,\n",
       " 't3_1401z1l': 4,\n",
       " 't3_13zxtx2': 7,\n",
       " 't3_13zvahh': 2,\n",
       " 't3_13zsm2d': 9,\n",
       " 't3_13zqf5q': 10,\n",
       " 't3_141be7b': 1,\n",
       " 't3_141fcm8': 1,\n",
       " 't3_141inu6': 2,\n",
       " 't3_142l7gq': 5,\n",
       " 't3_142qczh': 1,\n",
       " 't3_1433lms': 5,\n",
       " 't3_143ixlp': 9,\n",
       " 't3_143yid0': 2,\n",
       " 't3_145a8fl': 2,\n",
       " 't3_1456sxm': 3,\n",
       " 't3_146p5ca': 9,\n",
       " 't3_1468tsl': 11,\n",
       " 't3_147p5ym': 4,\n",
       " 't3_147oawn': 19,\n",
       " 't3_147wb7o': 1,\n",
       " 't3_1488pwc': 3,\n",
       " 't3_148nbff': 15,\n",
       " 't3_148yc0q': 6,\n",
       " 't3_1499vuh': 20,\n",
       " 't3_14a1omt': 1,\n",
       " 't3_14a61km': 12,\n",
       " 't3_14acmyy': 1,\n",
       " 't3_14ayug0': 1,\n",
       " 't3_14bnt21': 8,\n",
       " 't3_14bvaot': 1,\n",
       " 't3_14contk': 1,\n",
       " 't3_14d6fhg': 12,\n",
       " 't3_14dxspe': 11,\n",
       " 't3_14edki7': 17,\n",
       " 't3_14f51u0': 1,\n",
       " 't3_14fa7rw': 1,\n",
       " 't3_14fp7ld': 6,\n",
       " 't3_14giqiy': 7,\n",
       " 't3_14h75j6': 3,\n",
       " 't3_14htsv6': 3,\n",
       " 't3_14i1ppn': 15,\n",
       " 't3_14j8tr8': 12,\n",
       " 't3_14j0scp': 15,\n",
       " 't3_14jmrh2': 16,\n",
       " 't3_14kfqfw': 4,\n",
       " 't3_14lrnbf': 2,\n",
       " 't3_14lpwx9': 10,\n",
       " 't3_14mdjt5': 19,\n",
       " 't3_14mdbfa': 10,\n",
       " 't3_14ngsbs': 16,\n",
       " 't3_14n5zby': 21,\n",
       " 't3_14ozrzy': 1,\n",
       " 't3_14odcg2': 10,\n",
       " 't3_14pu6n9': 1,\n",
       " 't3_14qf9j1': 1,\n",
       " 't3_14r2li6': 4,\n",
       " 't3_14rbjqj': 1,\n",
       " 't3_14rpejz': 6,\n",
       " 't3_14rsuxq': 7,\n",
       " 't3_14ssvb2': 19,\n",
       " 't3_14sq8or': 12,\n",
       " 't3_14tji8u': 4,\n",
       " 't3_1525wxg': 8,\n",
       " 't3_1524ebz': 1,\n",
       " 't3_151soos': 7,\n",
       " 't3_151sguc': 1,\n",
       " 't3_151rb0o': 7,\n",
       " 't3_151pyeq': 12,\n",
       " 't3_151ggsb': 7,\n",
       " 't3_151fdrl': 2,\n",
       " 't3_152dtlv': 1,\n",
       " 't3_152ejwy': 13,\n",
       " 't3_153ukht': 2,\n",
       " 't3_153p3b7': 15,\n",
       " 't3_154021b': 1,\n",
       " 't3_15427rb': 3,\n",
       " 't3_154uy96': 5,\n",
       " 't3_155mqqj': 2,\n",
       " 't3_155qfus': 2,\n",
       " 't3_155sxwz': 1,\n",
       " 't3_1562ktm': 1,\n",
       " 't3_156m21q': 11,\n",
       " 't3_1580byk': 28,\n",
       " 't3_157wono': 13,\n",
       " 't3_158kw3d': 2,\n",
       " 't3_158xm55': 14,\n",
       " 't3_159fm06': 4,\n",
       " 't3_159k65p': 2,\n",
       " 't3_15a83ex': 3,\n",
       " 't3_15aae6q': 10,\n",
       " 't3_15av638': 9,\n",
       " 't3_15bv42o': 1,\n",
       " 't3_15bql24': 1,\n",
       " 't3_15bywxg': 7,\n",
       " 't3_15edznr': 3,\n",
       " 't3_15e1vvl': 12,\n",
       " 't3_15dbudc': 20,\n",
       " 't3_15fbomq': 13,\n",
       " 't3_15g91qa': 1,\n",
       " 't3_15gb9ch': 5,\n",
       " 't3_15gd65a': 1,\n",
       " 't3_15gjq3x': 3,\n",
       " 't3_15h9cil': 3,\n",
       " 't3_15hqpe9': 4,\n",
       " 't3_15i5w6j': 6,\n",
       " 't3_15i9vl8': 4,\n",
       " 't3_15ix5ck': 5,\n",
       " 't3_15ji8tx': 1,\n",
       " 't3_15k9xjn': 2,\n",
       " 't3_15k27l2': 2,\n",
       " 't3_15llwvl': 1,\n",
       " 't3_15llbwd': 2,\n",
       " 't3_15lojxb': 2,\n",
       " 't3_15m9int': 3,\n",
       " 't3_15mgqsl': 3,\n",
       " 't3_15qxpq6': 1,\n",
       " 't3_15qw6fe': 1,\n",
       " 't3_15qmnlg': 4,\n",
       " 't3_15qlois': 5,\n",
       " 't3_15qii74': 44,\n",
       " 't3_15r2u1w': 14,\n",
       " 't3_15t3heu': 5,\n",
       " 't3_15sx2rf': 49,\n",
       " 't3_15u9r2c': 2,\n",
       " 't3_15u9jff': 3,\n",
       " 't3_15undf7': 2,\n",
       " 't3_15uvfps': 1,\n",
       " 't3_15vfuy5': 1,\n",
       " 't3_15w5del': 4,\n",
       " 't3_15x4pfj': 7,\n",
       " 't3_15wven5': 14,\n",
       " 't3_15x93dd': 12,\n",
       " 't3_15xt7tl': 1,\n",
       " 't3_15z4pv7': 6,\n",
       " 't3_15ysn49': 6,\n",
       " 't3_15zvejv': 3,\n",
       " 't3_163c6jo': 2,\n",
       " 't3_163boli': 7,\n",
       " 't3_163516u': 2,\n",
       " 't3_162jj9o': 33,\n",
       " 't3_163ly9b': 1,\n",
       " 't3_163pvog': 1,\n",
       " 't3_164c3d5': 1,\n",
       " 't3_166098f': 3,\n",
       " 't3_165uvo6': 6,\n",
       " 't3_165utsm': 6,\n",
       " 't3_166h8m2': 4,\n",
       " 't3_166yzb4': 2,\n",
       " 't3_16agd8r': 2,\n",
       " 't3_16acqhk': 3,\n",
       " 't3_16acd96': 5,\n",
       " 't3_16a4p09': 2,\n",
       " 't3_16b9xbu': 12,\n",
       " 't3_16b5ojs': 2,\n",
       " 't3_16fik5c': 9,\n",
       " 't3_16fgugi': 15,\n",
       " 't3_16f92pv': 38,\n",
       " 't3_16f8ctj': 35,\n",
       " 't3_16f2cj3': 27,\n",
       " 't3_16fwh09': 6,\n",
       " 't3_16gr6yw': 1,\n",
       " 't3_16gzemq': 1,\n",
       " 't3_16ha775': 23,\n",
       " 't3_16i9r6w': 5,\n",
       " 't3_16i97tj': 11,\n",
       " 't3_16j3lhl': 6,\n",
       " 't3_16j0ber': 14,\n",
       " 't3_16jgp0r': 3,\n",
       " 't3_16k2v8f': 5,\n",
       " 't3_16lm5rm': 3,\n",
       " 't3_16llnsh': 6,\n",
       " 't3_16m5r96': 3,\n",
       " 't3_16mhlrm': 12,\n",
       " 't3_16mtxcj': 5,\n",
       " 't3_16ng0ma': 4,\n",
       " 't3_16o66as': 6,\n",
       " 't3_16o2ggz': 27,\n",
       " 't3_16qy9to': 3,\n",
       " 't3_16qxgr3': 1,\n",
       " 't3_16qkwpq': 4,\n",
       " 't3_16qk2n0': 23,\n",
       " 't3_16rlmqn': 4,\n",
       " 't3_16s15xr': 2,\n",
       " 't3_16sjzfc': 2,\n",
       " 't3_16tgwnq': 2,\n",
       " 't3_16tbzpv': 2,\n",
       " 't3_16tnb4l': 5,\n",
       " 't3_16u7oy8': 4,\n",
       " 't3_16xtxc6': 1,\n",
       " 't3_16xmkq5': 3,\n",
       " 't3_16xgjm6': 9,\n",
       " 't3_16x9tat': 6,\n",
       " 't3_16yg035': 7,\n",
       " 't3_16zfhm3': 3,\n",
       " 't3_16z9f83': 5,\n",
       " 't3_1709kko': 3,\n",
       " 't3_1706arf': 2,\n",
       " 't3_170i31u': 2,\n",
       " 't3_17138re': 20,\n",
       " 't3_1711zc9': 3,\n",
       " 't3_1730y1t': 4,\n",
       " 't3_172rufs': 2,\n",
       " 't3_172op5n': 3,\n",
       " 't3_173incm': 5,\n",
       " 't3_173u96t': 11,\n",
       " 't3_17491o2': 17,\n",
       " 't3_174zpmr': 2,\n",
       " 't3_174wtq6': 7,\n",
       " 't3_176si7n': 1,\n",
       " 't3_176mkmh': 8,\n",
       " 't3_176khx7': 5,\n",
       " 't3_179amsj': 20,\n",
       " 't3_1799ef9': 3,\n",
       " 't3_1797njv': 15,\n",
       " 't3_178w5qv': 13,\n",
       " 't3_179tf75': 2,\n",
       " 't3_179z965': 2,\n",
       " 't3_17aav23': 3,\n",
       " 't3_17afl2d': 2,\n",
       " 't3_17beebi': 3,\n",
       " 't3_17b9nci': 8,\n",
       " 't3_17c47xn': 3,\n",
       " 't3_17bt0f4': 3,\n",
       " 't3_17cwpja': 12,\n",
       " 't3_17cqno6': 11,\n",
       " 't3_17dd9oh': 4,\n",
       " 't3_17eebec': 9,\n",
       " 't3_17ec504': 2,\n",
       " 't3_17erw6z': 1,\n",
       " 't3_17fdtjg': 4,\n",
       " 't3_17g3her': 3,\n",
       " 't3_17g2qpo': 3,\n",
       " 't3_17hl1aj': 6,\n",
       " 't3_17hevu5': 2,\n",
       " 't3_17hcd8v': 5,\n",
       " 't3_17hu8la': 3,\n",
       " 't3_17k03ol': 2,\n",
       " 't3_17jw7cz': 3,\n",
       " 't3_17jm7rf': 2,\n",
       " 't3_17lnjrw': 13,\n",
       " 't3_17lnbeu': 26,\n",
       " 't3_17lh5hw': 6,\n",
       " 't3_17mj4b8': 4,\n",
       " 't3_17mfqi4': 7,\n",
       " 't3_17p23dl': 2,\n",
       " 't3_17ou027': 4,\n",
       " 't3_17oqjex': 6,\n",
       " 't3_17ohqb6': 10,\n",
       " 't3_17p4qj0': 10,\n",
       " 't3_17pu1mx': 3,\n",
       " 't3_17qft1f': 1,\n",
       " 't3_17qtzbc': 1,\n",
       " 't3_17ra2vj': 6,\n",
       " 't3_17rvtb2': 5,\n",
       " 't3_17rr3l7': 13,\n",
       " 't3_17u9bwi': 1,\n",
       " 't3_17u48uk': 1,\n",
       " 't3_17u42sm': 2,\n",
       " 't3_17u32my': 6,\n",
       " 't3_17v02uj': 2,\n",
       " 't3_17uy1x2': 16,\n",
       " 't3_17vtbkq': 3,\n",
       " 't3_17vp23a': 8,\n",
       " 't3_17vvc1s': 1,\n",
       " 't3_17w72zr': 15,\n",
       " 't3_17wc0dz': 2,\n",
       " 't3_17xl97l': 2,\n",
       " 't3_17xkdt5': 3,\n",
       " 't3_17xvood': 2,\n",
       " 't3_17zs58m': 1,\n",
       " 't3_17zr1qp': 4,\n",
       " 't3_17zerbl': 3,\n",
       " 't3_180fig7': 4,\n",
       " 't3_184z3yg': 1,\n",
       " 't3_184x4bc': 2,\n",
       " 't3_184ton2': 4,\n",
       " 't3_184op4l': 10,\n",
       " 't3_184nv7e': 5,\n",
       " 't3_184lnct': 4,\n",
       " 't3_184kpv0': 13,\n",
       " 't3_1854pgl': 5,\n",
       " 't3_1856c40': 1,\n",
       " 't3_185uu2f': 2,\n",
       " 't3_1863yky': 1,\n",
       " 't3_186vyi7': 5,\n",
       " 't3_186tg8k': 1,\n",
       " 't3_187e2d9': 1,\n",
       " 't3_18axgd2': 1,\n",
       " 't3_18asc6v': 5,\n",
       " 't3_18al9it': 10,\n",
       " 't3_18ajcq0': 12,\n",
       " 't3_18fveko': 13,\n",
       " 't3_18flxg9': 12,\n",
       " 't3_18fj34y': 10,\n",
       " 't3_18fh4ws': 6,\n",
       " 't3_18fg6xi': 2,\n",
       " 't3_18ff4a2': 48,\n",
       " 't3_18f9ds2': 43,\n",
       " 't3_18gkpyi': 1,\n",
       " 't3_18tvxp1': 4,\n",
       " 't3_18ttsxk': 15,\n",
       " 't3_18trbo5': 7,\n",
       " 't3_18tbjcs': 19,\n",
       " 't3_18taqx6': 18,\n",
       " 't3_18t9hkr': 5,\n",
       " 't3_18sii53': 13,\n",
       " 't3_18shx12': 3,\n",
       " 't3_18sha15': 15,\n",
       " 't3_18sf67d': 18,\n",
       " 't3_18s4t1o': 2,\n",
       " 't3_18rrnnn': 12,\n",
       " 't3_18rr59a': 7,\n",
       " 't3_18rpavt': 8,\n",
       " 't3_18rmjyn': 5,\n",
       " 't3_18rlrxg': 7,\n",
       " 't3_18rkcrr': 100,\n",
       " 't3_18ria2d': 4,\n",
       " 't3_18ri17l': 10,\n",
       " 't3_18xmerw': 2,\n",
       " 't3_18xlalb': 5,\n",
       " 't3_18wz6o9': 18,\n",
       " 't3_18wvtbt': 31,\n",
       " 't3_18wq670': 14,\n",
       " 't3_18whr29': 11,\n",
       " 't3_18xv207': 8,\n",
       " 't3_18xs6pl': 6,\n",
       " 't3_18z296s': 9,\n",
       " 't3_18yqr3v': 11,\n",
       " 't3_18zal4f': 4,\n",
       " 't3_190xa92': 1,\n",
       " 't3_190r6sq': 17,\n",
       " 't3_190k29n': 15,\n",
       " 't3_191967a': 16,\n",
       " 't3_192kvhk': 15,\n",
       " 't3_191xiyx': 7,\n",
       " 't3_193ut5c': 1,\n",
       " 't3_193oeil': 2,\n",
       " 't3_193g094': 10,\n",
       " 't3_1946y1u': 1,\n",
       " 't3_194ouwp': 1,\n",
       " 't3_1955iyi': 1,\n",
       " 't3_195pqi7': 5,\n",
       " 't3_196nm7f': 4,\n",
       " 't3_196f2un': 8,\n",
       " 't3_198k8di': 8,\n",
       " 't3_198a0ow': 18,\n",
       " 't3_19843n7': 32,\n",
       " 't3_1982yvq': 17,\n",
       " 't3_199lfzu': 3,\n",
       " 't3_199f5h8': 8,\n",
       " 't3_199vtyn': 10,\n",
       " 't3_19cja6q': 8,\n",
       " 't3_19bjliq': 17,\n",
       " 't3_19bepp6': 8,\n",
       " 't3_19beivz': 18,\n",
       " 't3_19b0v7j': 1,\n",
       " 't3_19eyno5': 20,\n",
       " 't3_19evzih': 6,\n",
       " 't3_19evfme': 20,\n",
       " 't3_19evcoa': 8,\n",
       " 't3_19eudqg': 17,\n",
       " 't3_1abafec': 6,\n",
       " 't3_1abac27': 2,\n",
       " 't3_1aco1c4': 6,\n",
       " 't3_1acdu8v': 15,\n",
       " 't3_1acbk7z': 6,\n",
       " 't3_1adkg2e': 7,\n",
       " 't3_1adjykm': 3,\n",
       " 't3_1afm8j1': 2,\n",
       " 't3_1aezxl0': 20,\n",
       " 't3_1aesmhl': 4,\n",
       " 't3_1ah4riy': 3,\n",
       " 't3_1agxnno': 3,\n",
       " 't3_1agv9xk': 3,\n",
       " 't3_1ahw8bb': 6,\n",
       " 't3_1ahnaml': 18,\n",
       " 't3_1ajbqt2': 3,\n",
       " 't3_1ajb6zi': 14,\n",
       " 't3_1aj1kcj': 48,\n",
       " 't3_1ajpb61': 1,\n",
       " 't3_1ajzzan': 5,\n",
       " 't3_1ajxapl': 20,\n",
       " 't3_1al2qd9': 1,\n",
       " 't3_1akx601': 1,\n",
       " 't3_1aloq6s': 6,\n",
       " 't3_1alnhnv': 3,\n",
       " 't3_1ampez5': 12,\n",
       " 't3_1amowbe': 1,\n",
       " 't3_1ap3bt3': 3,\n",
       " 't3_1ap2t5q': 3,\n",
       " 't3_1aosk8s': 3,\n",
       " 't3_1aorbc0': 17,\n",
       " 't3_1apu81e': 1,\n",
       " 't3_1apsoeh': 1,\n",
       " 't3_1aqmruh': 5,\n",
       " 't3_1aqfhx4': 3,\n",
       " 't3_1aus6z9': 1,\n",
       " 't3_1aupjmc': 7,\n",
       " 't3_1aucdoj': 20,\n",
       " 't3_1au73g1': 8,\n",
       " 't3_1atz9sc': 18,\n",
       " 't3_1atycal': 27,\n",
       " 't3_1avjz5g': 10,\n",
       " 't3_1avj5ag': 5,\n",
       " 't3_1avrduy': 6,\n",
       " 't3_1aw92b1': 1,\n",
       " 't3_1awjt9k': 2,\n",
       " 't3_1axee5v': 2,\n",
       " 't3_1axcch5': 7,\n",
       " 't3_1axres4': 11,\n",
       " 't3_1ayeafw': 29,\n",
       " 't3_1b0e9rc': 6,\n",
       " 't3_1b08l8n': 24,\n",
       " 't3_1b05ndk': 16,\n",
       " 't3_1b008z3': 4,\n",
       " 't3_1b114ek': 55,\n",
       " 't3_1b10k7e': 16,\n",
       " 't3_1b40jbd': 2,\n",
       " 't3_1b3q8pk': 8,\n",
       " 't3_1b3m8ax': 9,\n",
       " 't3_1b3ha3q': 10,\n",
       " 't3_1b3dll6': 16,\n",
       " 't3_1b6f31p': 1,\n",
       " 't3_1b6560d': 10,\n",
       " 't3_1b5o9sb': 4,\n",
       " 't3_1b5aepu': 12,\n",
       " 't3_1b6xd9l': 7,\n",
       " 't3_1b6sz3z': 20,\n",
       " 't3_1b7r86h': 7,\n",
       " 't3_1b7n0jo': 30,\n",
       " 't3_1b95u2a': 1,\n",
       " 't3_1b91wdi': 10}"
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_len_dict={}\n",
    "\n",
    "for id in all_reddit_ids:\n",
    "    thread = construct_thread.ConsrtuctThread(df_subreddit, id)\n",
    "    thread_len_dict.update({id : len(thread.get_thread())})\n",
    "\n",
    "thread_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "7a34db13-c90e-4453-b247-a18421025c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_len=[]\n",
    "\n",
    "for id in all_reddit_ids:\n",
    "    thread = construct_thread.ConsrtuctThread(df_subreddit, id)\n",
    "    thread_len.append([id, len(thread.get_thread())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "id": "bda10825-9091-4bd9-9d48-90a1aff665d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t3_zxspyw', 1],\n",
       " ['t3_znjz7f', 1],\n",
       " ['t3_z9cfmt', 1],\n",
       " ['t3_w6md2b', 1],\n",
       " ['t3_ugbg5l', 1],\n",
       " ['t3_udczwq', 1],\n",
       " ['t3_u9gfjv', 1],\n",
       " ['t3_tginwj', 1],\n",
       " ['t3_tfdswx', 1],\n",
       " ['t3_q3jp1k', 1],\n",
       " ['t3_q1jumh', 1],\n",
       " ['t3_pz2gf4', 1],\n",
       " ['t3_ptpa4y', 1],\n",
       " ['t3_p5w70t', 1],\n",
       " ['t3_oicyxd', 1],\n",
       " ['t3_of5f1k', 1],\n",
       " ['t3_o55u3j', 1],\n",
       " ['t3_nw7y6z', 1],\n",
       " ['t3_mz4g6j', 1],\n",
       " ['t3_mfeb7f', 1],\n",
       " ['t3_lwgv4o', 1],\n",
       " ['t3_lsmfjh', 1],\n",
       " ['t3_low5bv', 1],\n",
       " ['t3_lhngdx', 1],\n",
       " ['t3_lhlpyv', 1],\n",
       " ['t3_lhb7ho', 1],\n",
       " ['t3_lg5of6', 1],\n",
       " ['t3_letwhq', 1],\n",
       " ['t3_lesmde', 1],\n",
       " ['t3_les77v', 1],\n",
       " ['t3_kchxrc', 1],\n",
       " ['t3_k2a20b', 1],\n",
       " ['t3_k18u9o', 1],\n",
       " ['t3_jdgiw1', 1],\n",
       " ['t3_iubdak', 1],\n",
       " ['t3_ib0drx', 1],\n",
       " ['t3_hyzv8a', 1],\n",
       " ['t3_hygauo', 1],\n",
       " ['t3_gin14r', 1],\n",
       " ['t3_g9xfim', 1],\n",
       " ['t3_g1xznw', 1],\n",
       " ['t3_g1upnx', 1],\n",
       " ['t3_134bzme', 1],\n",
       " ['t3_1365mo9', 1],\n",
       " ['t3_13c0ls4', 1],\n",
       " ['t3_13dyobj', 1],\n",
       " ['t3_13fq098', 1],\n",
       " ['t3_13kxky3', 1],\n",
       " ['t3_13ngkrz', 1],\n",
       " ['t3_140k1b8', 1],\n",
       " ['t3_141be7b', 1],\n",
       " ['t3_141fcm8', 1],\n",
       " ['t3_142qczh', 1],\n",
       " ['t3_147wb7o', 1],\n",
       " ['t3_14a1omt', 1],\n",
       " ['t3_14acmyy', 1],\n",
       " ['t3_14ayug0', 1],\n",
       " ['t3_14bvaot', 1],\n",
       " ['t3_14contk', 1],\n",
       " ['t3_14f51u0', 1],\n",
       " ['t3_14fa7rw', 1],\n",
       " ['t3_14ozrzy', 1],\n",
       " ['t3_14pu6n9', 1],\n",
       " ['t3_14qf9j1', 1],\n",
       " ['t3_14rbjqj', 1],\n",
       " ['t3_1524ebz', 1],\n",
       " ['t3_151sguc', 1],\n",
       " ['t3_152dtlv', 1],\n",
       " ['t3_154021b', 1],\n",
       " ['t3_155sxwz', 1],\n",
       " ['t3_1562ktm', 1],\n",
       " ['t3_15bv42o', 1],\n",
       " ['t3_15bql24', 1],\n",
       " ['t3_15g91qa', 1],\n",
       " ['t3_15gd65a', 1],\n",
       " ['t3_15ji8tx', 1],\n",
       " ['t3_15llwvl', 1],\n",
       " ['t3_15qxpq6', 1],\n",
       " ['t3_15qw6fe', 1],\n",
       " ['t3_15uvfps', 1],\n",
       " ['t3_15vfuy5', 1],\n",
       " ['t3_15xt7tl', 1],\n",
       " ['t3_163ly9b', 1],\n",
       " ['t3_163pvog', 1],\n",
       " ['t3_164c3d5', 1],\n",
       " ['t3_16gr6yw', 1],\n",
       " ['t3_16gzemq', 1],\n",
       " ['t3_16qxgr3', 1],\n",
       " ['t3_16xtxc6', 1],\n",
       " ['t3_176si7n', 1],\n",
       " ['t3_17erw6z', 1],\n",
       " ['t3_17qft1f', 1],\n",
       " ['t3_17qtzbc', 1],\n",
       " ['t3_17u9bwi', 1],\n",
       " ['t3_17u48uk', 1],\n",
       " ['t3_17vvc1s', 1],\n",
       " ['t3_17zs58m', 1],\n",
       " ['t3_184z3yg', 1],\n",
       " ['t3_1856c40', 1],\n",
       " ['t3_1863yky', 1],\n",
       " ['t3_186tg8k', 1],\n",
       " ['t3_187e2d9', 1],\n",
       " ['t3_18axgd2', 1],\n",
       " ['t3_18gkpyi', 1],\n",
       " ['t3_190xa92', 1],\n",
       " ['t3_193ut5c', 1],\n",
       " ['t3_1946y1u', 1],\n",
       " ['t3_194ouwp', 1],\n",
       " ['t3_1955iyi', 1],\n",
       " ['t3_19b0v7j', 1],\n",
       " ['t3_1ajpb61', 1],\n",
       " ['t3_1al2qd9', 1],\n",
       " ['t3_1akx601', 1],\n",
       " ['t3_1amowbe', 1],\n",
       " ['t3_1apu81e', 1],\n",
       " ['t3_1apsoeh', 1],\n",
       " ['t3_1aus6z9', 1],\n",
       " ['t3_1aw92b1', 1],\n",
       " ['t3_1b6f31p', 1],\n",
       " ['t3_1b95u2a', 1],\n",
       " ['t3_11ew3fs', 2],\n",
       " ['t3_117t418', 2],\n",
       " ['t3_10jui63', 2],\n",
       " ['t3_10gllu9', 2],\n",
       " ['t3_zyq49u', 2],\n",
       " ['t3_ztu253', 2],\n",
       " ['t3_zrrenz', 2],\n",
       " ['t3_zfknoy', 2],\n",
       " ['t3_zcqnma', 2],\n",
       " ['t3_yxac0a', 2],\n",
       " ['t3_yub7qv', 2],\n",
       " ['t3_ymq1zi', 2],\n",
       " ['t3_ydn44q', 2],\n",
       " ['t3_y0hlyc', 2],\n",
       " ['t3_xcmb3p', 2],\n",
       " ['t3_wylhu9', 2],\n",
       " ['t3_wey7mh', 2],\n",
       " ['t3_uyokbi', 2],\n",
       " ['t3_ueb255', 2],\n",
       " ['t3_teamht', 2],\n",
       " ['t3_tdejak', 2],\n",
       " ['t3_td2nwr', 2],\n",
       " ['t3_sb7lwq', 2],\n",
       " ['t3_s80fuy', 2],\n",
       " ['t3_qwu04w', 2],\n",
       " ['t3_qqysf1', 2],\n",
       " ['t3_qp4czc', 2],\n",
       " ['t3_qkcr6h', 2],\n",
       " ['t3_qieejj', 2],\n",
       " ['t3_qeroej', 2],\n",
       " ['t3_q1zf00', 2],\n",
       " ['t3_pl0yqk', 2],\n",
       " ['t3_pdbmth', 2],\n",
       " ['t3_oyzq2w', 2],\n",
       " ['t3_ourgi4', 2],\n",
       " ['t3_mud11u', 2],\n",
       " ['t3_ktqdd1', 2],\n",
       " ['t3_kqtsws', 2],\n",
       " ['t3_kacts6', 2],\n",
       " ['t3_j3pm4i', 2],\n",
       " ['t3_it7img', 2],\n",
       " ['t3_io665d', 2],\n",
       " ['t3_i8i9p0', 2],\n",
       " ['t3_gtgu7v', 2],\n",
       " ['t3_g1j93a', 2],\n",
       " ['t3_131g4hk', 2],\n",
       " ['t3_1371lok', 2],\n",
       " ['t3_13bbmyk', 2],\n",
       " ['t3_13dhtj6', 2],\n",
       " ['t3_13f4xst', 2],\n",
       " ['t3_13gm5y5', 2],\n",
       " ['t3_140gz27', 2],\n",
       " ['t3_13zvahh', 2],\n",
       " ['t3_141inu6', 2],\n",
       " ['t3_143yid0', 2],\n",
       " ['t3_145a8fl', 2],\n",
       " ['t3_14lrnbf', 2],\n",
       " ['t3_151fdrl', 2],\n",
       " ['t3_153ukht', 2],\n",
       " ['t3_155mqqj', 2],\n",
       " ['t3_155qfus', 2],\n",
       " ['t3_158kw3d', 2],\n",
       " ['t3_159k65p', 2],\n",
       " ['t3_15k9xjn', 2],\n",
       " ['t3_15k27l2', 2],\n",
       " ['t3_15llbwd', 2],\n",
       " ['t3_15lojxb', 2],\n",
       " ['t3_15u9r2c', 2],\n",
       " ['t3_15undf7', 2],\n",
       " ['t3_163c6jo', 2],\n",
       " ['t3_163516u', 2],\n",
       " ['t3_166yzb4', 2],\n",
       " ['t3_16agd8r', 2],\n",
       " ['t3_16a4p09', 2],\n",
       " ['t3_16b5ojs', 2],\n",
       " ['t3_16s15xr', 2],\n",
       " ['t3_16sjzfc', 2],\n",
       " ['t3_16tgwnq', 2],\n",
       " ['t3_16tbzpv', 2],\n",
       " ['t3_1706arf', 2],\n",
       " ['t3_170i31u', 2],\n",
       " ['t3_172rufs', 2],\n",
       " ['t3_174zpmr', 2],\n",
       " ['t3_179tf75', 2],\n",
       " ['t3_179z965', 2],\n",
       " ['t3_17afl2d', 2],\n",
       " ['t3_17ec504', 2],\n",
       " ['t3_17hevu5', 2],\n",
       " ['t3_17k03ol', 2],\n",
       " ['t3_17jm7rf', 2],\n",
       " ['t3_17p23dl', 2],\n",
       " ['t3_17u42sm', 2],\n",
       " ['t3_17v02uj', 2],\n",
       " ['t3_17wc0dz', 2],\n",
       " ['t3_17xl97l', 2],\n",
       " ['t3_17xvood', 2],\n",
       " ['t3_184x4bc', 2],\n",
       " ['t3_185uu2f', 2],\n",
       " ['t3_18fg6xi', 2],\n",
       " ['t3_18s4t1o', 2],\n",
       " ['t3_18xmerw', 2],\n",
       " ['t3_193oeil', 2],\n",
       " ['t3_1abac27', 2],\n",
       " ['t3_1afm8j1', 2],\n",
       " ['t3_1awjt9k', 2],\n",
       " ['t3_1axee5v', 2],\n",
       " ['t3_1b40jbd', 2],\n",
       " ['t3_12opsul', 3],\n",
       " ['t3_12ectgf', 3],\n",
       " ['t3_11wk8fh', 3],\n",
       " ['t3_11qb2k2', 3],\n",
       " ['t3_114sq9a', 3],\n",
       " ['t3_10d1dj8', 3],\n",
       " ['t3_zzx2wq', 3],\n",
       " ['t3_zxia62', 3],\n",
       " ['t3_zut2tr', 3],\n",
       " ['t3_zqi52d', 3],\n",
       " ['t3_zj1p6d', 3],\n",
       " ['t3_z44t79', 3],\n",
       " ['t3_z1iupm', 3],\n",
       " ['t3_yzicds', 3],\n",
       " ['t3_yo7rpy', 3],\n",
       " ['t3_yae71u', 3],\n",
       " ['t3_y3ewha', 3],\n",
       " ['t3_xwbtt4', 3],\n",
       " ['t3_xu6znu', 3],\n",
       " ['t3_xrazhv', 3],\n",
       " ['t3_wniz1j', 3],\n",
       " ['t3_sv7wub', 3],\n",
       " ['t3_spsus3', 3],\n",
       " ['t3_si5q4d', 3],\n",
       " ['t3_r6v772', 3],\n",
       " ['t3_r3bzg2', 3],\n",
       " ['t3_qso748', 3],\n",
       " ['t3_qoaeba', 3],\n",
       " ['t3_qo85m9', 3],\n",
       " ['t3_qi4wrh', 3],\n",
       " ['t3_qgxqdr', 3],\n",
       " ['t3_pyykcv', 3],\n",
       " ['t3_ppvrs6', 3],\n",
       " ['t3_poxhx7', 3],\n",
       " ['t3_p953t1', 3],\n",
       " ['t3_p3aot6', 3],\n",
       " ['t3_ozadh6', 3],\n",
       " ['t3_o8lx12', 3],\n",
       " ['t3_mxjjiu', 3],\n",
       " ['t3_mvautv', 3],\n",
       " ['t3_lsk11y', 3],\n",
       " ['t3_l9l2ac', 3],\n",
       " ['t3_kt7dp9', 3],\n",
       " ['t3_kkpk0q', 3],\n",
       " ['t3_k4b7mt', 3],\n",
       " ['t3_jzib0w', 3],\n",
       " ['t3_jx68il', 3],\n",
       " ['t3_jw78lp', 3],\n",
       " ['t3_jv7r7b', 3],\n",
       " ['t3_jo8di6', 3],\n",
       " ['t3_jo4kbh', 3],\n",
       " ['t3_jkc0sq', 3],\n",
       " ['t3_j2aewq', 3],\n",
       " ['t3_ixq5nx', 3],\n",
       " ['t3_iubp8g', 3],\n",
       " ['t3_itpfzm', 3],\n",
       " ['t3_i5e8z7', 3],\n",
       " ['t3_i1p9b9', 3],\n",
       " ['t3_hyscsg', 3],\n",
       " ['t3_g9ackt', 3],\n",
       " ['t3_g1ipnr', 3],\n",
       " ['t3_12vpyl1', 3],\n",
       " ['t3_1307myr', 3],\n",
       " ['t3_138xt52', 3],\n",
       " ['t3_13beb91', 3],\n",
       " ['t3_1456sxm', 3],\n",
       " ['t3_1488pwc', 3],\n",
       " ['t3_14h75j6', 3],\n",
       " ['t3_14htsv6', 3],\n",
       " ['t3_15427rb', 3],\n",
       " ['t3_15a83ex', 3],\n",
       " ['t3_15edznr', 3],\n",
       " ['t3_15gjq3x', 3],\n",
       " ['t3_15h9cil', 3],\n",
       " ['t3_15m9int', 3],\n",
       " ['t3_15mgqsl', 3],\n",
       " ['t3_15u9jff', 3],\n",
       " ['t3_15zvejv', 3],\n",
       " ['t3_166098f', 3],\n",
       " ['t3_16acqhk', 3],\n",
       " ['t3_16jgp0r', 3],\n",
       " ['t3_16lm5rm', 3],\n",
       " ['t3_16m5r96', 3],\n",
       " ['t3_16qy9to', 3],\n",
       " ['t3_16xmkq5', 3],\n",
       " ['t3_16zfhm3', 3],\n",
       " ['t3_1709kko', 3],\n",
       " ['t3_1711zc9', 3],\n",
       " ['t3_172op5n', 3],\n",
       " ['t3_1799ef9', 3],\n",
       " ['t3_17aav23', 3],\n",
       " ['t3_17beebi', 3],\n",
       " ['t3_17c47xn', 3],\n",
       " ['t3_17bt0f4', 3],\n",
       " ['t3_17g3her', 3],\n",
       " ['t3_17g2qpo', 3],\n",
       " ['t3_17hu8la', 3],\n",
       " ['t3_17jw7cz', 3],\n",
       " ['t3_17pu1mx', 3],\n",
       " ['t3_17vtbkq', 3],\n",
       " ['t3_17xkdt5', 3],\n",
       " ['t3_17zerbl', 3],\n",
       " ['t3_18shx12', 3],\n",
       " ['t3_199lfzu', 3],\n",
       " ['t3_1adjykm', 3],\n",
       " ['t3_1ah4riy', 3],\n",
       " ['t3_1agxnno', 3],\n",
       " ['t3_1agv9xk', 3],\n",
       " ['t3_1ajbqt2', 3],\n",
       " ['t3_1alnhnv', 3],\n",
       " ['t3_1ap3bt3', 3],\n",
       " ['t3_1ap2t5q', 3],\n",
       " ['t3_1aosk8s', 3],\n",
       " ['t3_1aqfhx4', 3],\n",
       " ['t3_119at7w', 4],\n",
       " ['t3_10fmkvw', 4],\n",
       " ['t3_z6ve99', 4],\n",
       " ['t3_y95bo3', 4],\n",
       " ['t3_xl74rb', 4],\n",
       " ['t3_xcctn9', 4],\n",
       " ['t3_wxixwd', 4],\n",
       " ['t3_vxega2', 4],\n",
       " ['t3_vu0jn5', 4],\n",
       " ['t3_vq2463', 4],\n",
       " ['t3_vl7x5z', 4],\n",
       " ['t3_vchd5h', 4],\n",
       " ['t3_v5ngr7', 4],\n",
       " ['t3_u9gjw5', 4],\n",
       " ['t3_tzf806', 4],\n",
       " ['t3_trfy8g', 4],\n",
       " ['t3_sygsv3', 4],\n",
       " ['t3_s7vrdv', 4],\n",
       " ['t3_rxnc8a', 4],\n",
       " ['t3_resym9', 4],\n",
       " ['t3_q6jfmp', 4],\n",
       " ['t3_q3du8t', 4],\n",
       " ['t3_pnr0ls', 4],\n",
       " ['t3_pl1q4b', 4],\n",
       " ['t3_o4fkm6', 4],\n",
       " ['t3_o1czhu', 4],\n",
       " ['t3_o0ol55', 4],\n",
       " ['t3_mpq92y', 4],\n",
       " ['t3_m4xdo7', 4],\n",
       " ['t3_lk0m34', 4],\n",
       " ['t3_khxev9', 4],\n",
       " ['t3_jod3zh', 4],\n",
       " ['t3_jf2d05', 4],\n",
       " ['t3_jezfvk', 4],\n",
       " ['t3_j5vbke', 4],\n",
       " ['t3_i6yj5y', 4],\n",
       " ['t3_hwbdxn', 4],\n",
       " ['t3_12rblul', 4],\n",
       " ['t3_12ukjb7', 4],\n",
       " ['t3_136ivb4', 4],\n",
       " ['t3_13phox6', 4],\n",
       " ['t3_1401z1l', 4],\n",
       " ['t3_147p5ym', 4],\n",
       " ['t3_14kfqfw', 4],\n",
       " ['t3_14r2li6', 4],\n",
       " ['t3_14tji8u', 4],\n",
       " ['t3_159fm06', 4],\n",
       " ['t3_15hqpe9', 4],\n",
       " ['t3_15i9vl8', 4],\n",
       " ['t3_15qmnlg', 4],\n",
       " ['t3_15w5del', 4],\n",
       " ['t3_166h8m2', 4],\n",
       " ['t3_16ng0ma', 4],\n",
       " ['t3_16qkwpq', 4],\n",
       " ['t3_16rlmqn', 4],\n",
       " ['t3_16u7oy8', 4],\n",
       " ['t3_1730y1t', 4],\n",
       " ['t3_17dd9oh', 4],\n",
       " ['t3_17fdtjg', 4],\n",
       " ['t3_17mj4b8', 4],\n",
       " ['t3_17ou027', 4],\n",
       " ['t3_17zr1qp', 4],\n",
       " ['t3_180fig7', 4],\n",
       " ['t3_184ton2', 4],\n",
       " ['t3_184lnct', 4],\n",
       " ['t3_18tvxp1', 4],\n",
       " ['t3_18ria2d', 4],\n",
       " ['t3_18zal4f', 4],\n",
       " ['t3_196nm7f', 4],\n",
       " ['t3_1aesmhl', 4],\n",
       " ['t3_1b008z3', 4],\n",
       " ['t3_1b5o9sb', 4],\n",
       " ['t3_12g25g2', 5],\n",
       " ['t3_11dxaol', 5],\n",
       " ['t3_111y6s8', 5],\n",
       " ['t3_zzj0lf', 5],\n",
       " ['t3_zxgmp4', 5],\n",
       " ['t3_zwzlqi', 5],\n",
       " ['t3_zoqq39', 5],\n",
       " ['t3_ywjf2p', 5],\n",
       " ['t3_vv304y', 5],\n",
       " ['t3_uyhtwa', 5],\n",
       " ['t3_t1if6w', 5],\n",
       " ['t3_quy853', 5],\n",
       " ['t3_qmrvlh', 5],\n",
       " ['t3_pdjgpi', 5],\n",
       " ['t3_oo5tzv', 5],\n",
       " ['t3_ongml1', 5],\n",
       " ['t3_nwkgsw', 5],\n",
       " ['t3_nndsf2', 5],\n",
       " ['t3_knxrpq', 5],\n",
       " ['t3_klk0z1', 5],\n",
       " ['t3_jxziau', 5],\n",
       " ['t3_jwlmpz', 5],\n",
       " ['t3_jsiasq', 5],\n",
       " ['t3_jsb4fa', 5],\n",
       " ['t3_jpi19f', 5],\n",
       " ['t3_joktym', 5],\n",
       " ['t3_jo19ko', 5],\n",
       " ['t3_jmxejg', 5],\n",
       " ['t3_jdiksx', 5],\n",
       " ['t3_iyk0me', 5],\n",
       " ['t3_ipfos1', 5],\n",
       " ['t3_hg8g80', 5],\n",
       " ['t3_12vlk9u', 5],\n",
       " ['t3_12wxirw', 5],\n",
       " ['t3_12woa5a', 5],\n",
       " ['t3_139rims', 5],\n",
       " ['t3_13cdeph', 5],\n",
       " ['t3_13evaou', 5],\n",
       " ['t3_1402nxm', 5],\n",
       " ['t3_142l7gq', 5],\n",
       " ['t3_1433lms', 5],\n",
       " ['t3_154uy96', 5],\n",
       " ['t3_15gb9ch', 5],\n",
       " ['t3_15ix5ck', 5],\n",
       " ['t3_15qlois', 5],\n",
       " ['t3_15t3heu', 5],\n",
       " ['t3_16acd96', 5],\n",
       " ['t3_16i9r6w', 5],\n",
       " ['t3_16k2v8f', 5],\n",
       " ['t3_16mtxcj', 5],\n",
       " ['t3_16tnb4l', 5],\n",
       " ['t3_16z9f83', 5],\n",
       " ['t3_173incm', 5],\n",
       " ['t3_176khx7', 5],\n",
       " ['t3_17hcd8v', 5],\n",
       " ['t3_17rvtb2', 5],\n",
       " ['t3_184nv7e', 5],\n",
       " ['t3_1854pgl', 5],\n",
       " ['t3_186vyi7', 5],\n",
       " ['t3_18asc6v', 5],\n",
       " ['t3_18t9hkr', 5],\n",
       " ['t3_18rmjyn', 5],\n",
       " ['t3_18xlalb', 5],\n",
       " ['t3_195pqi7', 5],\n",
       " ['t3_1ajzzan', 5],\n",
       " ['t3_1aqmruh', 5],\n",
       " ['t3_1avj5ag', 5],\n",
       " ['t3_11fb73q', 6],\n",
       " ['t3_11bgct9', 6],\n",
       " ['t3_10711zn', 6],\n",
       " ['t3_yj60zs', 6],\n",
       " ['t3_yf030k', 6],\n",
       " ['t3_xtqoo8', 6],\n",
       " ['t3_xj7tu0', 6],\n",
       " ['t3_x2b57k', 6],\n",
       " ['t3_x0qutw', 6],\n",
       " ['t3_wyruo5', 6],\n",
       " ['t3_vlfo5s', 6],\n",
       " ['t3_vd0kfl', 6],\n",
       " ['t3_u9ubpe', 6],\n",
       " ['t3_tqcpc5', 6],\n",
       " ['t3_t4i7f9', 6],\n",
       " ['t3_t3mcbk', 6],\n",
       " ['t3_spl527', 6],\n",
       " ['t3_sj3cnq', 6],\n",
       " ['t3_r4f01w', 6],\n",
       " ['t3_qz3l4m', 6],\n",
       " ['t3_qty7j8', 6],\n",
       " ['t3_q1fhwu', 6],\n",
       " ['t3_pebp8a', 6],\n",
       " ['t3_o75ftk', 6],\n",
       " ['t3_nq49o1', 6],\n",
       " ['t3_ma92by', 6],\n",
       " ['t3_lu2yq7', 6],\n",
       " ['t3_ltjksq', 6],\n",
       " ['t3_lh82ay', 6],\n",
       " ['t3_km802q', 6],\n",
       " ['t3_k2mp32', 6],\n",
       " ['t3_jx0q4y', 6],\n",
       " ['t3_jwayfc', 6],\n",
       " ['t3_jr56wa', 6],\n",
       " ['t3_jh2ws3', 6],\n",
       " ['t3_j0wau6', 6],\n",
       " ['t3_j0gpr9', 6],\n",
       " ['t3_itkyfs', 6],\n",
       " ['t3_gwyedp', 6],\n",
       " ['t3_12un85y', 6],\n",
       " ['t3_13hop0a', 6],\n",
       " ['t3_13lv3v7', 6],\n",
       " ['t3_13nu7mm', 6],\n",
       " ['t3_148yc0q', 6],\n",
       " ['t3_14fp7ld', 6],\n",
       " ['t3_14rpejz', 6],\n",
       " ['t3_15i5w6j', 6],\n",
       " ['t3_15z4pv7', 6],\n",
       " ['t3_15ysn49', 6],\n",
       " ['t3_165uvo6', 6],\n",
       " ['t3_165utsm', 6],\n",
       " ['t3_16fwh09', 6],\n",
       " ['t3_16j3lhl', 6],\n",
       " ['t3_16llnsh', 6],\n",
       " ['t3_16o66as', 6],\n",
       " ['t3_16x9tat', 6],\n",
       " ['t3_17hl1aj', 6],\n",
       " ['t3_17lh5hw', 6],\n",
       " ['t3_17oqjex', 6],\n",
       " ['t3_17ra2vj', 6],\n",
       " ['t3_17u32my', 6],\n",
       " ['t3_18fh4ws', 6],\n",
       " ['t3_18xs6pl', 6],\n",
       " ['t3_19evzih', 6],\n",
       " ['t3_1abafec', 6],\n",
       " ['t3_1aco1c4', 6],\n",
       " ['t3_1acbk7z', 6],\n",
       " ['t3_1ahw8bb', 6],\n",
       " ['t3_1aloq6s', 6],\n",
       " ['t3_1avrduy', 6],\n",
       " ['t3_1b0e9rc', 6],\n",
       " ['t3_1157t7i', 7],\n",
       " ['t3_10ljawj', 7],\n",
       " ['t3_10brc4f', 7],\n",
       " ['t3_yw5weg', 7],\n",
       " ['t3_ydy8u2', 7],\n",
       " ['t3_u2fy38', 7],\n",
       " ['t3_u05hnv', 7],\n",
       " ['t3_tdyte4', 7],\n",
       " ['t3_rehv42', 7],\n",
       " ['t3_r5dkhb', 7],\n",
       " ['t3_r3z17r', 7],\n",
       " ['t3_qpi3rj', 7],\n",
       " ['t3_qkqiaw', 7],\n",
       " ['t3_psiueg', 7],\n",
       " ['t3_mesrs0', 7],\n",
       " ['t3_lm7x0c', 7],\n",
       " ['t3_kfzs8n', 7],\n",
       " ['t3_k7zxab', 7],\n",
       " ['t3_g1skrg', 7],\n",
       " ['t3_12r8ob9', 7],\n",
       " ['t3_12scwfu', 7],\n",
       " ['t3_13jrye3', 7],\n",
       " ['t3_13k5bg4', 7],\n",
       " ['t3_13zxtx2', 7],\n",
       " ['t3_14giqiy', 7],\n",
       " ['t3_14rsuxq', 7],\n",
       " ['t3_151soos', 7],\n",
       " ['t3_151rb0o', 7],\n",
       " ['t3_151ggsb', 7],\n",
       " ['t3_15bywxg', 7],\n",
       " ['t3_15x4pfj', 7],\n",
       " ['t3_163boli', 7],\n",
       " ['t3_16yg035', 7],\n",
       " ['t3_174wtq6', 7],\n",
       " ['t3_17mfqi4', 7],\n",
       " ['t3_18trbo5', 7],\n",
       " ['t3_18rr59a', 7],\n",
       " ['t3_18rlrxg', 7],\n",
       " ['t3_191xiyx', 7],\n",
       " ['t3_1adkg2e', 7],\n",
       " ['t3_1aupjmc', 7],\n",
       " ['t3_1axcch5', 7],\n",
       " ['t3_1b6xd9l', 7],\n",
       " ['t3_1b7r86h', 7],\n",
       " ['t3_103bmqk', 8],\n",
       " ['t3_yy085z', 8],\n",
       " ['t3_y1psff', 8],\n",
       " ['t3_s55io5', 8],\n",
       " ['t3_qujmiu', 8],\n",
       " ['t3_q1fg8q', 8],\n",
       " ['t3_px8g4w', 8],\n",
       " ['t3_plkh35', 8],\n",
       " ['t3_p5u2zs', 8],\n",
       " ['t3_mkdqfb', 8],\n",
       " ['t3_mc9gtw', 8],\n",
       " ['t3_ldg74q', 8],\n",
       " ['t3_kdngf0', 8],\n",
       " ['t3_k127qt', 8],\n",
       " ['t3_jom7lh', 8],\n",
       " ['t3_13qwi0k', 8],\n",
       " ['t3_14bnt21', 8],\n",
       " ['t3_1525wxg', 8],\n",
       " ['t3_176mkmh', 8],\n",
       " ['t3_17b9nci', 8],\n",
       " ['t3_17vp23a', 8],\n",
       " ['t3_18rpavt', 8],\n",
       " ['t3_18xv207', 8],\n",
       " ['t3_196f2un', 8],\n",
       " ['t3_198k8di', 8],\n",
       " ['t3_199f5h8', 8],\n",
       " ['t3_19cja6q', 8],\n",
       " ['t3_19bepp6', 8],\n",
       " ['t3_19evcoa', 8],\n",
       " ['t3_1au73g1', 8],\n",
       " ['t3_1b3q8pk', 8],\n",
       " ['t3_11hyxkb', 9],\n",
       " ['t3_zh0k71', 9],\n",
       " ['t3_vj5q19', 9],\n",
       " ['t3_u3qebi', 9],\n",
       " ['t3_u2sqlk', 9],\n",
       " ['t3_nqqgpx', 9],\n",
       " ['t3_kwnyzr', 9],\n",
       " ['t3_kwmip5', 9],\n",
       " ['t3_jf4kz1', 9],\n",
       " ['t3_j15u03', 9],\n",
       " ['t3_13zsm2d', 9],\n",
       " ['t3_143ixlp', 9],\n",
       " ['t3_146p5ca', 9],\n",
       " ['t3_15av638', 9],\n",
       " ['t3_16fik5c', 9],\n",
       " ['t3_16xgjm6', 9],\n",
       " ['t3_17eebec', 9],\n",
       " ['t3_18z296s', 9],\n",
       " ['t3_1b3m8ax', 9],\n",
       " ['t3_t4locz', 10],\n",
       " ['t3_qafhhx', 10],\n",
       " ['t3_kw7yxb', 10],\n",
       " ['t3_kchdeu', 10],\n",
       " ['t3_jm5ce0', 10],\n",
       " ['t3_13poqek', 10],\n",
       " ['t3_13zqf5q', 10],\n",
       " ['t3_14lpwx9', 10],\n",
       " ['t3_14mdbfa', 10],\n",
       " ['t3_14odcg2', 10],\n",
       " ['t3_15aae6q', 10],\n",
       " ['t3_17ohqb6', 10],\n",
       " ['t3_17p4qj0', 10],\n",
       " ['t3_184op4l', 10],\n",
       " ['t3_18al9it', 10],\n",
       " ['t3_18fj34y', 10],\n",
       " ['t3_18ri17l', 10],\n",
       " ['t3_193g094', 10],\n",
       " ['t3_199vtyn', 10],\n",
       " ['t3_1avjz5g', 10],\n",
       " ['t3_1b3ha3q', 10],\n",
       " ['t3_1b6560d', 10],\n",
       " ['t3_1b91wdi', 10],\n",
       " ['t3_12du2t0', 11],\n",
       " ['t3_1142913', 11],\n",
       " ['t3_10d2uum', 11],\n",
       " ['t3_ykuv6m', 11],\n",
       " ['t3_y6s4ot', 11],\n",
       " ['t3_xcbqxx', 11],\n",
       " ['t3_x1ofw2', 11],\n",
       " ['t3_j6v9a4', 11],\n",
       " ['t3_g1jnvc', 11],\n",
       " ['t3_13hdeiv', 11],\n",
       " ['t3_13kzfia', 11],\n",
       " ['t3_13q5tph', 11],\n",
       " ['t3_1468tsl', 11],\n",
       " ['t3_14dxspe', 11],\n",
       " ['t3_156m21q', 11],\n",
       " ['t3_16i97tj', 11],\n",
       " ['t3_173u96t', 11],\n",
       " ['t3_17cqno6', 11],\n",
       " ['t3_18whr29', 11],\n",
       " ['t3_18yqr3v', 11],\n",
       " ['t3_1axres4', 11],\n",
       " ['t3_11ne756', 12],\n",
       " ['t3_10usgg4', 12],\n",
       " ['t3_yuacvb', 12],\n",
       " ['t3_wtr1q0', 12],\n",
       " ['t3_qhb9v1', 12],\n",
       " ['t3_g1h57g', 12],\n",
       " ['t3_135562t', 12],\n",
       " ['t3_14a61km', 12],\n",
       " ['t3_14d6fhg', 12],\n",
       " ['t3_14j8tr8', 12],\n",
       " ['t3_14sq8or', 12],\n",
       " ['t3_151pyeq', 12],\n",
       " ['t3_15e1vvl', 12],\n",
       " ['t3_15x93dd', 12],\n",
       " ['t3_16b9xbu', 12],\n",
       " ['t3_16mhlrm', 12],\n",
       " ['t3_17cwpja', 12],\n",
       " ['t3_18ajcq0', 12],\n",
       " ['t3_18flxg9', 12],\n",
       " ['t3_18rrnnn', 12],\n",
       " ['t3_1ampez5', 12],\n",
       " ['t3_1b5aepu', 12],\n",
       " ['t3_12gb7ps', 13],\n",
       " ['t3_10i9ao0', 13],\n",
       " ['t3_10cxfxu', 13],\n",
       " ['t3_yi0dl4', 13],\n",
       " ['t3_v5thte', 13],\n",
       " ['t3_g1glzh', 13],\n",
       " ['t3_12z00x0', 13],\n",
       " ['t3_152ejwy', 13],\n",
       " ['t3_157wono', 13],\n",
       " ['t3_15fbomq', 13],\n",
       " ['t3_178w5qv', 13],\n",
       " ['t3_17lnjrw', 13],\n",
       " ['t3_17rr3l7', 13],\n",
       " ['t3_184kpv0', 13],\n",
       " ['t3_18fveko', 13],\n",
       " ['t3_18sii53', 13],\n",
       " ['t3_10v8unp', 14],\n",
       " ['t3_wqed2b', 14],\n",
       " ['t3_158xm55', 14],\n",
       " ['t3_15r2u1w', 14],\n",
       " ['t3_15wven5', 14],\n",
       " ['t3_16j0ber', 14],\n",
       " ['t3_18wq670', 14],\n",
       " ['t3_1ajb6zi', 14],\n",
       " ['t3_12m0ozl', 15],\n",
       " ['t3_148nbff', 15],\n",
       " ['t3_14i1ppn', 15],\n",
       " ['t3_14j0scp', 15],\n",
       " ['t3_153p3b7', 15],\n",
       " ['t3_16fgugi', 15],\n",
       " ['t3_1797njv', 15],\n",
       " ['t3_17w72zr', 15],\n",
       " ['t3_18ttsxk', 15],\n",
       " ['t3_18sha15', 15],\n",
       " ['t3_190k29n', 15],\n",
       " ['t3_192kvhk', 15],\n",
       " ['t3_1acdu8v', 15],\n",
       " ['t3_z7kfmo', 16],\n",
       " ['t3_yoa9td', 16],\n",
       " ['t3_wapsy3', 16],\n",
       " ['t3_tvae2o', 16],\n",
       " ['t3_qvep5e', 16],\n",
       " ['t3_14jmrh2', 16],\n",
       " ['t3_14ngsbs', 16],\n",
       " ['t3_17uy1x2', 16],\n",
       " ['t3_191967a', 16],\n",
       " ['t3_1b05ndk', 16],\n",
       " ['t3_1b10k7e', 16],\n",
       " ['t3_1b3dll6', 16],\n",
       " ['t3_12e4wef', 17],\n",
       " ['t3_10tdmkb', 17],\n",
       " ['t3_14edki7', 17],\n",
       " ['t3_17491o2', 17],\n",
       " ['t3_190r6sq', 17],\n",
       " ['t3_1982yvq', 17],\n",
       " ['t3_19bjliq', 17],\n",
       " ['t3_19eudqg', 17],\n",
       " ['t3_1aorbc0', 17],\n",
       " ['t3_gfep79', 18],\n",
       " ['t3_18taqx6', 18],\n",
       " ['t3_18sf67d', 18],\n",
       " ['t3_18wz6o9', 18],\n",
       " ['t3_198a0ow', 18],\n",
       " ['t3_19beivz', 18],\n",
       " ['t3_1ahnaml', 18],\n",
       " ['t3_1atz9sc', 18],\n",
       " ['t3_12l3md8', 19],\n",
       " ['t3_wkca45', 19],\n",
       " ['t3_132dgjh', 19],\n",
       " ['t3_147oawn', 19],\n",
       " ['t3_14mdjt5', 19],\n",
       " ['t3_14ssvb2', 19],\n",
       " ['t3_18tbjcs', 19],\n",
       " ['t3_uihx76', 20],\n",
       " ['t3_12usy5i', 20],\n",
       " ['t3_1499vuh', 20],\n",
       " ['t3_15dbudc', 20],\n",
       " ['t3_17138re', 20],\n",
       " ['t3_179amsj', 20],\n",
       " ['t3_19eyno5', 20],\n",
       " ['t3_19evfme', 20],\n",
       " ['t3_1aezxl0', 20],\n",
       " ['t3_1ajxapl', 20],\n",
       " ['t3_1aucdoj', 20],\n",
       " ['t3_1b6sz3z', 20],\n",
       " ['t3_xcoddj', 21],\n",
       " ['t3_14n5zby', 21],\n",
       " ['t3_lwjpin', 22],\n",
       " ['t3_136z3ru', 22],\n",
       " ['t3_16ha775', 23],\n",
       " ['t3_16qk2n0', 23],\n",
       " ['t3_omfjc4', 24],\n",
       " ['t3_1b08l8n', 24],\n",
       " ['t3_17lnbeu', 26],\n",
       " ['t3_16f2cj3', 27],\n",
       " ['t3_16o2ggz', 27],\n",
       " ['t3_1atycal', 27],\n",
       " ['t3_1580byk', 28],\n",
       " ['t3_132bzni', 29],\n",
       " ['t3_1ayeafw', 29],\n",
       " ['t3_1b7n0jo', 30],\n",
       " ['t3_18wvtbt', 31],\n",
       " ['t3_19843n7', 32],\n",
       " ['t3_162jj9o', 33],\n",
       " ['t3_z1mzn6', 34],\n",
       " ['t3_16f8ctj', 35],\n",
       " ['t3_1382vj6', 38],\n",
       " ['t3_16f92pv', 38],\n",
       " ['t3_18f9ds2', 43],\n",
       " ['t3_tqejhm', 44],\n",
       " ['t3_15qii74', 44],\n",
       " ['t3_18ff4a2', 48],\n",
       " ['t3_1aj1kcj', 48],\n",
       " ['t3_15sx2rf', 49],\n",
       " ['t3_1b114ek', 55],\n",
       " ['t3_18rkcrr', 100]]"
      ]
     },
     "execution_count": 1138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_len.sort(key=lambda x: x[1])\n",
    "thread_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d430ade-ae81-4fae-9c47-992f46b23c72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get the threads of the relevent context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d493fa9-e4f7-4bcb-b913-48b46ea8d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def set_subreddit(sub_reddit):\n",
    "    with open(sub_reddit+'.json', 'r', encoding='utf-8') as f:\n",
    "        dat = json.load(f)\n",
    "    return pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "899a3296-6d60-4c73-ba3a-f1f6a4c19b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subreddit = set_subreddit('BestBuyWorkers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0781e-59cb-454b-919b-b2a4a668cfc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Getting the relevent threads for the three questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4e7356e0-e4ee-4490-8aff-174334ad1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reddit_link_id (comments) reddit_name (submissions) for the given content in DataFrame form\n",
    "def get_ids(temp_dataset):\n",
    "    temp_l1 = list(temp_dataset[temp_dataset.aware_post_type == 'comment'].reddit_link_id)\n",
    "    temp_list = list(temp_dataset[temp_dataset.aware_post_type == 'submission'].reddit_id)\n",
    "    [temp_l1.append(df_subreddit[df_subreddit.reddit_id==e].iloc[0].reddit_name) for e in temp_list]\n",
    "    return temp_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "911ef310-43d6-4c3c-9568-ee5ad3677dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "18\n",
      "19\n",
      "\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(set(get_ids(q1_dataset_not_relevant))))\n",
    "print(len(set(get_ids(q2_dataset_not_relevant))))\n",
    "print(len(set(get_ids(q3_dataset_not_relevant))))\n",
    "\n",
    "print()\n",
    "\n",
    "print(len(set(get_ids(q1_dataset_relevant))))\n",
    "print(len(set(get_ids(q2_dataset_relevant))))\n",
    "print(len(set(get_ids(q3_dataset_relevant))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2b20ec95-b687-4fbe-bf10-ee1243819547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import construct_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1ef8babd-5035-4698-b5e7-81a4dfc487c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_threads = []\n",
    "q2_threads = []\n",
    "q3_threads = []\n",
    "\n",
    "for id in get_ids(q1_dataset_relevant):\n",
    "    q1_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))\n",
    "\n",
    "for id in get_ids(q2_dataset_relevant):\n",
    "    q2_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))\n",
    "\n",
    "for id in get_ids(q3_dataset_relevant):\n",
    "    q3_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "91edf5df-747b-4c77-b6da-4dc70bd22a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_irr_threads = []\n",
    "q2_irr_threads = []\n",
    "q3_irr_threads = []\n",
    "\n",
    "for id in get_ids(q1_dataset_not_relevant):\n",
    "    q1_irr_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))\n",
    "\n",
    "for id in get_ids(q2_dataset_not_relevant):\n",
    "    q2_irr_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))\n",
    "\n",
    "for id in get_ids(q3_dataset_not_relevant):\n",
    "    q3_irr_threads.append(construct_thread.ConsrtuctThread(df_subreddit, id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42136910-83e4-4432-8fb5-c9681bcafb42",
   "metadata": {},
   "source": [
    "There are some conversations that are very long (as can be seen from the following) and therefore problematic for LLM summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "b13bf52e-a1fa-43a9-9eaa-80e09273dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3915, 3034, 7184, 2791, 411, 9732, 4796, 1559, 14606, 0]\n",
      "[3915, 3034, 7184, 2791, 411, 9732, 4796, 1559, 12793, 0]\n",
      "[589, 7931, 1398, 4464, 3731, 19962, 4808, 9343, 2645, 4678]\n",
      "[589, 7931, 1398, 4464, 3731, 9066, 4808, 9343, 2645, 4678]\n",
      "[3304, 3105, 8506, 2132, 1773, 600, 2284, 2289, 1516, 879]\n"
     ]
    }
   ],
   "source": [
    "print([len(' '.join(thread.get_conversation())) for thread in q1_threads])\n",
    "print([len(' '.join(thread.get_conversation()[:26])) for thread in q1_threads])\n",
    "print([len(' '.join(thread.get_conversation())) for thread in q2_threads])\n",
    "print([len(' '.join(thread.get_conversation()[:50])) for thread in q2_threads])\n",
    "print([len(' '.join(thread.get_conversation())) for thread in q3_threads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e48a78b9-d74e-4f86-a997-258dc242a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3241, 4293, 610, 5695, 6231, 3198, 525, 2408, 1282, 1468, 2920, 1910, 1213, 19962, 967, 2403, 10428, 1950, 327, 938]\n",
      "[3241, 4293, 610, 5695, 6231, 3198, 525, 2408, 1282, 1468, 2920, 1910, 1213, 9066, 967, 2403, 10428, 1950, 327, 938]\n",
      "[3915, 638, 2638, 6162, 1246, 1093, 1566, 11677, 1864, 9449, 9449, 1996, 2026, 2826, 3650, 336, 1845, 11677, 2308, 816]\n",
      "[3915, 638, 2638, 5479, 1246, 1093, 1566, 7776, 1864, 5749, 5749, 1996, 2026, 2826, 3650, 336, 1845, 7776, 2308, 816]\n",
      "[7479, 634, 389, 1144, 5177, 8275, 947, 2906, 2903, 2079, 105, 254, 7931, 10720, 7479, 1845, 3531, 1398, 2548, 400]\n"
     ]
    }
   ],
   "source": [
    "print([len(' '.join(thread.get_conversation())) for thread in q1_irr_threads])\n",
    "print([len(' '.join(thread.get_conversation()[:50])) for thread in q1_irr_threads])\n",
    "print([len(' '.join(thread.get_conversation())) for thread in q2_irr_threads])\n",
    "print([len(' '.join(thread.get_conversation()[:30])) for thread in q2_irr_threads])\n",
    "print([len(' '.join(thread.get_conversation())) for thread in q3_irr_threads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8fff5525-39f2-410b-88eb-ccd04dcdf9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_18rkcrr'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ids(q1_dataset_not_relevant)[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "468c06b4-f020-4865-a1b6-a1671bafac6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_18rkcrr'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ids(q2_dataset_relevant)[-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cec383-5c0d-43d9-8466-f570fb1f576a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ae2e611f-f846-4973-8e4b-c9f9e70ed98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1b8ab9a6-b856-46c4-b400-b242cfd23d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(title, conversation_list):\n",
    "    template = \"\"\"You are a conversation summarizing AI agent. The conversation with title, \"{title}\", is given to you in form of a Python list: {list}.\n",
    "                  Paraphrase a precise summary capturing key highlights of this conversation. If the conversation is empty then discuss the context using the given title. \n",
    "               \"\"\"\n",
    "    # template = \"\"\"Summarize the following list of conversations: {list} \n",
    "    #               Paraphrase your output.\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=['title', 'list'])\n",
    "    prompt_formatted_str: str = prompt.format(title=title, list=conversation_list)\n",
    "    return prompt_formatted_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c7b13e7c-b2bb-48d8-ab35-aa11b5b2653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(title, conv_list, llm=llm_mistral):\n",
    "    prompt = gen_prompt(title, conv_list)\n",
    "    response = llm.invoke(prompt)\n",
    "    # return out.split('Write a summary capturing key highlights of the above conversation.')[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0d5757a5-08c7-46d1-b95c-618afaf28de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b2f7b2a2-5804-4f9b-8885-09277d8aca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:37<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "q3_summary = []\n",
    "\n",
    "for thread in tqdm(q3_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()\n",
    "    q3_summary.append(llm_response(title, conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "aac3ef18-8c4d-4005-bb1f-d772e89d692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:51<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "q3_irr_summary = []\n",
    "\n",
    "for thread in tqdm(q3_irr_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()\n",
    "    q3_irr_summary.append(llm_response(title, conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2f47ad28-0cf7-47b3-9068-223e1fce33d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: Corie Berry's mole\\n\\n                This conversation appears to be centered around Corie Berry's mole and why she hasn't had it removed, as suggested by the first comment. The conversation then takes a humorous turn with references to aiming at objects during urination and the use of a fake fly. The conversation continues with comments from various participants expressing their reactions to the mole and the situation, with some adding their own humorous perspectives. There is no clear indication of who Corie Berry is or what context this conversation is taking place in, so it remains unclear why the topic of her mole has come up.\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_irr_summary[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "74b3ee29-448f-4ee6-b22d-b50a7d5edf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/BestBuyWorkers/comments/14ngsbs'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_irr_threads[3].get_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4a262a1d-5c02-4f1b-b18d-8631b70acc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What do Best Buy employees think of the company?',\n",
       "       'What are the most common reasons for employees to leave Best Buy?',\n",
       "       'Do employees feel understaffed?'], dtype=object)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b79167-8b3f-4f14-a99a-6c9ae552b057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summarization with mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "648303c0-9d32-4fb1-9044-b904836eb8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q1_threads[8].get_conversation()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "ebe238c2-7238-4058-9800-374f87edb003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "q1_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q1_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()[:25]\n",
    "    q1_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "fb43e269-593c-4825-8393-c08d172dcb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [02:48<00:00,  8.45s/it]\n"
     ]
    }
   ],
   "source": [
    "q1_irr_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q1_irr_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()[:50]\n",
    "    q1_irr_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d73ebdce-3433-4b67-b4a9-e78905a73a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [01:00<00:06,  6.71s/it]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: th4c5-isp3-6LSu4e9Rh3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[404], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m title \u001b[38;5;241m=\u001b[39m thread\u001b[38;5;241m.\u001b[39mget_title()\n\u001b[1;32m      5\u001b[0m conv \u001b[38;5;241m=\u001b[39m thread\u001b[38;5;241m.\u001b[39mget_conversation()[:\u001b[38;5;241m50\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m q2_summary_mix\u001b[38;5;241m.\u001b[39mappend(\u001b[43mllm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_mixtral\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[205], line 3\u001b[0m, in \u001b[0;36mllm_response\u001b[0;34m(title, conv_list, llm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_response\u001b[39m(title, conv_list, llm\u001b[38;5;241m=\u001b[39mllm_mistral):\n\u001b[1;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m gen_prompt(title, conv_list)\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# return out.split('Write a summary capturing key highlights of the above conversation.')[-1]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[0;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 593\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1209\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1208\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1209\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[1;32m   1213\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/huggingface_endpoint.py:256\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: th4c5-isp3-6LSu4e9Rh3)"
     ]
    }
   ],
   "source": [
    "q2_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q2_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()[:50]\n",
    "    q2_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "04e5f8bf-e21d-4d01-bcae-ecf76c00afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_summary_mix.append(llm_response(q2_threads[-1].get_title(), q2_threads[-1].get_conversation()[:12], llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "b30c5257-958c-461e-bf14-216e2dcf761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [01:46<00:00,  5.33s/it]\n"
     ]
    }
   ],
   "source": [
    "q2_irr_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q2_irr_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()[:30]\n",
    "    q2_irr_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "14a2991d-1c20-4683-bcfe-d7c48fb165f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [01:00<00:00,  6.07s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [02:11<00:00,  6.56s/it]\n"
     ]
    }
   ],
   "source": [
    "q3_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q3_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()\n",
    "    q3_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))\n",
    "\n",
    "q3_irr_summary_mix = []\n",
    "\n",
    "for thread in tqdm(q3_irr_threads):\n",
    "    title = thread.get_title()\n",
    "    conv = thread.get_conversation()\n",
    "    q3_irr_summary_mix.append(llm_response(title, conv, llm=llm_mixtral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "302f4376-1462-4647-af4c-fbc590e7912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The conversation revolves around employees' experiences and frustrations with management and Best Buy since recent changes have been implemented. Many employees express feeling hopeless, mentally and emotionally drained, and that HR is unresponsive. Some mention that upper management is difficult to work with and that EET (employee experience team) is a joke. Several employees have left the company due to these issues and some are considering doing so. The conversation also touches on the topic of unions and their role in protecting workers. Some employees share their personal experiences of being retaliated against or treated unfairly by management. Overall, the conversation reflects a sense of dissatisfaction and frustration among employees towards management and Best Buy.\n",
      "\n",
      "The conversation revolves around the experiences of employees working at Best Buy post certain changes. The employees express their dissatisfaction with the management, HR, and the EET. They feel that HR is not responsive to their issues and upper management is pressuring them to meet sales targets. Some employees have also faced harassment and retaliation. The employees are considering whether to continue working at Best Buy or to find a new job. They also mention the importance of unions in protecting workers' rights. Some employees have already left the company and share their reasons for doing so. Overall, the employees feel that Best Buy is no longer a good place to work due to poor management, lack of support from HR, and high pressure to meet sales targets.\n"
     ]
    }
   ],
   "source": [
    "print(q3_irr_summary[0])\n",
    "print(q3_irr_summary_mix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "968ba819-c553-4a65-a1ba-fd5b39f1b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'reddit_id' :get_ids(q1_dataset_relevant), 'context': q1_summary_mix})\n",
    "df2 = pd.DataFrame({'reddit_id' :get_ids(q1_dataset_not_relevant), 'context': q1_irr_summary_mix})\n",
    "q1_summarized_contexts = pd.concat([df1, df2]).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "d59235e2-ca89-4139-a807-a17f1c9b4093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nThe conversation is about tips and advice for people employed at Best Buy. The first tip is to work hard but not too hard, as being likeable is often more important than personal performance for promotions. The second tip is to consider switching stores if management is not favorable. The third tip is that there is no job security in retail and the ability to adapt is crucial. The fourth tip is to not be afraid to apply for positions one does not feel qualified for. The fifth tip is to avoid post-military employees in management roles as they take their jobs too seriously. The sixth tip is to stand up for oneself. The seventh tip is to be nice to installers as they can help with mistakes on orders. The eighth tip is to also be nice to Geek Squad employees. The ninth tip is that HR does not mess around and will take action if there is a valid case. The tenth tip is that corporate does not view retail positions as careers. The eleventh tip is that the current sales model is broken. The twelfth tip is to prioritize mental and physical health. The conversation also mentions the importance of having an open mind and using skills and knowledge learned at Best Buy to advance in one's career.\",\n",
       " \"\\nThe conversation revolves around a potential new hire's concerns about working at Best Buy. The advice given includes being prepared for sales performance tracking, dealing with pressure to sell protection plans, credit cards, and memberships, and handling understaffing. Some users share their negative experiences, mentioning that Best Buy has become too focused on sales metrics and no longer values its employees as it used to. Others highlight some benefits, such as help with school and a good discount. The general consensus seems to be that while there are some advantages, the pressure to meet sales targets can be overwhelming and not everyone's experience is positive. Users advise the potential hire to be prepared for this and to consider their own priorities and values when deciding whether to accept the job.\",\n",
       " \"\\nThe conversation revolves around a seasonal sales advisor's struggle with Poor Monthly Sales (PMS) and Appointments (apps). The advisor has been performing poorly in terms of PMS over the past two months, with only 6 Product Knowledge (PK) and 1 app in the last month, and 6 PK and 2 apps this month. Despite being on a seasonal schedule, the advisor is eager to improve. \\n\\nSeveral suggestions are made to help the advisor improve their PMS and apps. These include focusing on building customer rapport, understanding customer needs, and offering tailored solutions. The advisor is also advised to leverage their product knowledge, particularly in computers, to increase totals. \\n\\nThe advisor is also encouraged to team up with the Geek Squad for packages and educating clients about technology and protection. This can help increase apps, as the Geek Squad can provide the necessary technical expertise and support. \\n\\nThe advisor is further advised to use the credit card offered by the company to increase sales, as it offers financing options at a 31.99% interest rate. However, it is noted that this card may not be suitable for everyone, particularly those who are not responsible with credit. \\n\\nOverall, the conversation emphasizes the importance of building customer rapport, leveraging product knowledge, and using available resources to improve sales performance. The advisor is encouraged to keep improving and not feel discouraged by their current performance.\",\n",
       " \"\\nThe conversation is about getting a job at Best Buy. The first speaker is looking for advice and tips for working at Best Buy's sales floor. The second speaker advises not to waste time at Best Buy if there is no advancement within a few years. The third speaker gives specific advice for sales floor jobs, such as showing enthusiasm for technology and being prepared for video interviews with tricky questions. The fourth speaker praises the team environment at Best Buy, while the fifth speaker criticizes the low pay and understaffing. The sixth speaker mentions that their store is different and they start at a higher wage. The seventh speaker advises to go for a DDC warehouse job due to the tightening manager positions and pay cap in store jobs.\",\n",
       " \"\\nThe conversation is about the potential cutting of Specialty Sales Manager (SSM) positions at Best Buy. The participants are discussing whether this has already happened and if it also affects appliances managers. One participant is not currently working at Best Buy but is interested in the company's recent developments.\",\n",
       " \" Summary: \\n\\nThe conversation is about a former employee's experience of retaliation after reporting sexual harassment at a Best Buy store in Orlando. The employee was sexually harassed by their supervisor in late 2019 and reported the incident. However, instead of addressing the issue, the employee's employment took a turn for the worse. The supervisor had high sales, which made it difficult for the company to take action against him. After the supervisor's termination, the employee felt uncomfortable and unsafe at work due to the General Manager's (GM) behavior towards them. The employee shared their concerns about a family member of the former supervisor, who was a seasonal employee and had a history of violent behavior. However, the GM kept him in the store with a promotion as a full-time employee. The GM began ignoring the employee, and they felt targeted and under scrutiny. The employee eventually received a final warning for an out-of-policy transaction, which they believe was a form of retaliation. The employee approached the GM about transferring to another location, but the GM never followed up. The employee called human resources to report their fear of retaliation, but they never reached out again. Six different managers confirmed that the GM had it out for the employee. The employee is now a former Best Buy employee and hopes that their story will help create a positive work environment for victims who have reported sexual harassment.\",\n",
       " \"\\nThe conversation revolves around an individual's decision to quit their job at Best Buy due to constant stress and mental health concerns. The participants express support for the individual's choice, emphasizing that their mental health is paramount and that the work environment at Best Buy is toxic. They share experiences of similar situations and suggest that the individual look for better opportunities. Some also mention the company's focus on sales goals over employee welfare. A few participants also discuss the possibility of taking a leave of absence instead of quitting, which would provide financial support during the transition.\\n\\nIn summary, the conversation revolves around the theme of mental health and job satisfaction, with participants sharing experiences and advice related to leaving a toxic work environment.\",\n",
       " \"\\nIn this conversation, an individual who has been working at Best Buy for two weeks expresses their preference to stay at the front end and avoid responsibilities such as sign-ups and Total Tech (TT). Another participant advises that it doesn't matter where the individual works, as they will still be expected to focus on sign-ups and TT, especially in front lanes. They encourage the individual to learn as much as they can and not limit themselves to one area. A third participant believes there is little chance of staying in the front end, as the company prioritizes memberships across all departments. They suggest that the individual consider a warehouse position or look for opportunities elsewhere, as the company is facing challenges and reducing its workforce. The final participant mentions that cashiers at their Best Buy are not prioritized and have low sales of credit cards due to their focus on the sales floor.\",\n",
       " ' \"\"\"\\n                This conversation revolves around the topic of unions in companies, specifically at BestBuy. The participants discuss the need for unions, especially in companies that are against them. They mention that unions are like condoms, the more someone tries to convince you that you don\\'t need one, the more you absolutely need one. The participants also discuss the concept of \\'voluntary recognition\\' which is becoming more popular among U.S employers. They share their experiences of how their companies react to even the mention of unions, with some companies sending in union busters. The participants also discuss the benefits of unions such as annual cost of living adjustments for wages, a four-day workweek, better protections against layoffs/restructures, better medical insurance options, better training, and better staffing. They also mention the trade-offs of unions, such as job security for long-term employees and the possibility of higher medical, dental, and vision contributions. The participants also discuss the financial implications of strikes and the potential impact on low-level employees. The conversation ends with a debate on who should make sacrifices, the overworked and underpaid staff or the company.',\n",
       " '\\nSummary: The conversation revolves around the experiences of a Best Buy worker, discussing various aspects of their job such as the work environment, customer interactions, and the impact of technology on their role. The worker expresses frustration towards certain aspects of the job, including long hours, low pay, and difficult customers. They also mention the importance of teamwork and the satisfaction derived from helping customers find solutions to their problems. The worker highlights the role of technology in their job, mentioning the use of handheld devices to check inventory and assist customers. They also express concern over the future impact of automation on their job security. Overall, the conversation paints a picture of a challenging yet rewarding job, with the worker finding fulfillment in the connections they make with customers and colleagues.\\n\\nContext: The conversation titled \"Life as a Best Buy worker 💀\" likely discusses the experiences of an individual working at Best Buy, a popular electronics retailer. The title suggests that the conversation may touch upon the challenges and rewards of working in such an environment, potentially including interactions with customers, the role of technology, and job satisfaction. Given the emoji in the title, there may also be a focus on the more difficult or frustrating aspects of the job.',\n",
       " '\\n---\\n\\nThe subreddit r/BestBuyWorkers has decided to disallow customer posts to maintain a space for employees to ask questions, discuss work, share memes, and vent. The new rule was implemented based on a community poll that ran 2:1 in favor of disallowing customer posts. Customers can contact Best Buy USA or Best Buy Canada for transaction-related issues or seek help from r/Bestbuy. Violators of the new rule may be banned from the subreddit. The rule currently applies only to posts, not comments, but this may change if it is abused. Customers are allowed to comment as long as they do not violate any other rules or Reddit site rules. The key highlights of the conversation include the introduction of the new rule, its rationale, and the consequences for violating it.',\n",
       " '\\nThe conversation revolves around the idea of improving working conditions at Best Buy by working together. The author argues that Best Buy cannot function without its employees, who are currently overworked and underpaid. They suggest that during the holiday season, employees should band together and walk out, causing a significant loss for the company. This, they believe, will force Best Buy to improve working conditions and wages. The conversation also touches on the role of leadership and the potential consequences of such actions. Some participants express their support for the idea, while others question its feasibility and potential impact.',\n",
       " '\\nThe conversation is about the project team and their travel. The inquirer is considering joining the team and wants to know how much they travel, particularly out of state. A respondent mentions that travel depends on various factors, primarily remodels or covering projects for remodeling teams. Another respondent notes that travel varies greatly from year to year. They also mention that project team can be a good fit for those who are suited for the work, and that some of the longest tenured employees are on the project team. A final comment mentions that someone was let go from the team, but it is unclear if this affects everyone.',\n",
       " \"\\nThe conversation is about NVIDIA GPU drop on 7/20. The drop will include all six NVIDIA Founders Edition cards, with 20-60 of each card per participating store. The stores are expected to open earlier than usual, similar to the 3080 Ti in-person sale. It is unclear whether AIB (custom) cards will be available. The drop is not confirmed for all stores, and the process for obtaining a GPU is not explicitly stated, but it is advised to arrive early and be prepared to wait in line. A list of participating stores can be found in a Google spreadsheet. It is recommended to limit tickets to one per person, and the official launch details can be found on Best Buy's website.\",\n",
       " ' |\\n                A conversation titled \"Scheduling Complaint\" revolves around an individual\\'s dissatisfaction with their work schedule. They are the only full-time employee required to work six days straight, while others get weekends off. This arrangement combines two partial pay weeks into one large pay week, followed by a single day off. The individual feels targeted and believes it\\'s unfair. Despite bringing it up with store leadership, they feel unheard. They contemplate escalating the issue to HR. Others in the conversation suggest that without concrete proof, it may be challenging to effect change. Some empathize with the individual\\'s situation, while others criticize the company\\'s practices. A few share their own experiences of unfavorable scheduling. The individual is determined to address the issue, considering a change in availability or seeking a new job.',\n",
       " \"\\nThe conversation revolves around the topic of counterfeit bill detection at Best Buy. Participants express confusion and frustration over the store's policy of not using counterfeit pens or black lights to detect bills, citing reasons such as customer offense and SOP restrictions. Some suggest that these methods should still be used, especially for checking identification cards. Others mention the inaccuracy of counterfeit pens. The real issue, as pointed out by some, is that common counterfeits use bleached small denominations and printed larger ones, which can bypass the pen test. Participants also mention the importance of checking watermarks, reflective ink, and ridges on the jacket of the portrait. The store's policy is attributed to avoiding customer dissatisfaction and the cost of replacing pens and lights. Some also mention the use of starch to defeat pen tests and the difficulty of detecting counterfeit $1 bills.\",\n",
       " '\\nThe conversation revolves around the topic of a General Motors (GM) call. The participants are discussing whether the call was related to job cuts or changes in working hours. According to some participants, the call was not related to any cuts, and they are waiting for the changes to occur. However, others believe that the call was about a change in working hours, specifically from 10-9 Monday to Saturday and 11-8 on Sunday, starting from the week of June 4th. It is unclear whether these changes are company-wide or limited to certain areas. There is no information in the conversation about any job cuts.',\n",
       " \"\\nThe user is facing a dilemma about whether to quit their job at Best Buy due to new management and a lack of training for a new role. The user has informed the General Manager (GM) about their availability and job preference, but they were still scheduled for a day they cannot work and a role they are not trained for. The GM has not provided any opportunities for the user to learn the new role, and the user feels that they are being treated unfairly. The user is also annoyed that they have not received their blue shirt, a uniform item. Other users have suggested that the user could either leave the job off their resume or provide a reason for the short employment if asked. They have also criticized the GM's leadership style and lack of technical knowledge.\",\n",
       " '\\nSummary: The user has applied for a job at Best Buy and has a scheduled Zoom interview for Wednesday. They are considering whether to accept the interview or focus on a grocery store job offer instead. The user is looking for short-term employment until New Year or early 2023. A conversation participant suggests that Best Buy is a good place to work, with better pay than minimum wage and a focus on technology. However, selling services and warranties might not be comfortable for everyone. Another participant mentions that grocery stores do not usually provide discounts, but Walmart and Costco offer employee discounts in some countries. The user clarifies that they are referring to regional grocery store chains.',\n",
       " \"\\nThe conversation revolves around workers sharing their craziest experiences while delivering items to people's houses. One individual encountered a house with numerous glass dolls, resembling those seen in movies. Another observed racist statues resembling blackface caricatures. A third person described a client's home filled with Nazi memorabilia and another with an X-frame and a large Easter Island head along with various BDSM items. Some also mentioned discovering unsettling content on clients' devices, such as bestiality-related searches and pornography. Despite these strange experiences, most workers chose to leave the premises immediately without further engagement.\",\n",
       " '\\nThe conversation revolves around negative experiences with Roadie pickups. Participants share instances of Roadie drivers being rude, not following rules, and expecting store employees to do all the work for them. Some participants have even banned rude Roadie drivers from their stores. There is also a mention of using carrier feedback forms to report negative experiences. A few participants mention positive experiences with some Roadie drivers. Overall, the conversation highlights the challenges and frustrations that store employees face when dealing with Roadie pickups.',\n",
       " '\\nThe conversation is about an individual who has been hired at BestBuy and has been assigned e-learnings as part of their job. The individual has a 4-hour shift where they were only able to complete half of the e-learnings and the next shift is not until a week later. The leader suggested doing the e-learnings at home and recording the hours, but the individual is unsure of how to log the hours as there is no log for e-learnings in the TLC. The individual is also unsure if they are expected to do the e-learnings at home or at work. A friend who worked retail at Target mentioned that they were pulled aside during their shifts to complete e-learnings, which is what the individual was expecting. The individual is concerned about clocking in from home and getting in trouble for it. The suggested solution is to do the e-learnings at home and then edit the time sheet manually during the next shift. The individual can add manual punches for \"clock on\" and \"clock off\" and enter the time they started and finished the e-learnings. It is important to note that BestBuy can check how long the individual was doing e-learnings and how many they completed, as well as monitor for any idle time. The individual can also call their manager and ask if they can come in for another short shift to complete the e-learnings.',\n",
       " \"\\nThe conversation revolves around a customer's pre-order issue with Best Buy. Corporate is sending the item to a store of the customer's choice to resolve the problem. The customer is frustrated with the waiting time and lack of information from store employees. The customer seeks a time period to wait before getting anxious to visit the store. A Best Buy employee clarifies that store employees usually have no knowledge of specific order exceptions made by corporate. The items just appear and are marked for pick-up within 30 minutes of receiving them in inventory. The customer shares that they received confirmation but without a tracking number, making it a guessing game. The inventory manager for gaming received an email from corporate, but the item won't show up in their inventory. The conversation is intended for Best Buy employees, and the customer seeks their input on the situation.\",\n",
       " \" The conversation revolves around an individual who fell sick with a 102-degree fever and was denied sick leave by their supervisor. They tried to manage the situation with medication but ended up fainting at work. The manager allowed them to leave only after a customer intervened, accusing the company of child labor. The individual is now scheduled for 8-hour shifts until the next Saturday and is contemplating calling in sick every day.\\n\\n                The conversation includes advice from various perspectives. Some suggest quitting the job due to the manager's insensitivity and the company's policies. Others recommend learning about company policies and understanding employee rights. A common theme is the importance of health over job, with suggestions to document everything if there's any backlash. Some also mention that it's against Best Buy policy to ask for a doctor's note, and it's not supposed to be accepted even if given.\\n\\n                There are also comments about the inappropriateness of the manager's behavior, suggesting that it could lead to litigation due to injury. Some advise contacting HR, while others caution that HR is never on the employee's side. There are also reminders about the existence of sick banks and attendance policies, and the importance of using them correctly.\\n\\n                In summary, the conversation revolves around an individual's difficult work situation due to illness and poor management, with advice focusing on health, understanding company policies, and potential actions such as quitting or contacting HR.\",\n",
       " '\\nThe conversation is about a job interview for an Alienware Computing Sales position. The interview is tomorrow and the person has 2 years of management experience at Walmart and is bilingual. They have never built a computer but are familiar with all the components of a desktop. The starting pay for this position is higher than normal due to it being a VPL (Value-Added Product Line) position, and pay can vary by location. A typical workday involves standing, greeting customers, and trying to sell not just the product but also a credit application and a total tech. The job is described as easy for a good salesperson but hard and unrewarding for others. A comment suggests not to work at Best Buy, but it is unclear if this is the store where the job interview is taking place.',\n",
       " '\\nThe conversation revolves around the topic of annual appraisals and increases at work. Some participants mentioned that annual increases are given in March, regardless of the work anniversary date. Others mentioned that annual increases are given based on the work anniversary date. It was also mentioned that the notification for the annual increase goes to the reporting manager first, who then informs the employee. Some participants mentioned that they received a lump sum increase because they were at their salary cap. The process of annual appraisals and increases is now automated through Workday, which sends an alert to each employee and their direct leadership. The increase is typically 3% across the board. Some participants expressed dissatisfaction with the leadership and management at their workplace.',\n",
       " \"\\nThe conversation is about a planned walkout at Best Buy (BF) locations on Black Friday (BF). The initiator wants to show the company they're done being treated like crap and make it a better place to work for future and current employees. They encourage spreading the word, talking to coworkers, and sharing the message on Reddit and other platforms. The conversation also involves discussions on the effectiveness of such actions, potential consequences, and the role of unions in protecting workers' rights. Some participants express support for the walkout, while others criticize it, suggesting alternative ways to bring about change, such as legally unionizing. The conversation also touches upon the challenges faced by workers, including long hours, low wages, and inadequate support from management. The initiator shares their personal experience of being banned from Best Buy and GeekSquad subreddits for posting about the walkout. The conversation highlights the importance of workers standing together and advocating for their rights, as well as the challenges and potential consequences of doing so.\",\n",
       " \"\\nThe conversation titled 'achievers' revolves around individuals discussing their anticipation and excitement for an event called 'achievers'. Some participants have already received their invitations, while others are still waiting. There is a shared sentiment of eagerness to meet new people and form connections at the event. Participants also express their appreciation for each other's support and well-wishes. There is some confusion regarding the submission deadlines and the distribution of invitations. A few individuals have not received any information about the event from their respective stores or districts. Despite this, there is a general sense of optimism and hope that everyone who qualifies will eventually receive their invitations. Some participants have also expressed interest in connecting with each other at the event, if they end up attending.\",\n",
       " ' Summary: The conversation revolves around the process of obtaining accommodations in the present day, in contrast to how it was done in the past. The speaker mentions searching for accommodations on \"learning lounge\" and securing \"hella deals\" through learnings. However, the current process appears to involve searching for accommodations on the Learning Network, with some accommodations being limited to specific job codes and certifications.',\n",
       " '\\nIn this conversation, participants discuss new blue polos with yellow collars that have recently appeared at their workplace. Some individuals have received the new shirts, while others have not. The new shirts are intended for seasonal staff, although there seems to be some confusion regarding their distribution. Participants also compare the fit and comfort of the new shirts to the existing blue ones. A link to an eBay listing for the shirts is shared.']"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(q1_summarized_contexts.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8af757-a680-43b8-982f-216165f12de5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM's classification of context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6fde4-39eb-4183-8be7-95774a1d309a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Marking relevancy for each question over the entire test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "51f21d37-4fec-454c-b5b3-b651d53b8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1 = q1_summarized_contexts.copy()\n",
    "tr2 = q2_summarized_contexts.copy()\n",
    "tr3 = q3_summarized_contexts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "e2299e49-8de9-4911-9ef8-0761871dc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = pd.concat([tr1, tr2, tr3]).reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "1006731b-75bd-4a20-8478-2339c7423355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_199f5h8</td>\n",
       "      <td>\\nThe conversation is about tips and advice fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_wapsy3</td>\n",
       "      <td>\\nThe conversation revolves around a potential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_1aucdoj</td>\n",
       "      <td>\\nThe conversation revolves around a seasonal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_yuacvb</td>\n",
       "      <td>\\nThe conversation is about getting a job at B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_12scwfu</td>\n",
       "      <td>\\nThe conversation is about the potential cutt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>t3_jx0q4y</td>\n",
       "      <td>\\nIn this conversation, a new employee at Best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>t3_1382vj6</td>\n",
       "      <td>\\nThe conversation revolves around potential j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>t3_qhb9v1</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>t3_198a0ow</td>\n",
       "      <td>\\nIn this conversation, participants discuss t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>t3_oo5tzv</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     reddit_id                                            context\n",
       "0   t3_199f5h8  \\nThe conversation is about tips and advice fo...\n",
       "1    t3_wapsy3  \\nThe conversation revolves around a potential...\n",
       "2   t3_1aucdoj  \\nThe conversation revolves around a seasonal ...\n",
       "3    t3_yuacvb  \\nThe conversation is about getting a job at B...\n",
       "4   t3_12scwfu  \\nThe conversation is about the potential cutt...\n",
       "..         ...                                                ...\n",
       "85   t3_jx0q4y  \\nIn this conversation, a new employee at Best...\n",
       "86  t3_1382vj6  \\nThe conversation revolves around potential j...\n",
       "87   t3_qhb9v1  \\nIn this conversation, the participants discu...\n",
       "88  t3_198a0ow  \\nIn this conversation, participants discuss t...\n",
       "89   t3_oo5tzv  \\nIn this conversation, the participants discu...\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "df742367-e358-4c4d-a091-ec2b5b62667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm_mistralv1_classify = HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.1', huggingfacehub_api_token=huggingfacehub_api_token, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "232392a8-a4b1-40ea-ae2d-afa29f0cb21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm_mistral_classify = HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token=huggingfacehub_api_token, temperature=0.01)\n",
    "llm_mixtral_classify =  HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', huggingfacehub_api_token=huggingfacehub_api_token, temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "c2486f49-ac1f-42d1-9e76-05d5b7c438d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  1\n",
      "1  :  1\n",
      "2  :  0\n",
      "3  :  1\n",
      "4  :  0\n",
      "5  :  0\n",
      "6  :  0\n",
      "7  :  1\n",
      "8  :  1\n",
      "                    \"\"\"\n",
      "9  :  1\n",
      "10  :  0\n",
      "11  :  1\n",
      "12  :  0\n",
      "13  :  0\n",
      "14  :  0\n",
      "15  :  0\n",
      "16  :  0\n",
      "17  :  1\n",
      "18  :  0\n",
      "19  :  0\n",
      "20  :  0\n",
      "21  :  0\n",
      "22  :  0\n",
      "23  :  0\n",
      "24  :  0\n",
      "25  :  0\n",
      "26  :  1\n",
      "27  :  0\n",
      "28  :  0\n",
      "29  :  0\n",
      "30  :  0\n",
      "31  :  0\n",
      "32  :  1\n",
      "33  :  1\n",
      "34  :  0\n",
      "35  :  0\n",
      "36  :  0\n",
      "37  :  1\n",
      "38  :  1\n",
      "39  :  1\n",
      "40  :  1\n",
      "41  :  0\n",
      "42  :  1\n",
      "43  :  0\n",
      "44  :  0\n",
      "45  :  0\n",
      "46  :  1\n",
      "47  :  0\n",
      "48  :  0\n",
      "49  :  1\n",
      "50  :  1\n",
      "51  :  0\n",
      "52  :  1\n",
      "53  :  0\n",
      "54  :  0\n",
      "55  :  0\n",
      "56  :  0\n",
      "57  :  0\n",
      "58  :  0\n",
      "59  :  0\n",
      "60  :  1\n",
      "61  :  0\n",
      "62  :  1\n",
      "63  :  0\n",
      "64  :  0\n",
      "65  :  0\n",
      "66  :  0\n",
      "67  :  0\n",
      "68  :  0\n",
      "69  :  0\n",
      "70  :  1\n",
      "71  :  0\n",
      "72  :  0\n",
      "73  :  0\n",
      "74  :  0\n",
      "75  :  0\n",
      "76  :  0\n",
      "77  :  1\n",
      "78  :  0\n",
      "79  :  0\n",
      "80  :  0\n",
      "81  :  0\n",
      "82  :  0\n",
      "83  :  0\n",
      "84  :  1\n",
      "85  :  0\n",
      "86  :  0\n",
      "87  :  1\n",
      "88  :  0\n",
      "89  :  0\n"
     ]
    }
   ],
   "source": [
    "q1_classify_mixtral_ALL = {}\n",
    "\n",
    "for i in range(90):\n",
    "    response = llm_mixtral_classify.invoke(classification_prompt(list(trf.context)[i], questions[0]))\n",
    "    q1_classify_mixtral_ALL.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "e2c3745c-0c25-4f6b-bee5-806055e179dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  1\n",
      "1  :  1\n",
      "2  :  0\n",
      "3  :  0\n",
      "4  :  0\n",
      "5  :  0\n",
      "6  :  1\n",
      "7  :  1\n",
      "8  :  1\n",
      "                    \"\"\"\n",
      "9  :  1\n",
      "10  :  0\n",
      "11  :  0\n",
      "12  :  0\n",
      "13  :  0\n",
      "14  :  0\n",
      "15  :  0\n",
      "16  :  0\n",
      "17  :  0\n",
      "18  :  0\n",
      "19  :  0\n",
      "20  :  0\n",
      "21  :  0\n",
      "22  :  0\n",
      "23  :  0\n",
      "24  :  0\n",
      "25  :  0\n",
      "26  :  0\n",
      "27  :  0\n",
      "28  :  0\n",
      "29  :  0\n",
      "30  :  0\n",
      "31  :  0\n",
      "32  :  1\n",
      "33  :  0\n",
      "34  :  0\n",
      "35  :  0\n",
      "36  :  0\n",
      "37  :  0\n",
      "38  :  0\n",
      "39  :  0\n",
      "40  :  1\n",
      "41  :  0\n",
      "42  :  1\n",
      "43  :  0\n",
      "44  :  0\n",
      "45  :  0\n",
      "46  :  0\n",
      "47  :  0\n",
      "48  :  0\n",
      "49  :  0\n",
      "50  :  0\n",
      "51  :  0\n",
      "52  :  0\n",
      "53  :  0\n",
      "54  :  1\n",
      "55  :  0\n",
      "56  :  0\n",
      "57  :  0\n",
      "58  :  0\n",
      "59  :  0\n",
      "60  :  1\n",
      "61  :  1\n",
      "62  :  1\n",
      "63  :  1\n",
      "64  :  0\n",
      "65  :  0\n",
      "66  :  0\n",
      "67  :  0\n",
      "68  :  1\n",
      "69  :  0\n",
      "70  :  1\n",
      "71  :  0\n",
      "72  :  0\n",
      "73  :  0\n",
      "74  :  0\n",
      "75  :  0\n",
      "76  :  0\n",
      "77  :  0\n",
      "78  :  0\n",
      "79  :  0\n",
      "80  :  0\n",
      "81  :  0\n",
      "82  :  0\n",
      "83  :  0\n",
      "84  :  1\n",
      "85  :  0\n",
      "86  :  0\n",
      "87  :  1\n",
      "88  :  0\n",
      "89  :  0\n"
     ]
    }
   ],
   "source": [
    "q2_classify_mixtral_ALL = {}\n",
    "\n",
    "for i in range(90):\n",
    "    response = llm_mixtral_classify.invoke(classification_prompt(list(trf.context)[i], questions[1]))\n",
    "    q2_classify_mixtral_ALL.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "e0cf268a-b771-426d-adb1-66cf22c201c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  0\n",
      "1  :  1\n",
      "2  :  0\n",
      "3  :  1\n",
      "4  :  0\n",
      "5  :  0\n",
      "6  :  0\n",
      "7  :  0\n",
      "8  :  1\n",
      "9  :  1\n",
      "10  :  0\n",
      "11  :  0\n",
      "12  :  0\n",
      "13  :  0\n",
      "14  :  1\n",
      "15  :  0\n",
      "16  :  0\n",
      "17  :  1\n",
      "18  :  0\n",
      "19  :  0\n",
      "20  :  0\n",
      "21  :  0\n",
      "22  :  0\n",
      "23  :  1\n",
      "24  :  0\n",
      "25  :  0\n",
      "26  :  1\n",
      "27  :  0\n",
      "28  :  0\n",
      "29  :  0\n",
      "30  :  0\n",
      "31  :  0\n",
      "32  :  0\n",
      "33  :  0\n",
      "34  :  0\n",
      "35  :  1\n",
      "36  :  0\n",
      "37  :  0\n",
      "38  :  1\n",
      "39  :  0\n",
      "40  :  0\n",
      "41  :  0\n",
      "42  :  1\n",
      "43  :  0\n",
      "44  :  0\n",
      "45  :  0\n",
      "46  :  0\n",
      "47  :  0\n",
      "48  :  0\n",
      "49  :  1\n",
      "50  :  1\n",
      "51  :  0\n",
      "52  :  0\n",
      "53  :  0\n",
      "54  :  0\n",
      "55  :  0\n",
      "56  :  0\n",
      "57  :  0\n",
      "58  :  0\n",
      "59  :  0\n",
      "60  :  1\n",
      "61  :  1\n",
      "62  :  1\n",
      "63  :  0\n",
      "64  :  1\n",
      "65  :  0\n",
      "66  :  1\n",
      "67  :  1\n",
      "68  :  1\n",
      "69  :  1\n",
      "70  :  1\n",
      "71  :  0\n",
      "72  :  0\n",
      "73  :  0\n",
      "74  :  0\n",
      "75  :  0\n",
      "76  :  0\n",
      "77  :  1\n",
      "78  :  0\n",
      "79  :  0\n",
      "80  :  0\n",
      "81  :  0\n",
      "82  :  0\n",
      "83  :  0\n",
      "84  :  1\n",
      "85  :  0\n",
      "86  :  0\n",
      "87  :  0\n",
      "88  :  0\n",
      "89  :  0\n"
     ]
    }
   ],
   "source": [
    "q3_classify_mixtral_ALL = {}\n",
    "\n",
    "for i in range(90):\n",
    "    response = llm_mixtral_classify.invoke(classification_prompt(list(trf.context)[i], questions[2]))\n",
    "    q3_classify_mixtral_ALL.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "61dc1c41-505d-4870-867b-696935a3a112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAnswer:\\nsqrt(2) + sqrt(7)\\n\\nExplanation:\\nThe sum of sqrt(2) and sqrt(7) cannot be simplified further, so the answer is sqrt(2) + sqrt(7).'"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_mixtral_classify(\"what is the sum of sqrt(2) and sqrt(7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "1d7cd0ae-1fed-4348-947e-e05c9e1a6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = trf.assign(**{'Question 1' : [int(q1_classify_mixtral_ALL[i]) for i in range(90)], \n",
    "              'Question 2' : [int(q2_classify_mixtral_ALL[i]) for i in range(90)],\n",
    "              'Question 3' : [int(q3_classify_mixtral_ALL[i]) for i in range(90)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "6efde83b-f4a7-4c30-9618-84ca8c82a3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(trf[\"Question 3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d91efc-50f0-4740-b995-559c3ceeb902",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check consistency with an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ce0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "ffcad2b3-d444-4918-bd42-d73f4f0a8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "9683489e-c8be-43ef-bd70-4f864b776a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "ef359ec2-5053-4604-a42a-76b6d9510c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_client = MistralClient(api_key=mistral_api_key)\n",
    "\n",
    "def create_mistral_embeddings(inputs):\n",
    "    response = mistral_client.embeddings(\n",
    "          model=\"mistral-embed\",\n",
    "          input=inputs\n",
    "      )\n",
    "    return [data.embedding for data in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "0aa9e295-413f-44b2-8257-a448e6d95729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts):\n",
    "    response = client.embeddings.create(    \n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=texts)  \n",
    "    response_dict = response.model_dump()\n",
    "    return [data['embedding'] for data in response_dict['data']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "d618d04d-120d-42d1-9b7c-03b7f224eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_embeddings = create_embeddings(list(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "e83e74e4-09f0-4e40-ade8-6e8838d2e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = create_embeddings(list(trf.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "id": "7125298a-6d17-443a-a0e6-987fc17a9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_embeddings_mistral = create_mistral_embeddings(list(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "bcf15f7e-f1f3-4c9f-973d-3f1beeabf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings_mistral = create_mistral_embeddings(list(trf.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "1360132e-1625-4d6e-a34a-422eb08687e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "72ae2334-fd24-4eed-a770-e2daf3b5c071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[84, 1, 0.1070032357786409],\n",
       " [70, 1, 0.1070475291170856],\n",
       " [1, 1, 0.11712483525827588],\n",
       " [52, 1, 0.11816193191184776],\n",
       " [77, 1, 0.12374541834564623],\n",
       " [39, 1, 0.12651445134262296],\n",
       " [87, 1, 0.12769744155219398],\n",
       " [32, 1, 0.1277928795815121],\n",
       " [11, 1, 0.1281779430023191],\n",
       " [61, 0, 0.13098839728786427],\n",
       " [26, 1, 0.131120404395226],\n",
       " [36, 0, 0.1322879599549348],\n",
       " [3, 1, 0.13581002084814942],\n",
       " [62, 1, 0.13703993791257818],\n",
       " [33, 1, 0.13709789959023522],\n",
       " [83, 0, 0.13979558009697224],\n",
       " [38, 1, 0.13986244565915118],\n",
       " [73, 0, 0.13988540875219246],\n",
       " [46, 1, 0.14195007226110357],\n",
       " [0, 1, 0.14287008201463625],\n",
       " [9, 1, 0.14289116039230854],\n",
       " [40, 1, 0.14323419664601544],\n",
       " [49, 1, 0.14641041859281134],\n",
       " [50, 1, 0.14641041859281134],\n",
       " [18, 0, 0.14747032952661443],\n",
       " [37, 1, 0.14971158768276405],\n",
       " [6, 0, 0.15000092443757262],\n",
       " [17, 1, 0.15329325440416264],\n",
       " [4, 0, 0.1581308479663036],\n",
       " [54, 0, 0.16045538036409757],\n",
       " [22, 0, 0.16157638420753018],\n",
       " [7, 1, 0.16269566874487773],\n",
       " [10, 0, 0.16643597253645004],\n",
       " [31, 0, 0.16764516177588007],\n",
       " [82, 0, 0.16764840350406907],\n",
       " [88, 0, 0.1683878967371486],\n",
       " [8, 1, 0.16992611494378762],\n",
       " [65, 0, 0.17119685979427168],\n",
       " [5, 0, 0.1716295346163489],\n",
       " [60, 1, 0.17363843066351192],\n",
       " [56, 0, 0.18051348664304945],\n",
       " [85, 0, 0.18051348664304945],\n",
       " [24, 0, 0.18084005297039107],\n",
       " [74, 0, 0.18122283892165703],\n",
       " [68, 0, 0.18208906053940255],\n",
       " [63, 0, 0.18276045591030965],\n",
       " [67, 0, 0.18537464586961316],\n",
       " [69, 0, 0.18551756665463892],\n",
       " [35, 0, 0.18857619783824286],\n",
       " [23, 0, 0.18867768892718773],\n",
       " [53, 0, 0.19002780844847034],\n",
       " [45, 0, 0.19247461206630023],\n",
       " [81, 0, 0.1962993675753122],\n",
       " [30, 0, 0.19963052082633392],\n",
       " [64, 0, 0.20009382412023335],\n",
       " [43, 0, 0.2071470206881708],\n",
       " [79, 0, 0.20847266748687832],\n",
       " [71, 0, 0.21081249544175007],\n",
       " [44, 0, 0.21147518118053366],\n",
       " [42, 1, 0.21368284815152105],\n",
       " [20, 0, 0.21406699600701162],\n",
       " [2, 0, 0.21802100645578726],\n",
       " [41, 0, 0.21838487879402635],\n",
       " [14, 0, 0.21894243997924057],\n",
       " [21, 0, 0.22061800829103528],\n",
       " [29, 0, 0.2207086112033546],\n",
       " [75, 0, 0.22175635943999983],\n",
       " [78, 0, 0.22409412978946952],\n",
       " [19, 0, 0.22535400447897125],\n",
       " [15, 0, 0.22650688080334946],\n",
       " [12, 0, 0.22985929288469786],\n",
       " [16, 0, 0.2322287196457724],\n",
       " [86, 0, 0.2331350944573627],\n",
       " [13, 0, 0.23363761623263546],\n",
       " [34, 0, 0.2337743431821755],\n",
       " [47, 0, 0.2352904925153143],\n",
       " [57, 0, 0.2352904925153143],\n",
       " [58, 0, 0.2376129529848765],\n",
       " [51, 0, 0.23776694567953782],\n",
       " [66, 0, 0.23809232057847485],\n",
       " [27, 0, 0.23958678327686622],\n",
       " [48, 0, 0.24243609103112407],\n",
       " [76, 0, 0.2427922517449217],\n",
       " [25, 0, 0.24503961354869142],\n",
       " [59, 0, 0.2488330822640873],\n",
       " [72, 0, 0.25059243777286244],\n",
       " [55, 0, 0.25340716981616085],\n",
       " [89, 0, 0.25604550130661063],\n",
       " [80, 0, 0.2577315645486813],\n",
       " [28, 0, 0.2898019396201402]]"
      ]
     },
     "execution_count": 1041,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = [[i, trf[\"Question 1\"].iloc[i], distance.cosine(context_embeddings[i], questions_embeddings[0])] for i in range(90)]\n",
    "ll.sort(key=lambda x: x[2])\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "id": "86533dd3-c34f-40c8-b1de-949a1460213a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52, 1, 0.15414895368939407],\n",
       " [70, 1, 0.15594257755110474],\n",
       " [84, 1, 0.15594257755110474],\n",
       " [32, 1, 0.17129064548428286],\n",
       " [87, 1, 0.17129064548428286],\n",
       " [1, 1, 0.1736624030528855],\n",
       " [62, 1, 0.18042414789870553],\n",
       " [46, 1, 0.18495497440709763],\n",
       " [38, 1, 0.18822622339032735],\n",
       " [37, 1, 0.1891480832644954],\n",
       " [77, 1, 0.1916445309649496],\n",
       " [0, 1, 0.1920999127365388],\n",
       " [40, 1, 0.1920999127365388],\n",
       " [18, 0, 0.19217861039763795],\n",
       " [36, 0, 0.19409399979391595],\n",
       " [11, 1, 0.19644387625313386],\n",
       " [3, 1, 0.20239975600210114],\n",
       " [65, 0, 0.20334882696900713],\n",
       " [7, 1, 0.20523497709083682],\n",
       " [39, 1, 0.2054193469333475],\n",
       " [74, 0, 0.20666373532738236],\n",
       " [73, 0, 0.20830516889398443],\n",
       " [61, 0, 0.20940764673519852],\n",
       " [6, 0, 0.21010583689731321],\n",
       " [83, 0, 0.21121858026647433],\n",
       " [49, 1, 0.2122486490465315],\n",
       " [50, 1, 0.2122486490465315],\n",
       " [31, 0, 0.21265474801836914],\n",
       " [82, 0, 0.21265474801836914],\n",
       " [33, 1, 0.2136791718265022],\n",
       " [17, 1, 0.21416324041490986],\n",
       " [26, 1, 0.21635673269466216],\n",
       " [22, 0, 0.22117766247070647],\n",
       " [88, 0, 0.22127917724914603],\n",
       " [60, 1, 0.22542988497902838],\n",
       " [64, 0, 0.22559583554722695],\n",
       " [5, 0, 0.2281137954413368],\n",
       " [54, 0, 0.22844005373885012],\n",
       " [24, 0, 0.2286634744850422],\n",
       " [10, 0, 0.22925532249680347],\n",
       " [9, 1, 0.2306470842525029],\n",
       " [4, 0, 0.23188174235497983],\n",
       " [69, 0, 0.23314452411916975],\n",
       " [53, 0, 0.23691226921967257],\n",
       " [71, 0, 0.2412530221719229],\n",
       " [23, 0, 0.24334755090695015],\n",
       " [35, 0, 0.24334755090695015],\n",
       " [8, 1, 0.24453392193818035],\n",
       " [56, 0, 0.24472591819096257],\n",
       " [85, 0, 0.24472591819096257],\n",
       " [48, 0, 0.24602661797215541],\n",
       " [81, 0, 0.2512972830118646],\n",
       " [21, 0, 0.2514024224654271],\n",
       " [15, 0, 0.2519652601764947],\n",
       " [68, 0, 0.2553394259949229],\n",
       " [67, 0, 0.2594337538932573],\n",
       " [42, 1, 0.26216696261375094],\n",
       " [63, 0, 0.26989496886774245],\n",
       " [79, 0, 0.27044538513165506],\n",
       " [86, 0, 0.2719131786916855],\n",
       " [30, 0, 0.2750217036954691],\n",
       " [78, 0, 0.27563415514033796],\n",
       " [45, 0, 0.27616879320092547],\n",
       " [41, 0, 0.27691635935266945],\n",
       " [12, 0, 0.2791777448708366],\n",
       " [43, 0, 0.28109243683661334],\n",
       " [14, 0, 0.2875956196653704],\n",
       " [44, 0, 0.2881992731636489],\n",
       " [29, 0, 0.28849790314985146],\n",
       " [2, 0, 0.288601704289289],\n",
       " [66, 0, 0.28874683063496565],\n",
       " [25, 0, 0.28878126136858095],\n",
       " [19, 0, 0.28893104042064177],\n",
       " [51, 0, 0.28935051169519543],\n",
       " [47, 0, 0.2915664893556329],\n",
       " [57, 0, 0.2915664893556329],\n",
       " [20, 0, 0.29274099234226847],\n",
       " [13, 0, 0.2951921817350287],\n",
       " [16, 0, 0.3013677145670465],\n",
       " [75, 0, 0.3046018503125276],\n",
       " [76, 0, 0.3048958977144116],\n",
       " [59, 0, 0.30734584284474464],\n",
       " [34, 0, 0.3082272863031461],\n",
       " [28, 0, 0.31932019882558405],\n",
       " [55, 0, 0.31971112113220945],\n",
       " [89, 0, 0.3219896954887017],\n",
       " [27, 0, 0.32444663433998955],\n",
       " [58, 0, 0.3273952541647863],\n",
       " [72, 0, 0.3310224415789156],\n",
       " [80, 0, 0.33745044564564197]]"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_mistral = [[i, trf[\"Question 1\"].iloc[i], distance.cosine(context_embeddings_mistral[i], questions_embeddings_mistral[0])] for i in range(90)]\n",
    "ll_mistral.sort(key=lambda x: x[2])\n",
    "ll_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "id": "a8e4dd18-fa69-4050-b033-9f687fbd679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What do Best Buy employees think of the company?',\n",
       " 'What are the most common reasons for employees to leave Best Buy?',\n",
       " 'Do employees feel understaffed?']"
      ]
     },
     "execution_count": 1086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "f1d5b800-ac1e-4077-8842-8c260298528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reddit_id                                             t3_wapsy3\n",
       "context       \\nThe conversation revolves around a potential...\n",
       "Question 1                                                    1\n",
       "Question 2                                                    1\n",
       "Question 3                                                    1\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 1115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c33eea-48ae-4d2e-9ad6-03c3f433f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = construct_thread.ConsrtuctThread(df_subreddit, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c6b59-bfbe-49ba-9e26-2bc8269b6e26",
   "metadata": {},
   "source": [
    "as expected the mistral embeddings seems to work better since the summarization was done with mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "id": "df039638-48d5-4845-b0d9-9dd425dd3170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 11, 26, 32, 33, 36, 38, 39, 46, 52, 61, 62, 70, 73, 77, 83, 84, 87]"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([e[0] for e in ll][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "id": "86eba5b5-0941-4710-91bf-7fda2b841ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 7, 11, 18, 32, 36, 37, 38, 39, 40, 46, 52, 62, 65, 70, 77, 84, 87]"
      ]
     },
     "execution_count": 1072,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([e[0] for e in ll_mistral][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "39302340-dbd1-45da-a63c-80be29e9dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSummary: The user has applied for a job at Best Buy and has a scheduled Zoom interview for Wednesday. They are considering whether to accept the interview or focus on a grocery store job offer instead. The user is looking for short-term employment until New Year or early 2023. A conversation participant suggests that Best Buy is a good place to work, with better pay than minimum wage and a focus on technology. However, selling services and warranties might not be comfortable for everyone. Another participant mentions that grocery stores do not usually provide discounts, but Walmart and Costco offer employee discounts in some countries. The user clarifies that they are referring to regional grocery store chains.'"
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf.iloc[18].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "b8c03ce5-55eb-4f4e-8bf9-af2ef4dd1b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What do Best Buy employees think of the company?',\n",
       "       'What are the most common reasons for employees to leave Best Buy?',\n",
       "       'Do employees feel understaffed?'], dtype=object)"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q1: ‘What do Best Buy employees think of the company?‘, q2: What are the most common reasons for employees to leave Best Buy?’, q3: ‘Do employees feel understaffed?’) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570652fd-3956-4bd4-abe8-69def98b597a",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2013f6-c9bc-4753-8699-fead9b755ec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Marking relevancy using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "1af95dba-7b8f-4526-93c8-8a2c05cf02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_prompt(context, question):\n",
    "    template = \"\"\"  You are an AI agent that classifies context as \"relevant\" or \"irrelevant\" depending on the question. \n",
    "                    Given the question and the context below classify weather the context is relevant or irrelevant for answering the given question. \n",
    "                    Do not give any justification for your descision. For relevant context answer with 1 and for irrelevant context answer with 0.\n",
    "                    Question: {question}\n",
    "                    Context: {context}\n",
    "                    Response:\n",
    "                    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "    prompt_formatted_str: str = prompt.format(context=context, question=question)\n",
    "    return prompt_formatted_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "22a437d3-8317-44de-9e97-b484875a32ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1.',\n",
       " 1: '1.',\n",
       " 2: '0.',\n",
       " 3: '1.',\n",
       " 4: '0.',\n",
       " 5: '0.',\n",
       " 6: '1.',\n",
       " 7: '1.',\n",
       " 8: '0.',\n",
       " 9: '1.',\n",
       " 10: '0.',\n",
       " 11: '1.',\n",
       " 12: '0.',\n",
       " 13: '0.',\n",
       " 14: '0.',\n",
       " 15: '0.',\n",
       " 16: '0.',\n",
       " 17: '1.',\n",
       " 18: '0.',\n",
       " 19: '0.',\n",
       " 20: '0.',\n",
       " 21: '0.',\n",
       " 22: '0.',\n",
       " 23: '1',\n",
       " 24: '0.',\n",
       " 25: '1.',\n",
       " 26: '1.',\n",
       " 27: '0.',\n",
       " 28: '0.',\n",
       " 29: '0.'}"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_classify_mistral[3] = '1.'\n",
    "q1_classify_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "76cb1891-3133-4472-aa77-0faaf0c459b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  1.\n",
      "1  :  1.\n",
      "2  :  0.\n",
      "3  :  1. The context mentions opinions of Best Buy employees, which is relevant to the question.\n",
      "4  :  0.\n",
      "5  :  0.\n",
      "6  :  1.\n",
      "7  :  1.\n",
      "8  :  0.\n",
      "9  :  1.\n",
      "10  :  0.\n",
      "11  :  1.\n",
      "12  :  0.\n",
      "13  :  0.\n",
      "14  :  0.\n",
      "15  :  0.\n",
      "16  :  0.\n",
      "17  :  1.\n",
      "18  :  0.\n",
      "19  :  0.\n",
      "20  :  0.\n",
      "21  :  0.\n",
      "22  :  0.\n",
      "23  :  1\n",
      "24  :  0.\n",
      "25  :  1.\n",
      "26  :  1.\n",
      "27  :  0.\n",
      "28  :  0.\n",
      "29  :  0.\n"
     ]
    }
   ],
   "source": [
    "q1_classify_mistral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mistral.invoke(classification_prompt(list(q1_summarized_contexts.context)[i], questions[0]))\n",
    "    q1_classify_mistral.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "42539ee5-05f2-4ce7-b6cd-53e9124ae1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1',\n",
       " 1: '1',\n",
       " 2: '0',\n",
       " 3: '1',\n",
       " 4: '0',\n",
       " 5: '0',\n",
       " 6: '0',\n",
       " 7: '1',\n",
       " 8: '1',\n",
       " 9: '1',\n",
       " 10: '0',\n",
       " 11: '1',\n",
       " 12: '0',\n",
       " 13: '0',\n",
       " 14: '0',\n",
       " 15: '0',\n",
       " 16: '0',\n",
       " 17: '1',\n",
       " 18: '0',\n",
       " 19: '0',\n",
       " 20: '0',\n",
       " 21: '0',\n",
       " 22: '0',\n",
       " 23: '0',\n",
       " 24: '0',\n",
       " 25: '0',\n",
       " 26: '1',\n",
       " 27: '0',\n",
       " 28: '0',\n",
       " 29: '0'}"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "c50fb237-5914-481e-804a-c62fc5843377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n",
      "0  :  1\n",
      "1  :  1\n",
      "2  :  0\n",
      "3  :  1\n",
      "4  :  0\n",
      "5  :  0\n",
      "6  :  0\n",
      "7  :  1\n",
      "8  :  1\n",
      "                    \"\"\"\n",
      "9  :  1\n",
      "10  :  0\n",
      "11  :  1\n",
      "12  :  0\n",
      "13  :  0\n",
      "14  :  0\n",
      "15  :  0\n",
      "16  :  0\n",
      "17  :  1\n",
      "18  :  0\n",
      "19  :  0\n",
      "20  :  0\n",
      "21  :  0\n",
      "22  :  0\n",
      "23  :  0\n",
      "24  :  0\n",
      "25  :  0\n",
      "26  :  1\n",
      "27  :  0\n",
      "28  :  0\n",
      "29  :  0\n"
     ]
    }
   ],
   "source": [
    "llm_mixtral_T =  HuggingFaceEndpoint(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1', huggingfacehub_api_token=huggingfacehub_api_token, max_new_tokens=30000, temperature=0.01)\n",
    "q1_classify_mixtral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q1_summarized_contexts.context)[i], questions[0]))\n",
    "    q1_classify_mixtral.update({i: response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f051d9b2-1c4d-483b-ad50-be50cd325303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/hraj/.cache/huggingface/token\n",
      "Login successful\n",
      "29  :  0\n"
     ]
    }
   ],
   "source": [
    "for i in range(29, 30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q1_summarized_contexts.context)[i], questions[0]))\n",
    "    q1_classify_mixtral_T.append({i:response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "173ba795-02b8-44fb-bdb9-ebcf2192b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :   Relevant.\n",
      "               The question asks about employees' feelings, and the context provides detailed information about employees' feelings towards their work environment, including their perception of being understaffed.\n",
      "1  :   Relevant or Irrelevant: irrelevent.\n",
      "               The context does not provide any information related to the question about employees feeling understaffed. It is focused on a lost/stolen gift card issue and potential abuse of discounts.\n",
      "2  :   Answer: irrelevent. The context does not provide any information related to the question about employees feeling understaffed.\n",
      "3  :   Relevant: irrelevent\n",
      "               The context does not provide any information related to the question about employees feeling understaffed at Best Buy.\n",
      "4  :   \"\"\"\n",
      "\n",
      "Answer: irrelevent. The context does not provide any information related to employees feeling understaffed.\n",
      "5  :   Relevant: irrelevent.\n",
      "               The context does not provide any information about the overall staffing levels or the feelings of employees regarding understaffing. It focuses on the specific situation of an individual who suspects they are being silently fired after returning from medical leave.\n",
      "6  :   Answer: irrelevent. The context discusses advice given to a new Mobile Sales Supervisor about managing their team and giving sales to their crew, but it does not provide any information about employees feeling understaffed.\n",
      "7  :   Relevant. The context mentions understaffing as a concern expressed by some participants.\n",
      "8  :   Relevant: irrelevent.\n",
      "               The context does not provide any information about the number of employees or their feelings towards being understaffed. It focuses on the new uniforms and the preferences of some employees regarding the color and material.\n",
      "9  :   Relevant: irrelevent.\n",
      "               The context does not provide any information about employees feeling understaffed. It discusses the legality and fairness of demoting full-time employees to part-time based on revenue expectations.\n",
      "10  :   Relevant: irrelevent\n",
      "                The context does not provide any information related to the question about employees feeling understaffed.\n",
      "11  :  \n",
      "Answer: irrelevent. The context does not provide any information related to employees feeling understaffed.\n",
      "12  :   Answer: irrelevent. The context does not provide any information about employees feeling understaffed.\n",
      "13  :   Answer: irrelevant\n",
      "               The context does not provide any direct information about employees feeling understaffed. It primarily discusses the topic of unionization and the various arguments for and against it. While some participants mention increased workload, this is not the same as feeling understaffed. Therefore, the context is irrelevant for answering the question.\n",
      "14  :   Relevant.\n",
      "               The question asks about employees' feelings, and the context provides detailed information about employees' feelings towards their work environment, including their perception of being understaffed.\n",
      "15  :   Answer: irrelevent.\n",
      "                Explanation: The context does not provide any information about the number of employees or their feelings about being understaffed. It focuses on the eLearning process and the importance of hands-on experience and additional learning.\n",
      "16  :   Relevant:\n",
      "                The context is relevant as it discusses potential changes in staffing levels within the company, which could impact the feeling of understaffing among employees. The mention of potential job cuts in leadership roles and the assessment of other managerial positions could suggest that there may be a reduction in overall staffing levels, leading to employees feeling understaffed. Additionally, the rumors of new positions could indicate a restructuring of the company, which could also impact staffing levels and the feelings of understaffing among employees. The context also mentions the possibility of PTO not being paid out in some states upon termination, which could be a concern for employees and potentially contribute to feelings of understaffing if they are unable to take necessary time off. Overall, the context provides relevant information that could help answer the question.\n",
      "17  :   Relevant or Irrelevant: irrelevent.\n",
      "               The context does not provide any information about employees feeling understaffed. It focuses on the decreasing appeal of working at Best Buy due to factors such as low wages, weak employee discounts, and lack of accommodations. While these factors could potentially impact employee morale and job satisfaction, they do not directly address the question of whether employees feel understaffed.\n",
      "18  :   Relevant: irrelevent.\n",
      "                The context does not provide any information related to employees feeling understaffed. It is focused on the topic of tips and the associated policies and risks.\n",
      "19  :   Relevant: irrelevent.\n",
      "               The context does not provide any information related to the number of employees or their feelings towards being understaffed. It is solely focused on the availability of 30x graphics cards in various stores.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    response = llm_mistral.invoke(classification_prompt(q3_irr_summary_mix[i], questions[2]))\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "eb7124a2-5b8a-4269-b155-0662dda31122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :   Answer: relevant.\n",
      "               The context mentions \"insufficient staffing levels\" which directly relates to the question about employees feeling understaffed.\n",
      "1  :   ---\n",
      "\n",
      "Relevant. The context directly addresses the issue of understaffing and its impact on employees at Best Buy. The employees' statements about long wait times for customers, inability to take breaks, and the company's policy of not scheduling enough staff for specialized departments all point to a clear issue of understaffing. The context also provides evidence of the negative consequences of understaffing, such as decreased morale and mental health, and suggests potential solutions to the problem.\n",
      "2  :   Relevant.\n",
      "               The question asks about the feelings of employees and the context provides information about their dissatisfaction and frustration, which can include feeling understaffed.\n",
      "3  :   Answer: irrelevant.\n",
      "                Explanation: The context discusses various issues related to job dissatisfaction and challenges faced by retail workers, but it does not directly mention anything about employees feeling understaffed. Therefore, the context is irrelevant to answering the question.\n",
      "4  :   Relevant: The context is relevant as it discusses the upcoming layoffs at Best Buy, specifically for Assistant Managers, which is related to the question about employees feeling understaffed. The context mentions that AMs are getting let go, which directly answers the question. Additionally, the context mentions that some stores are planning to reduce their staff by half, which further supports the idea that employees may feel understaffed.\n",
      "5  :   Answer: irrelevent.\n",
      "               The context does not provide any information about the overall staffing levels or feelings of employees at Best Buy. It focuses on the job application process, pay rates, benefits, and the individual's personal concerns.\n",
      "6  :   Relevant.\n",
      "               The question asks about employees feeling understaffed, and the context discusses the experiences of SP drivers who express frustration with inconsistent workloads and heavy workloads due to a lack of staff.\n",
      "7  :   Relevant.\n",
      "               The context discusses the increased workload and expectations placed on employees, which could contribute to feelings of understaffing and being overwhelmed. The mention of the lack of support from sales advisors and the upcoming change of vendors bringing in their own staff also relates to the question.\n",
      "8  :   Relevant: The context is relevant as it discusses the workload and staffing levels in the sales environment, which could potentially impact the employees' feelings of being understaffed. The reduction in seasonal staff and the high expectations from management are specific factors that could contribute to employees feeling understaffed.\n",
      "9  :   Answer: relevant. The context mentions staffing cuts and hiring difficulties, which are directly related to the question about employees feeling understaffed.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = llm_mistral.invoke(classification_prompt(q3_summary_mix[i], questions[2]))\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b883ee-4d28-45ab-947a-e69fdedd1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Relevant among irrelevants = {0, 7, 12?, 14}\n",
    "\n",
    "Irrelevant among relevants = {3, 5?, 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed8d55-55d3-4605-9d60-aa0a811711b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Making the test dataset for Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "531e2eb5-8d32-404b-a8d6-34229c1b92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "e1340607-6de6-44c0-b9fb-f51dcd0f61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_final_dataset = q1_summarized_contexts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "aa9c1a43-322f-4cdd-aff2-b440066f19e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_199f5h8</td>\n",
       "      <td>\\nThe conversation is about tips and advice fo...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_wapsy3</td>\n",
       "      <td>\\nThe conversation revolves around a potential...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_1aucdoj</td>\n",
       "      <td>\\nThe conversation revolves around a seasonal ...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_yuacvb</td>\n",
       "      <td>\\nThe conversation is about getting a job at B...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_12scwfu</td>\n",
       "      <td>\\nThe conversation is about the potential cutt...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_j5vbke</td>\n",
       "      <td>Summary: \\n\\nThe conversation is about a form...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_14i1ppn</td>\n",
       "      <td>\\nThe conversation revolves around an individu...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_ywjf2p</td>\n",
       "      <td>\\nIn this conversation, an individual who has ...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_16f8ctj</td>\n",
       "      <td>\"\"\"\\n                This conversation revolv...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_134bzme</td>\n",
       "      <td>\\nSummary: The conversation revolves around th...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3_15e1vvl</td>\n",
       "      <td>\\n---\\n\\nThe subreddit r/BestBuyWorkers has de...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3_qafhhx</td>\n",
       "      <td>\\nThe conversation revolves around the idea of...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3_12rblul</td>\n",
       "      <td>\\nThe conversation is about the project team a...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t3_omfjc4</td>\n",
       "      <td>\\nThe conversation is about NVIDIA GPU drop on...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3_1atycal</td>\n",
       "      <td>|\\n                A conversation titled \"Sch...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t3_18tbjcs</td>\n",
       "      <td>\\nThe conversation revolves around the topic o...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t3_13q5tph</td>\n",
       "      <td>\\nThe conversation revolves around the topic o...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t3_17vp23a</td>\n",
       "      <td>\\nThe user is facing a dilemma about whether t...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t3_q1fhwu</td>\n",
       "      <td>\\nSummary: The user has applied for a job at B...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t3_12usy5i</td>\n",
       "      <td>\\nThe conversation revolves around workers sha...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t3_18sha15</td>\n",
       "      <td>\\nThe conversation revolves around negative ex...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t3_yw5weg</td>\n",
       "      <td>\\nThe conversation is about an individual who ...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t3_jwlmpz</td>\n",
       "      <td>\\nThe conversation revolves around a customer'...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>t3_18rkcrr</td>\n",
       "      <td>The conversation revolves around an individua...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t3_t1if6w</td>\n",
       "      <td>\\nThe conversation is about a job interview fo...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>t3_16fik5c</td>\n",
       "      <td>\\nThe conversation revolves around the topic o...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>t3_z1mzn6</td>\n",
       "      <td>\\nThe conversation is about a planned walkout ...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>t3_1b7n0jo</td>\n",
       "      <td>\\nThe conversation titled 'achievers' revolves...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>t3_16b5ojs</td>\n",
       "      <td>Summary: The conversation revolves around the...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>t3_184op4l</td>\n",
       "      <td>\\nIn this conversation, participants discuss n...</td>\n",
       "      <td>What do Best Buy employees think of the company?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     reddit_id                                            context  \\\n",
       "0   t3_199f5h8  \\nThe conversation is about tips and advice fo...   \n",
       "1    t3_wapsy3  \\nThe conversation revolves around a potential...   \n",
       "2   t3_1aucdoj  \\nThe conversation revolves around a seasonal ...   \n",
       "3    t3_yuacvb  \\nThe conversation is about getting a job at B...   \n",
       "4   t3_12scwfu  \\nThe conversation is about the potential cutt...   \n",
       "5    t3_j5vbke   Summary: \\n\\nThe conversation is about a form...   \n",
       "6   t3_14i1ppn  \\nThe conversation revolves around an individu...   \n",
       "7    t3_ywjf2p  \\nIn this conversation, an individual who has ...   \n",
       "8   t3_16f8ctj   \"\"\"\\n                This conversation revolv...   \n",
       "9   t3_134bzme  \\nSummary: The conversation revolves around th...   \n",
       "10  t3_15e1vvl  \\n---\\n\\nThe subreddit r/BestBuyWorkers has de...   \n",
       "11   t3_qafhhx  \\nThe conversation revolves around the idea of...   \n",
       "12  t3_12rblul  \\nThe conversation is about the project team a...   \n",
       "13   t3_omfjc4  \\nThe conversation is about NVIDIA GPU drop on...   \n",
       "14  t3_1atycal   |\\n                A conversation titled \"Sch...   \n",
       "15  t3_18tbjcs  \\nThe conversation revolves around the topic o...   \n",
       "16  t3_13q5tph  \\nThe conversation revolves around the topic o...   \n",
       "17  t3_17vp23a  \\nThe user is facing a dilemma about whether t...   \n",
       "18   t3_q1fhwu  \\nSummary: The user has applied for a job at B...   \n",
       "19  t3_12usy5i  \\nThe conversation revolves around workers sha...   \n",
       "20  t3_18sha15  \\nThe conversation revolves around negative ex...   \n",
       "21   t3_yw5weg  \\nThe conversation is about an individual who ...   \n",
       "22   t3_jwlmpz  \\nThe conversation revolves around a customer'...   \n",
       "23  t3_18rkcrr   The conversation revolves around an individua...   \n",
       "24   t3_t1if6w  \\nThe conversation is about a job interview fo...   \n",
       "25  t3_16fik5c  \\nThe conversation revolves around the topic o...   \n",
       "26   t3_z1mzn6  \\nThe conversation is about a planned walkout ...   \n",
       "27  t3_1b7n0jo  \\nThe conversation titled 'achievers' revolves...   \n",
       "28  t3_16b5ojs   Summary: The conversation revolves around the...   \n",
       "29  t3_184op4l  \\nIn this conversation, participants discuss n...   \n",
       "\n",
       "                                            question  \n",
       "0   What do Best Buy employees think of the company?  \n",
       "1   What do Best Buy employees think of the company?  \n",
       "2   What do Best Buy employees think of the company?  \n",
       "3   What do Best Buy employees think of the company?  \n",
       "4   What do Best Buy employees think of the company?  \n",
       "5   What do Best Buy employees think of the company?  \n",
       "6   What do Best Buy employees think of the company?  \n",
       "7   What do Best Buy employees think of the company?  \n",
       "8   What do Best Buy employees think of the company?  \n",
       "9   What do Best Buy employees think of the company?  \n",
       "10  What do Best Buy employees think of the company?  \n",
       "11  What do Best Buy employees think of the company?  \n",
       "12  What do Best Buy employees think of the company?  \n",
       "13  What do Best Buy employees think of the company?  \n",
       "14  What do Best Buy employees think of the company?  \n",
       "15  What do Best Buy employees think of the company?  \n",
       "16  What do Best Buy employees think of the company?  \n",
       "17  What do Best Buy employees think of the company?  \n",
       "18  What do Best Buy employees think of the company?  \n",
       "19  What do Best Buy employees think of the company?  \n",
       "20  What do Best Buy employees think of the company?  \n",
       "21  What do Best Buy employees think of the company?  \n",
       "22  What do Best Buy employees think of the company?  \n",
       "23  What do Best Buy employees think of the company?  \n",
       "24  What do Best Buy employees think of the company?  \n",
       "25  What do Best Buy employees think of the company?  \n",
       "26  What do Best Buy employees think of the company?  \n",
       "27  What do Best Buy employees think of the company?  \n",
       "28  What do Best Buy employees think of the company?  \n",
       "29  What do Best Buy employees think of the company?  "
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_final_dataset.insert(2, 'question', [questions[0]]*30)\n",
    "q1_final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ca3c8059-e40d-47c4-9f01-68f02abdacdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1.',\n",
       " 1: '1.',\n",
       " 2: '0.',\n",
       " 3: '1.',\n",
       " 4: '0.',\n",
       " 5: '0.',\n",
       " 6: '1.',\n",
       " 7: '1.',\n",
       " 8: '0.',\n",
       " 9: '1.',\n",
       " 10: '0.',\n",
       " 11: '1.',\n",
       " 12: '0.',\n",
       " 13: '0.',\n",
       " 14: '0.',\n",
       " 15: '0.',\n",
       " 16: '0.',\n",
       " 17: '1.',\n",
       " 18: '0.',\n",
       " 19: '0.',\n",
       " 20: '0.',\n",
       " 21: '0.',\n",
       " 22: '0.',\n",
       " 23: '1',\n",
       " 24: '0.',\n",
       " 25: '1.',\n",
       " 26: '1.',\n",
       " 27: '0.',\n",
       " 28: '0.',\n",
       " 29: '0.'}"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q1_classify_mixtral[8] = '1'\n",
    "q1_classify_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "a35c01e1-3c9a-45cb-a342-45d13d089288",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = list(q1_classify_mixtral.values())\n",
    "q1_classify_mixtral_1 = [int(ele) for ele in crap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "13908f01-3198-4359-ab32-f21e8a63f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = list(q1_classify_mistral.values())\n",
    "q1_classify_mistral_1 = [float(ele) for ele in crap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "99606cd8-3052-4c17-887e-5db3504fab12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_classify_mistral_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "3ffb1c89-8558-45b7-9ffb-7c0d1a94a22e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_classify_mixtral_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "0e0fcc58-560c-443d-862b-f9c6a90bd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_final_dataset = q1_final_dataset.assign(**{'mistral' : q1_classify_mistral_1, 'mixtral' : q1_classify_mixtral_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "2ae81830-da1d-4cc7-bb29-6067374063a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(q1_final_dataset.mistral).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "7571497f-b12e-465c-995d-222f6b1b7a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(q1_final_dataset.mixtral).count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cc457-ea70-4c94-b3ef-d9c35d738f19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767941a5-b2d3-4165-bae7-d341906ff7cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Marking relevancy using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d28d7833-bc2c-4356-9244-e8fa53b033af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'reddit_id' :get_ids(q2_dataset_relevant), 'context': q2_summary_mix})\n",
    "df2 = pd.DataFrame({'reddit_id' :get_ids(q2_dataset_not_relevant), 'context': q2_irr_summary_mix})\n",
    "q2_summarized_contexts = pd.concat([df1, df2]).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "37f0e269-7305-4dd8-9970-a627dd54760f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  0.\n",
      "1  :  0.\n",
      "2  :  1.\n",
      "3  :  1.\n",
      "4  :  0.\n",
      "5  :  1\n",
      "6  :  1\n",
      "7  :  1.\n",
      "8  :  1.\n",
      "9  :  1.\n",
      "10  :  1.\n",
      "                    The context mentions that there is no job security in retail and the ability to adapt is crucial, which could be relevant to the reasons why employees leave Best Buy.\n",
      "11  :  0.\n",
      "12  :  1.\n",
      "13  :  0.\n",
      "14  :  0.\n",
      "15  :  0.\n",
      "16  :  0.\n",
      "17  :  0.\n",
      "18  :  0.\n",
      "19  :  1.\n",
      "20  :  1.\n",
      "21  :  0.\n",
      "22  :  1.\n",
      "23  :  1.\n",
      "24  :  1.\n",
      "25  :  0.\n",
      "26  :  0.\n",
      "27  :  0.\n",
      "28  :  0.\n",
      "29  :  0.\n"
     ]
    }
   ],
   "source": [
    "q2_classify_mistral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mistral.invoke(classification_prompt(list(q2_summarized_contexts.context)[i], questions[1]))\n",
    "    q2_classify_mistral.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "c891129a-31c5-4ade-acc1-9d31e89a5b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  0\n",
      "1  :  0\n",
      "2  :  1\n",
      "3  :  1\n",
      "4  :  0\n",
      "5  :  0\n",
      "6  :  0\n",
      "7  :  0\n",
      "8  :  0\n",
      "9  :  0\n",
      "10  :  1\n",
      "11  :  0\n",
      "12  :  1\n",
      "13  :  0\n",
      "14  :  0\n",
      "15  :  0\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: QvZvAwRGJHr5tFyFVZ5Wc)\n\nModel is overloaded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[514], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m q2_classify_mixtral \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_mixtral_T\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassification_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq2_summarized_contexts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     q2_classify_mixtral\u001b[38;5;241m.\u001b[39mupdate({i: response})\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[0;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 593\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1209\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1208\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1209\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[1;32m   1213\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/huggingface_endpoint.py:256\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: QvZvAwRGJHr5tFyFVZ5Wc)\n\nModel is overloaded"
     ]
    }
   ],
   "source": [
    "q2_classify_mixtral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q2_summarized_contexts.context)[i], questions[1]))\n",
    "    q2_classify_mixtral.update({i: response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "ad6cb74d-d899-4119-b69e-fb735bf73463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29  :  0\n"
     ]
    }
   ],
   "source": [
    "for i in range(29, 30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q2_summarized_contexts.context)[i], questions[1]))\n",
    "    q2_classify_mixtral.update({i: response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4594c91-cc1d-4203-8414-d7ca41775bdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Making the test dataset for Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "d4756f79-2d2c-4b42-a9b6-b2b12e8f8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_final_dataset = q2_summarized_contexts.copy()\n",
    "q2_final_dataset.insert(2, 'question', [questions[1]]*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "6a6e0bf7-49b9-4123-b448-051d6f431fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0.',\n",
       " 1: '0.',\n",
       " 2: '1.',\n",
       " 3: '1.',\n",
       " 4: '0.',\n",
       " 5: '1',\n",
       " 6: '1',\n",
       " 7: '1.',\n",
       " 8: '1.',\n",
       " 9: '1.',\n",
       " 10: '1',\n",
       " 11: '0.',\n",
       " 12: '1.',\n",
       " 13: '0.',\n",
       " 14: '0.',\n",
       " 15: '0.',\n",
       " 16: '0.',\n",
       " 17: '0.',\n",
       " 18: '0.',\n",
       " 19: '1.',\n",
       " 20: '1.',\n",
       " 21: '0.',\n",
       " 22: '1.',\n",
       " 23: '1.',\n",
       " 24: '1.',\n",
       " 25: '0.',\n",
       " 26: '0.',\n",
       " 27: '0.',\n",
       " 28: '0.',\n",
       " 29: '0.'}"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_classify_mistral[10] = '1'\n",
    "q2_classify_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "aba32dd0-7a73-45fb-8de1-a9d4d0a479c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = list(q2_classify_mixtral.values())\n",
    "q2_classify_mixtral_1 = [int(ele) for ele in crap]\n",
    "crap = list(q2_classify_mistral.values())\n",
    "q2_classify_mistral_1 = [float(ele) for ele in crap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "656c34c0-6c67-4691-8b90-6302a4ae20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_final_dataset = q2_final_dataset.assign(**{'mistral' : q2_classify_mistral_1, 'mixtral' : q2_classify_mixtral_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "5f622fea-4980-451f-ab6a-2da53c64c084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>mistral</th>\n",
       "      <th>mixtral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_yy085z</td>\n",
       "      <td>\\nThe conversation revolves around the topic o...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1580byk</td>\n",
       "      <td>The conversation revolves around the incentiv...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_qhb9v1</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1499vuh</td>\n",
       "      <td>\\nThe conversation is about the idea of unioni...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_wtr1q0</td>\n",
       "      <td>\\nThe conversation revolves around a prankster...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_18rkcrr</td>\n",
       "      <td>The conversation revolves around an individua...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_qpi3rj</td>\n",
       "      <td>\\n---\\n\\nThe person is anxious about an upcomi...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_1aj1kcj</td>\n",
       "      <td>\\nThe conversation revolves around the issue o...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_11dxaol</td>\n",
       "      <td>\\nThe conversation is about a person who is ne...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_18ttsxk</td>\n",
       "      <td>\\nThe conversation is about an individual shar...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3_199f5h8</td>\n",
       "      <td>\\nThe conversation is about tips and advice fo...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3_1ap3bt3</td>\n",
       "      <td>\\nThe conversation is about whether internal a...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3_17138re</td>\n",
       "      <td>\\nThe conversation is about a walkout/callout ...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t3_19843n7</td>\n",
       "      <td>\\nThe conversation revolves around the challen...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3_15427rb</td>\n",
       "      <td>\\nThe conversation revolves around the new cus...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t3_z7kfmo</td>\n",
       "      <td>\\nIn this conversation, the individual shares ...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t3_178w5qv</td>\n",
       "      <td>\\nIn this conversation, an 18-year-old college...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t3_15qii74</td>\n",
       "      <td>\\nThe conversation revolves around the best TV...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t3_13poqek</td>\n",
       "      <td>\\nThe conversation is about an employee seekin...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t3_1b114ek</td>\n",
       "      <td>\\nThe conversation is between an employee and ...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t3_1b114ek</td>\n",
       "      <td>\\nThe conversation is between an employee and ...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t3_132bzni</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t3_14d6fhg</td>\n",
       "      <td>\\nThe conversation revolves around people's se...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>t3_146p5ca</td>\n",
       "      <td>\\nSummary: The individual is facing difficulty...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t3_179amsj</td>\n",
       "      <td>\\nThe conversation is about an individual who ...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>t3_jf2d05</td>\n",
       "      <td>Summary: The conversation revolves around the...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>t3_jx0q4y</td>\n",
       "      <td>\\nIn this conversation, a new employee at Best...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>t3_15qii74</td>\n",
       "      <td>\\nThe conversation revolves around the best TV...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>t3_19bjliq</td>\n",
       "      <td>\\nThe conversation revolves around a store not...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>t3_klk0z1</td>\n",
       "      <td>\\nThe conversation revolves around the duratio...</td>\n",
       "      <td>What are the most common reasons for employees...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     reddit_id                                            context  \\\n",
       "0    t3_yy085z  \\nThe conversation revolves around the topic o...   \n",
       "1   t3_1580byk   The conversation revolves around the incentiv...   \n",
       "2    t3_qhb9v1  \\nIn this conversation, the participants discu...   \n",
       "3   t3_1499vuh  \\nThe conversation is about the idea of unioni...   \n",
       "4    t3_wtr1q0  \\nThe conversation revolves around a prankster...   \n",
       "5   t3_18rkcrr   The conversation revolves around an individua...   \n",
       "6    t3_qpi3rj  \\n---\\n\\nThe person is anxious about an upcomi...   \n",
       "7   t3_1aj1kcj  \\nThe conversation revolves around the issue o...   \n",
       "8   t3_11dxaol  \\nThe conversation is about a person who is ne...   \n",
       "9   t3_18ttsxk  \\nThe conversation is about an individual shar...   \n",
       "10  t3_199f5h8  \\nThe conversation is about tips and advice fo...   \n",
       "11  t3_1ap3bt3  \\nThe conversation is about whether internal a...   \n",
       "12  t3_17138re  \\nThe conversation is about a walkout/callout ...   \n",
       "13  t3_19843n7  \\nThe conversation revolves around the challen...   \n",
       "14  t3_15427rb  \\nThe conversation revolves around the new cus...   \n",
       "15   t3_z7kfmo  \\nIn this conversation, the individual shares ...   \n",
       "16  t3_178w5qv  \\nIn this conversation, an 18-year-old college...   \n",
       "17  t3_15qii74  \\nThe conversation revolves around the best TV...   \n",
       "18  t3_13poqek  \\nThe conversation is about an employee seekin...   \n",
       "19  t3_1b114ek  \\nThe conversation is between an employee and ...   \n",
       "20  t3_1b114ek  \\nThe conversation is between an employee and ...   \n",
       "21  t3_132bzni  \\nIn this conversation, the participants discu...   \n",
       "22  t3_14d6fhg  \\nThe conversation revolves around people's se...   \n",
       "23  t3_146p5ca  \\nSummary: The individual is facing difficulty...   \n",
       "24  t3_179amsj  \\nThe conversation is about an individual who ...   \n",
       "25   t3_jf2d05   Summary: The conversation revolves around the...   \n",
       "26   t3_jx0q4y  \\nIn this conversation, a new employee at Best...   \n",
       "27  t3_15qii74  \\nThe conversation revolves around the best TV...   \n",
       "28  t3_19bjliq  \\nThe conversation revolves around a store not...   \n",
       "29   t3_klk0z1  \\nThe conversation revolves around the duratio...   \n",
       "\n",
       "                                             question  mistral  mixtral  \n",
       "0   What are the most common reasons for employees...      0.0        0  \n",
       "1   What are the most common reasons for employees...      0.0        0  \n",
       "2   What are the most common reasons for employees...      1.0        1  \n",
       "3   What are the most common reasons for employees...      1.0        1  \n",
       "4   What are the most common reasons for employees...      0.0        0  \n",
       "5   What are the most common reasons for employees...      1.0        0  \n",
       "6   What are the most common reasons for employees...      1.0        0  \n",
       "7   What are the most common reasons for employees...      1.0        0  \n",
       "8   What are the most common reasons for employees...      1.0        0  \n",
       "9   What are the most common reasons for employees...      1.0        0  \n",
       "10  What are the most common reasons for employees...      1.0        1  \n",
       "11  What are the most common reasons for employees...      0.0        0  \n",
       "12  What are the most common reasons for employees...      1.0        1  \n",
       "13  What are the most common reasons for employees...      0.0        0  \n",
       "14  What are the most common reasons for employees...      0.0        0  \n",
       "15  What are the most common reasons for employees...      0.0        0  \n",
       "16  What are the most common reasons for employees...      0.0        0  \n",
       "17  What are the most common reasons for employees...      0.0        0  \n",
       "18  What are the most common reasons for employees...      0.0        0  \n",
       "19  What are the most common reasons for employees...      1.0        0  \n",
       "20  What are the most common reasons for employees...      1.0        0  \n",
       "21  What are the most common reasons for employees...      0.0        0  \n",
       "22  What are the most common reasons for employees...      1.0        0  \n",
       "23  What are the most common reasons for employees...      1.0        0  \n",
       "24  What are the most common reasons for employees...      1.0        1  \n",
       "25  What are the most common reasons for employees...      0.0        0  \n",
       "26  What are the most common reasons for employees...      0.0        0  \n",
       "27  What are the most common reasons for employees...      0.0        0  \n",
       "28  What are the most common reasons for employees...      0.0        0  \n",
       "29  What are the most common reasons for employees...      0.0        0  "
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "3073fec3-e9d2-4a65-95f8-759fc309cfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the most common reasons for employees to leave Best Buy?'"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "b747d4d1-1bd8-41fe-a3ad-9f4504843894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The conversation revolves around an individual who fell sick with a 102-degree fever and was denied sick leave by their supervisor. They tried to manage the situation with medication but ended up fainting at work. The manager allowed them to leave only after a customer intervened, accusing the company of child labor. The individual is now scheduled for 8-hour shifts until the next Saturday and is contemplating calling in sick every day.\n",
      "\n",
      "                The conversation includes advice from various perspectives. Some suggest quitting the job due to the manager's insensitivity and the company's policies. Others recommend learning about company policies and understanding employee rights. A common theme is the importance of health over job, with suggestions to document everything if there's any backlash. Some also mention that it's against Best Buy policy to ask for a doctor's note, and it's not supposed to be accepted even if given.\n",
      "\n",
      "                There are also comments about the inappropriateness of the manager's behavior, suggesting that it could lead to litigation due to injury. Some advise contacting HR, while others caution that HR is never on the employee's side. There are also reminders about the existence of sick banks and attendance policies, and the importance of using them correctly.\n",
      "\n",
      "                In summary, the conversation revolves around an individual's difficult work situation due to illness and poor management, with advice focusing on health, understanding company policies, and potential actions such as quitting or contacting HR.\n"
     ]
    }
   ],
   "source": [
    "print(q2_final_dataset.iloc[5].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "9b3463eb-9bf3-4319-ad6f-da56a96d6c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The conversation revolves around an individual who fell sick with a 102-degree fever and was denied sick leave by their supervisor. They tried to manage the situation with medication but ended up fainting at work. The manager allowed them to leave only after a customer intervened, accusing the company of child labor. The individual is now scheduled for 8-hour shifts until the next Saturday and is contemplating calling in sick every day.\n",
      "\n",
      "                The conversation includes advice from various perspectives. Some suggest quitting the job due to the manager's insensitivity and the company's policies. Others recommend learning about company policies and understanding employee rights. A common theme is the importance of health over job, with suggestions to document everything if there's any backlash. Some also mention that it's against Best Buy policy to ask for a doctor's note, and it's not supposed to be accepted even if given.\n",
      "\n",
      "                There are also comments about the inappropriateness of the manager's behavior, suggesting that it could lead to litigation due to injury. Some advise contacting HR, while others caution that HR is never on the employee's side. There are also reminders about the existence of sick banks and attendance policies, and the importance of using them correctly.\n",
      "\n",
      "                In summary, the conversation revolves around an individual's difficult work situation due to illness and poor management, with advice focusing on health, understanding company policies, and potential actions such as quitting or contacting HR.\n"
     ]
    }
   ],
   "source": [
    "print(q2_summarized_contexts.iloc[5].context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "991cb3ec-705c-422f-a476-593110287e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(list(q2_final_dataset.mistral).count(1))\n",
    "print(list(q2_final_dataset.mixtral).count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebb8bb-4d73-40de-9fc6-dfcad51b3bc3",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4226bd-5ced-4917-8c16-021d7b1d1fdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Marking relevancy using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "cf208215-188e-4737-b50a-a35d465d7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'reddit_id' :get_ids(q3_dataset_relevant), 'context': q3_summary_mix})\n",
    "df2 = pd.DataFrame({'reddit_id' :get_ids(q3_dataset_not_relevant), 'context': q3_irr_summary_mix})\n",
    "q3_summarized_contexts = pd.concat([df1, df2]).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "a1b127af-1ec6-4c23-b05f-b88d27810aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  1.\n",
      "1  :  1\n",
      "2  :  1.\n",
      "3  :  0.\n",
      "4  :  1\n",
      "5  :  0.\n",
      "6  :  1.\n",
      "7  :  1.\n",
      "8  :  1.\n",
      "9  :  1.\n",
      "10  :  1.\n",
      "11  :  0.\n",
      "12  :  0.\n",
      "13  :  0.\n",
      "14  :  0.\n",
      "15  :  0.\n",
      "16  :  0.\n",
      "17  :  1.\n",
      "18  :  0.\n",
      "19  :  0.\n",
      "20  :  0.\n",
      "21  :  0.\n",
      "22  :  0.\n",
      "23  :  0.\n",
      "24  :  1.\n",
      "25  :  0.\n",
      "26  :  0.\n",
      "27  :  0.\n",
      "28  :  0.\n",
      "29  :  0.\n"
     ]
    }
   ],
   "source": [
    "q3_classify_mistral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mistral.invoke(classification_prompt(list(q3_summarized_contexts.context)[i], questions[2]))\n",
    "    q3_classify_mistral.update({i: response})\n",
    "    print(i, \" : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "27c3db13-7e44-4995-9f53-db5264886c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  1\n",
      "1  :  1\n",
      "2  :  1\n",
      "3  :  0\n",
      "4  :  1\n",
      "5  :  0\n",
      "6  :  1\n",
      "7  :  1\n",
      "8  :  1\n",
      "9  :  1\n",
      "10  :  1\n",
      "11  :  0\n",
      "12  :  0\n",
      "13  :  0\n",
      "14  :  0\n",
      "15  :  0\n",
      "16  :  0\n",
      "17  :  1\n",
      "18  :  0\n",
      "19  :  0\n",
      "20  :  0\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: vU7KO3B59xMdubHgwz5QR)\n\nModel is overloaded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[519], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m q3_classify_mixtral \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_mixtral_T\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassification_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mq3_summarized_contexts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     q3_classify_mixtral\u001b[38;5;241m.\u001b[39mupdate({i: response})\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    260\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         )\n\u001b[1;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     ]\n\u001b[0;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 593\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1209\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1208\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1209\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[1;32m   1213\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/huggingface_endpoint.py:256\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:242\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:362\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: vU7KO3B59xMdubHgwz5QR)\n\nModel is overloaded"
     ]
    }
   ],
   "source": [
    "q3_classify_mixtral = {}\n",
    "\n",
    "for i in range(30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q3_summarized_contexts.context)[i], questions[2]))\n",
    "    q3_classify_mixtral.update({i: response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "b546124a-8855-41e4-be0a-5445b4802e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21  :  0\n",
      "22  :  0\n",
      "23  :  0\n",
      "24  :  1\n",
      "25  :  0\n",
      "26  :  0\n",
      "27  :  0\n",
      "28  :  0\n",
      "29  :  0\n"
     ]
    }
   ],
   "source": [
    "for i in range(21, 30):\n",
    "    response = llm_mixtral_T.invoke(classification_prompt(list(q3_summarized_contexts.context)[i], questions[2]))\n",
    "    q3_classify_mixtral.update({i: response})\n",
    "    print(i, \" : \", response)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ccfe5-6ead-430b-b502-bb12167fdb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e7af46-a258-4562-9bf2-836e1e76eda8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Making the test dataset for Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "b841f4f5-0269-40d0-b411-09e9d10e468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_final_dataset = q3_summarized_contexts.copy()\n",
    "q3_final_dataset.insert(2, 'question', [questions[2]]*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "380aede1-fbe8-49eb-a61c-8d76db42bc8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1',\n",
       " 1: '1',\n",
       " 2: '1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '0',\n",
       " 6: '1',\n",
       " 7: '1',\n",
       " 8: '1',\n",
       " 9: '1',\n",
       " 10: '1',\n",
       " 11: '0',\n",
       " 12: '0',\n",
       " 13: '0',\n",
       " 14: '0',\n",
       " 15: '0',\n",
       " 16: '0',\n",
       " 17: '1',\n",
       " 18: '0',\n",
       " 19: '0',\n",
       " 20: '0',\n",
       " 21: '0',\n",
       " 22: '0',\n",
       " 23: '0',\n",
       " 24: '1',\n",
       " 25: '0',\n",
       " 26: '0',\n",
       " 27: '0',\n",
       " 28: '0',\n",
       " 29: '0'}"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_classify_mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "5164dd09-e468-4866-8bdb-460e37525093",
   "metadata": {},
   "outputs": [],
   "source": [
    "crap = list(q3_classify_mixtral.values())\n",
    "q3_classify_mixtral_1 = [int(ele) for ele in crap]\n",
    "crap = list(q3_classify_mistral.values())\n",
    "q3_classify_mistral_1 = [float(ele) for ele in crap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "e110f57b-683d-4b64-857f-9c5c62f7671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_final_dataset = q3_final_dataset.assign(**{'mistral' : q3_classify_mistral_1, 'mixtral' : q3_classify_mixtral_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "1595ade8-3d9d-4fba-9a1c-fcbaeb29ef18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>mistral</th>\n",
       "      <th>mixtral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_v5thte</td>\n",
       "      <td>\\nThe conversation revolves around the desire ...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_sygsv3</td>\n",
       "      <td>\\n---\\n\\nThe conversation revolves around the ...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_16ha775</td>\n",
       "      <td>\\nThe conversation revolves around the difficu...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_16b9xbu</td>\n",
       "      <td>\\nThe conversation revolves around the theme o...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_136z3ru</td>\n",
       "      <td>The conversation titled 'Am cuts' revolves ar...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t3_zzj0lf</td>\n",
       "      <td>Summary: The individual is seeking informatio...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t3_r4f01w</td>\n",
       "      <td>\\nThe conversation revolves around the experie...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t3_1axres4</td>\n",
       "      <td>\\nThe conversation revolves around the frustra...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t3_18rrnnn</td>\n",
       "      <td>\\nThe conversation revolves around the frustra...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t3_qo85m9</td>\n",
       "      <td>\\nThe conversation revolves around the partici...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t3_17491o2</td>\n",
       "      <td>\\nThe conversation revolves around the experie...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t3_19evzih</td>\n",
       "      <td>\\nThe conversation revolves around a lost/stol...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t3_13zsm2d</td>\n",
       "      <td>\\nThe conversation revolves around a mole on C...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t3_14ngsbs</td>\n",
       "      <td>\\nThe conversation revolves around the deliver...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3_14mdjt5</td>\n",
       "      <td>\"\"\"\\nThe conversation revolves around employe...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t3_18wvtbt</td>\n",
       "      <td>\\nThe conversation revolves around an individu...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t3_pebp8a</td>\n",
       "      <td>\\nThe conversation is between a newly hired Mo...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t3_15dbudc</td>\n",
       "      <td>\\nIn this conversation, several individuals ex...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t3_17lnbeu</td>\n",
       "      <td>\\nThe conversation revolves around the introdu...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t3_19evcoa</td>\n",
       "      <td>\\nIn this conversation, participants discuss w...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>\\nThe conversation titled '☠☠☠' appears to be ...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t3_k2mp32</td>\n",
       "      <td>\\nThe conversation is about finding a work sch...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t3_1580byk</td>\n",
       "      <td>The conversation revolves around the incentiv...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>t3_18ff4a2</td>\n",
       "      <td>\\nThe conversation revolves around the topic o...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t3_17491o2</td>\n",
       "      <td>\\nThe conversation revolves around the experie...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>t3_jx0q4y</td>\n",
       "      <td>\\nIn this conversation, a new employee at Best...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>t3_1382vj6</td>\n",
       "      <td>\\nThe conversation revolves around potential j...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>t3_qhb9v1</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>t3_198a0ow</td>\n",
       "      <td>\\nIn this conversation, participants discuss t...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>t3_oo5tzv</td>\n",
       "      <td>\\nIn this conversation, the participants discu...</td>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     reddit_id                                            context  \\\n",
       "0    t3_v5thte  \\nThe conversation revolves around the desire ...   \n",
       "1    t3_sygsv3  \\n---\\n\\nThe conversation revolves around the ...   \n",
       "2   t3_16ha775  \\nThe conversation revolves around the difficu...   \n",
       "3   t3_16b9xbu  \\nThe conversation revolves around the theme o...   \n",
       "4   t3_136z3ru   The conversation titled 'Am cuts' revolves ar...   \n",
       "5    t3_zzj0lf   Summary: The individual is seeking informatio...   \n",
       "6    t3_r4f01w  \\nThe conversation revolves around the experie...   \n",
       "7   t3_1axres4  \\nThe conversation revolves around the frustra...   \n",
       "8   t3_18rrnnn  \\nThe conversation revolves around the frustra...   \n",
       "9    t3_qo85m9  \\nThe conversation revolves around the partici...   \n",
       "10  t3_17491o2  \\nThe conversation revolves around the experie...   \n",
       "11  t3_19evzih  \\nThe conversation revolves around a lost/stol...   \n",
       "12  t3_13zsm2d  \\nThe conversation revolves around a mole on C...   \n",
       "13  t3_14ngsbs  \\nThe conversation revolves around the deliver...   \n",
       "14  t3_14mdjt5   \"\"\"\\nThe conversation revolves around employe...   \n",
       "15  t3_18wvtbt  \\nThe conversation revolves around an individu...   \n",
       "16   t3_pebp8a  \\nThe conversation is between a newly hired Mo...   \n",
       "17  t3_15dbudc  \\nIn this conversation, several individuals ex...   \n",
       "18  t3_17lnbeu  \\nThe conversation revolves around the introdu...   \n",
       "19  t3_19evcoa  \\nIn this conversation, participants discuss w...   \n",
       "20   t3_s7vrdv  \\nThe conversation titled '☠☠☠' appears to be ...   \n",
       "21   t3_k2mp32  \\nThe conversation is about finding a work sch...   \n",
       "22  t3_1580byk   The conversation revolves around the incentiv...   \n",
       "23  t3_18ff4a2  \\nThe conversation revolves around the topic o...   \n",
       "24  t3_17491o2  \\nThe conversation revolves around the experie...   \n",
       "25   t3_jx0q4y  \\nIn this conversation, a new employee at Best...   \n",
       "26  t3_1382vj6  \\nThe conversation revolves around potential j...   \n",
       "27   t3_qhb9v1  \\nIn this conversation, the participants discu...   \n",
       "28  t3_198a0ow  \\nIn this conversation, participants discuss t...   \n",
       "29   t3_oo5tzv  \\nIn this conversation, the participants discu...   \n",
       "\n",
       "                           question  mistral  mixtral  \n",
       "0   Do employees feel understaffed?      1.0        1  \n",
       "1   Do employees feel understaffed?      1.0        1  \n",
       "2   Do employees feel understaffed?      1.0        1  \n",
       "3   Do employees feel understaffed?      0.0        0  \n",
       "4   Do employees feel understaffed?      1.0        1  \n",
       "5   Do employees feel understaffed?      0.0        0  \n",
       "6   Do employees feel understaffed?      1.0        1  \n",
       "7   Do employees feel understaffed?      1.0        1  \n",
       "8   Do employees feel understaffed?      1.0        1  \n",
       "9   Do employees feel understaffed?      1.0        1  \n",
       "10  Do employees feel understaffed?      1.0        1  \n",
       "11  Do employees feel understaffed?      0.0        0  \n",
       "12  Do employees feel understaffed?      0.0        0  \n",
       "13  Do employees feel understaffed?      0.0        0  \n",
       "14  Do employees feel understaffed?      0.0        0  \n",
       "15  Do employees feel understaffed?      0.0        0  \n",
       "16  Do employees feel understaffed?      0.0        0  \n",
       "17  Do employees feel understaffed?      1.0        1  \n",
       "18  Do employees feel understaffed?      0.0        0  \n",
       "19  Do employees feel understaffed?      0.0        0  \n",
       "20  Do employees feel understaffed?      0.0        0  \n",
       "21  Do employees feel understaffed?      0.0        0  \n",
       "22  Do employees feel understaffed?      0.0        0  \n",
       "23  Do employees feel understaffed?      0.0        0  \n",
       "24  Do employees feel understaffed?      1.0        1  \n",
       "25  Do employees feel understaffed?      0.0        0  \n",
       "26  Do employees feel understaffed?      0.0        0  \n",
       "27  Do employees feel understaffed?      0.0        0  \n",
       "28  Do employees feel understaffed?      0.0        0  \n",
       "29  Do employees feel understaffed?      0.0        0  "
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "8733c3fd-1387-4273-a398-4f3ec2f3e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(list(q3_final_dataset.mistral).count(1))\n",
    "print(list(q3_final_dataset.mixtral).count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a100da-aba7-4db4-bdf2-ee6c751ec551",
   "metadata": {},
   "source": [
    "This is the dataset on which both the LLMs agree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac772b-d80d-42fc-9d05-27a32bb154b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Saving all the generated test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "f278a3e1-4afd-4106-b695-5c552259a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_final_dataset.to_csv(\"q1_final_dataset.csv\")\n",
    "q2_final_dataset.to_csv(\"q2_final_dataset.csv\")\n",
    "q3_final_dataset.to_csv(\"q3_final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "bf7c5333-4b07-4f99-a604-97fbfc51a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hraj/Documents/Erdos/aware-nlp-local-copy/notebooks/HimanshuNotebooks/CreatingTestDataset\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "id": "c95aa99e-de4f-4b32-a740-b00c14334a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>context</th>\n",
       "      <th>Question 1</th>\n",
       "      <th>Question 2</th>\n",
       "      <th>Question 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_199f5h8</td>\n",
       "      <td>\\nThe conversation is about tips and advice fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_wapsy3</td>\n",
       "      <td>\\nThe conversation revolves around a potential...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_1aucdoj</td>\n",
       "      <td>\\nThe conversation revolves around a seasonal ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_yuacvb</td>\n",
       "      <td>\\nThe conversation is about getting a job at B...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_12scwfu</td>\n",
       "      <td>\\nThe conversation is about the potential cutt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reddit_id                                            context  Question 1  \\\n",
       "0  t3_199f5h8  \\nThe conversation is about tips and advice fo...           1   \n",
       "1   t3_wapsy3  \\nThe conversation revolves around a potential...           1   \n",
       "2  t3_1aucdoj  \\nThe conversation revolves around a seasonal ...           0   \n",
       "3   t3_yuacvb  \\nThe conversation is about getting a job at B...           1   \n",
       "4  t3_12scwfu  \\nThe conversation is about the potential cutt...           0   \n",
       "\n",
       "   Question 2  Question 3  \n",
       "0           1           0  \n",
       "1           1           1  \n",
       "2           0           0  \n",
       "3           0           1  \n",
       "4           0           0  "
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "d1567b31-43f9-465b-8970-bd8b8db0dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf.to_csv(\"all_questions_final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "24fa0f52-7689-4051-9f26-3ab104417d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe conversation revolves around potential job cuts in the leadership roles of a company. The roles that might be affected include ASMs, SSMs, GSMs, and Sups. It is advised to use PTO as soon as possible. There are rumors of new positions like pay a services experience manager and a services experience supervisor. The company might be shifting towards a service center model. The conversation also mentions that PTO might not be paid out in some states upon termination. The roles of Ops managers and C&D managers are also being assessed.\\n\\nKey highlights:\\n1. Potential job cuts in leadership roles.\\n2. Advised to use PTO asap.\\n3. Rumors of new positions.\\n4. Company might be shifting towards a service center model.\\n5. PTO might not be paid out in some states.\\n6. Roles of Ops managers and C&D managers are also being assessed.'"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf.iloc[86].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "60b2f60c-0867-40ff-9df5-fffac1751cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = construct_thread.ConsrtuctThread(df_subreddit , 't3_s7vrdv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "c61834a9-3d91-4201-8236-5eb5ed1dc71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aware_post_type</th>\n",
       "      <th>aware_created_ts</th>\n",
       "      <th>reddit_id</th>\n",
       "      <th>reddit_name</th>\n",
       "      <th>reddit_created_utc</th>\n",
       "      <th>reddit_author</th>\n",
       "      <th>reddit_text</th>\n",
       "      <th>reddit_permalink</th>\n",
       "      <th>reddit_title</th>\n",
       "      <th>reddit_url</th>\n",
       "      <th>reddit_subreddit</th>\n",
       "      <th>reddit_link_id</th>\n",
       "      <th>reddit_parent_id</th>\n",
       "      <th>reddit_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>submission</td>\n",
       "      <td>2022-01-19T12:44:01</td>\n",
       "      <td>s7vrdv</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>1642614241</td>\n",
       "      <td>bbythrowaway8675309</td>\n",
       "      <td></td>\n",
       "      <td>/r/BestBuyWorkers/comments/s7vrdv/_/</td>\n",
       "      <td>☠☠☠</td>\n",
       "      <td>https://i.redd.it/2w5nts6tloc81.jpg</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>comment</td>\n",
       "      <td>2022-01-19T15:10:55</td>\n",
       "      <td>htd27qo</td>\n",
       "      <td>t1_htd27qo</td>\n",
       "      <td>1642623055</td>\n",
       "      <td>MattB6x</td>\n",
       "      <td>Sounds like when we had AP and people coming i...</td>\n",
       "      <td>/r/BestBuyWorkers/comments/s7vrdv/_/htd27qo/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>s7vrdv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>comment</td>\n",
       "      <td>2022-03-09T16:13:15</td>\n",
       "      <td>i00timy</td>\n",
       "      <td>t1_i00timy</td>\n",
       "      <td>1646860395</td>\n",
       "      <td>Hefty-Market-8845</td>\n",
       "      <td>😂</td>\n",
       "      <td>/r/BestBuyWorkers/comments/s7vrdv/_/i00timy/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>s7vrdv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>comment</td>\n",
       "      <td>2022-04-18T23:03:58</td>\n",
       "      <td>i5an2fh</td>\n",
       "      <td>t1_i5an2fh</td>\n",
       "      <td>1650337438</td>\n",
       "      <td>ksuhistory</td>\n",
       "      <td>isnt that your job?</td>\n",
       "      <td>/r/BestBuyWorkers/comments/s7vrdv/_/i5an2fh/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>BestBuyWorkers</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>t3_s7vrdv</td>\n",
       "      <td>s7vrdv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    aware_post_type     aware_created_ts reddit_id reddit_name  \\\n",
       "967      submission  2022-01-19T12:44:01    s7vrdv   t3_s7vrdv   \n",
       "968         comment  2022-01-19T15:10:55   htd27qo  t1_htd27qo   \n",
       "969         comment  2022-03-09T16:13:15   i00timy  t1_i00timy   \n",
       "970         comment  2022-04-18T23:03:58   i5an2fh  t1_i5an2fh   \n",
       "\n",
       "     reddit_created_utc        reddit_author  \\\n",
       "967          1642614241  bbythrowaway8675309   \n",
       "968          1642623055              MattB6x   \n",
       "969          1646860395    Hefty-Market-8845   \n",
       "970          1650337438           ksuhistory   \n",
       "\n",
       "                                           reddit_text  \\\n",
       "967                                                      \n",
       "968  Sounds like when we had AP and people coming i...   \n",
       "969                                                  😂   \n",
       "970                                isnt that your job?   \n",
       "\n",
       "                                 reddit_permalink reddit_title  \\\n",
       "967          /r/BestBuyWorkers/comments/s7vrdv/_/          ☠☠☠   \n",
       "968  /r/BestBuyWorkers/comments/s7vrdv/_/htd27qo/         None   \n",
       "969  /r/BestBuyWorkers/comments/s7vrdv/_/i00timy/         None   \n",
       "970  /r/BestBuyWorkers/comments/s7vrdv/_/i5an2fh/         None   \n",
       "\n",
       "                              reddit_url reddit_subreddit reddit_link_id  \\\n",
       "967  https://i.redd.it/2w5nts6tloc81.jpg   BestBuyWorkers           None   \n",
       "968                                 None   BestBuyWorkers      t3_s7vrdv   \n",
       "969                                 None   BestBuyWorkers      t3_s7vrdv   \n",
       "970                                 None   BestBuyWorkers      t3_s7vrdv   \n",
       "\n",
       "    reddit_parent_id reddit_submission  \n",
       "967             None              None  \n",
       "968        t3_s7vrdv            s7vrdv  \n",
       "969        t3_s7vrdv            s7vrdv  \n",
       "970        t3_s7vrdv            s7vrdv  "
      ]
     },
     "execution_count": 958,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread.get_thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b345902-3ad4-40e5-863d-90a8e3efdcbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Building a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "dbd02b89-7e99-4967-af0e-ee53bbf81c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "0410ec4c-860f-4d4d-84ab-f0a58174626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(q3_final_dataset, page_content_column='context' )\n",
    "q3_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "a79c4cae-743a-4fe3-a506-1de71e5a9e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\nThe conversation revolves around the frustration of an individual who feels they are being asked to perform tasks outside of their job description, specifically selling products and pushing memberships in their micro market. They express their desire to focus on their job and go home, and criticize the lack of support from sales advisors in helping with tasks such as shuttle twice a week. The conversation also touches on the upcoming change of vendors bringing in their own staff to sell directly in pilot markets, and the expectation of this by some participants. There is also mention of the increased workload and expectations placed on everyone in the store, including product flow, front lanes, and even those in leadership roles. Some participants express their disagreement with these practices and suggest hiring more sales advisors and showing data to management to prove a point.', metadata={'reddit_id': 't3_1axres4', 'question': 'Do employees feel understaffed?', 'mistral': 1.0, 'mixtral': 1})"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_documents[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "193d24bf-8e99-49a9-b4bc-003c7b9031f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "54568516-7387-4958-8eec-da03f8d6a249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q3_documents_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "226db6f8-41d3-4181-91a2-bac313f261cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=100)\n",
    "\n",
    "q3_documents_splitted = text_splitter.split_documents(q3_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "e088946a-6162-4737-88cd-3be438610d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "20fe7f34-a43f-4a0c-b748-fd94de1b766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "6f85c4df-43ae-4707-a8e4-2a336f8c434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "b6bc3d20-ee26-4763-afda-1e7a03f571ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.dict of OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x15d11c390>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x15cf30550>, model='text-embedding-3-large', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)>"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10a147-8154-4a9d-b9f8-24a6c475cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbf051-5c90-48fe-aeaa-265b19f2a771",
   "metadata": {},
   "source": [
    "## ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639c2bf-7d99-4902-8d14-1cc31f363b42",
   "metadata": {},
   "source": [
    "### initial attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "45134667-4934-47c5-b696-fe78b4e39ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=q3_documents_splitted, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "4e1d78aa-5089-4c99-acd2-9d752cd4d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "del retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "1c0aed44-2609-4af0-854c-0992660874d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrive_documents = retriever.invoke(questions[2])\n",
    "retrived_instances = list(set([docs.metadata['reddit_id'] for docs in retrive_documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "c30e6212-e402-46dd-a66c-67e88489084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_relevant = list(q3_final_dataset[q3_final_dataset.mixtral==1].reddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "eeecbeea-1115-4100-bd0d-005127c2ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_relevant = list(q3_final_dataset[q3_final_dataset.mistral==1].reddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "df90afc9-4904-48b5-874a-b4ce2362c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.8888888888888888\n",
      "recall :  0.7272727272727273\n",
      "f1 :  0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "recall = len([item for item in retrived_instances if item in mistral_relevant])/len(mistral_relevant)\n",
    "prec = len([item for item in retrived_instances if item in mistral_relevant])/len(retrived_instances)\n",
    "f1_score = 2*prec*recall/(prec+recall)\n",
    "\n",
    "print('prec : ', prec)\n",
    "print('recall : ', recall)\n",
    "print('f1 : ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e501d95-7ad0-403c-9530-500102aa9f8a",
   "metadata": {},
   "source": [
    "When the number of retrevial (that we need to set) is high then precision will obviously be low. So precision is not very meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb9734-7b4e-4d02-9623-c40801283f0a",
   "metadata": {},
   "source": [
    "### More systematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "3803b705-f654-41cc-8ba1-881c91a215ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(q1_final_dataset, page_content_column='context')\n",
    "q1_documents = loader.load()\n",
    "q1_documents_splitted = text_splitter.split_documents(q1_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "0fc980b0-1641-4327-85c8-35c4bd6992b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(q2_final_dataset, page_content_column='context')\n",
    "q2_documents = loader.load()\n",
    "q2_documents_splitted = text_splitter.split_documents(q2_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "25418922-627f-4104-91d2-b82f75fc9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_vectorstore_chroma = Chroma.from_documents(documents=q1_documents_splitted, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295e2d4-6977-4c48-9b24-f4f8305fd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "# embeddings3 = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "814503ca-2feb-4b7a-82b7-02f968441d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidDimensionException",
     "evalue": "Embedding dimension 768 does not match collection dimensionality 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[802], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m q1_vectorstore_chroma_2 \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq3_documents_splitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    735\u001b[0m     ):\n\u001b[0;32m--> 736\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:297\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_with_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:487\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/segment.py:465\u001b[0m, in \u001b[0;36mSegmentAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    455\u001b[0m records_to_submit \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m _records(\n\u001b[1;32m    457\u001b[0m     t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mUPSERT,\n\u001b[1;32m    458\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    464\u001b[0m ):\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     records_to_submit\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_producer\u001b[38;5;241m.\u001b[39msubmit_embeddings(coll[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m], records_to_submit)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/segment.py:803\u001b[0m, in \u001b[0;36mSegmentAPI._validate_embedding_record\u001b[0;34m(self, collection, record)\u001b[0m\n\u001b[1;32m    801\u001b[0m add_attributes_to_current_span({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])})\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 803\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/segment.py:818\u001b[0m, in \u001b[0;36mSegmentAPI._validate_dimension\u001b[0;34m(self, collection, dim, update)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_cache[\u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidDimensionException(\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match collection dimensionality \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mInvalidDimensionException\u001b[0m: Embedding dimension 768 does not match collection dimensionality 3072"
     ]
    }
   ],
   "source": [
    "q1_vectorstore_chroma_2 = Chroma.from_documents(documents=q3_documents_splitted, embedding=embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "id": "be11f325-7c7a-40cb-99dc-c021c24120aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.dict of HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-mpnet-base-v1', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)>"
      ]
     },
     "execution_count": 1251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f5ae0-6794-4e00-8c38-5c26bda13c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_vectorstore_chroma_1 = Chroma.from_documents(documents=q1_documents_splitted, embedding=embeddings1)\n",
    "q1_vectorstore_chroma_2 = Chroma.from_documents(documents=q1_documents_splitted, embedding=embeddings2)\n",
    "\n",
    "q2_vectorstore_chroma = Chroma.from_documents(documents=q2_documents_splitted, embedding=embeddings)\n",
    "q2_vectorstore_chroma_1 = Chroma.from_documents(documents=q2_documents_splitted, embedding=embeddings1)\n",
    "q2_vectorstore_chroma_2 = Chroma.from_documents(documents=q2_documents_splitted, embedding=embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7934b4-e61a-4e9e-84b0-41b27271cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore_chroma.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d59c89-6996-4f1a-803c-b82180e09144",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "257a36d8-1e35-4dc0-adcb-7321ed52b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents=q3_documents_splitted, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "dcb8e4d4-5030-4b09-99df-22ba176ab026",
   "metadata": {},
   "outputs": [],
   "source": [
    "del retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "0245adec-49b6-4502-8e76-19ddb8bd394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrive_documents = retriever.invoke(questions[2])\n",
    "retrived_instances = list(set([docs.metadata['reddit_id'] for docs in retrive_documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "50bdc75e-6f7c-4f22-84d0-2181df0f076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_relevant = list(q3_final_dataset[q3_final_dataset.mixtral==1].reddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "2b66e923-c0d7-4dd2-83b9-27859cc3fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_relevant = list(q3_final_dataset[q3_final_dataset.mistral==1].reddit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "a2bc68bd-556a-4f0a-a749-d041343cdcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.8888888888888888\n",
      "recall :  0.7272727272727273\n",
      "f1 :  0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "recall = len([item for item in retrived_instances if item in mistral_relevant])/len(mistral_relevant)\n",
    "prec = len([item for item in retrived_instances if item in mistral_relevant])/len(retrived_instances)\n",
    "f1_score = 2*prec*recall/(prec+recall)\n",
    "\n",
    "print('prec : ', prec)\n",
    "print('recall : ', recall)\n",
    "print('f1 : ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "c953587d-8eaf-473d-a440-241c5a9e3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_vectorstore_faiss = FAISS.from_documents(documents=q1_documents_splitted, embedding=embeddings)\n",
    "q1_vectorstore_faiss_1 = FAISS.from_documents(documents=q1_documents_splitted, embedding=embeddings1)\n",
    "q1_vectorstore_faiss_2 = FAISS.from_documents(documents=q1_documents_splitted, embedding=embeddings2)\n",
    "\n",
    "q2_vectorstore_faiss = FAISS.from_documents(documents=q2_documents_splitted, embedding=embeddings)\n",
    "q2_vectorstore_faiss_1 = FAISS.from_documents(documents=q2_documents_splitted, embedding=embeddings1)\n",
    "q2_vectorstore_faiss_2 = FAISS.from_documents(documents=q2_documents_splitted, embedding=embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "41089e30-fc0b-4be6-8a45-45702cc5be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = q1_vectorstore_faiss.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "abc74690-b3eb-4360-9fa3-c1966626ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.875   recall :  0.6363636363636364  f1_score :  0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "retrive_documents = retriever.invoke(questions[0])\n",
    "evaluate(retrive_documents, q1_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "30198ce4-442a-4a98-840d-a909fbbf242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.875   recall :  0.7777777777777778  f1_score :  0.823529411764706\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q1_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "1ee74e3b-890d-4164-9b8a-354bb001bc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.625   recall :  0.45454545454545453  f1_score :  0.5263157894736842\n"
     ]
    }
   ],
   "source": [
    "retriever = q1_vectorstore_faiss_1.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrive_documents = retriever.invoke(questions[0])\n",
    "evaluate(retrive_documents, q1_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "e7e70a69-6b0c-49e3-9ed6-664733132ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.5   recall :  0.4444444444444444  f1_score :  0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q1_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "eb481d5b-2983-41e9-b0ef-41970f5478ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.5555555555555556   recall :  0.45454545454545453  f1_score :  0.5\n"
     ]
    }
   ],
   "source": [
    "retriever = q1_vectorstore_faiss_2.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrive_documents = retriever.invoke(questions[0])\n",
    "evaluate(retrive_documents, q1_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "f40e5f4c-0c09-41b0-9e0a-1c636e333ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.6666666666666666   recall :  0.6666666666666666  f1_score :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q1_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "1fc6f35c-4660-44db-b860-7b2edfd6db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  1.0   recall :  0.5  f1_score :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "retriever = q2_vectorstore_faiss.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrive_documents = retriever.invoke(questions[1])\n",
    "evaluate(retrive_documents, q2_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "243a275d-c9a8-453b-8a45-bb3d817c0589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.2857142857142857   recall :  0.4  f1_score :  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q2_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "1dcae0d6-268a-46a3-a68f-a14f0d8cada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.75   recall :  0.42857142857142855  f1_score :  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "retriever = q2_vectorstore_faiss_1.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrive_documents = retriever.invoke(questions[1])\n",
    "evaluate(retrive_documents, q2_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "06cfbda9-1b97-47da-93bf-1a177a7b83c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.125   recall :  0.2  f1_score :  0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q2_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "04ef90a9-87c3-4fae-bc5d-e2d36460dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  1.0   recall :  0.42857142857142855  f1_score :  0.6\n"
     ]
    }
   ],
   "source": [
    "retriever = q2_vectorstore_faiss_2.as_retriever(search_kwargs={\"k\": 10})\n",
    "retrive_documents = retriever.invoke(questions[1])\n",
    "evaluate(retrive_documents, q2_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "e1e2242b-237b-491f-8e0f-47025810c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  1.0   recall :  0.42857142857142855  f1_score :  0.6\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q2_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d8d32-b5c1-4d90-bd4a-9a8319aba36f",
   "metadata": {},
   "source": [
    "### Trying with a relatively poor embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "2d46cd6e-da34-4763-89d5-067d7d403895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/bert-base-nli-mean-tokens')\n",
    "embeddings1 = HuggingFaceEmbeddings(model_name='all-mpnet-base-v1')\n",
    "embeddings2 = HuggingFaceEmbeddings(model_name='all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "bf748091-4f15-490d-af08-797c47b7ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_FAISS_1 = FAISS.from_documents(documents=q3_documents_splitted, embedding=embeddings1)\n",
    "vectorstore_FAISS_2 = FAISS.from_documents(documents=q3_documents_splitted, embedding=embeddings2)\n",
    "retriever_1 = vectorstore_FAISS_1.as_retriever(search_kwargs={\"k\": 15})\n",
    "retriever_2 = vectorstore_FAISS_2.as_retriever(search_kwargs={\"k\": 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "ebd24463-fb2f-4817-8a2e-22fd6dadb208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.6923076923076923   recall :  0.8181818181818182  f1_score :  0.7500000000000001\n"
     ]
    }
   ],
   "source": [
    "retrive_documents = retriever_1.invoke(questions[2])\n",
    "retrived_instances = list(set([docs.metadata['reddit_id'] for docs in retrive_documents]))\n",
    "evaluate(retrive_documents, q3_final_dataset, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "d576489f-2566-4a84-9cee-e93beca96e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec :  0.6923076923076923   recall :  0.8181818181818182  f1_score :  0.7500000000000001\n"
     ]
    }
   ],
   "source": [
    "evaluate(retrive_documents, q3_final_dataset, 'mixtral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "id": "f5521023-4162-4f74-8d76-ad18cca1dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method as_retriever in module langchain_core.vectorstores:\n",
      "\n",
      "as_retriever(**kwargs: 'Any') -> 'VectorStoreRetriever' method of langchain_community.vectorstores.faiss.FAISS instance\n",
      "    Return VectorStoreRetriever initialized from this VectorStore.\n",
      "    \n",
      "    Args:\n",
      "        search_type (Optional[str]): Defines the type of search that\n",
      "            the Retriever should perform.\n",
      "            Can be \"similarity\" (default), \"mmr\", or\n",
      "            \"similarity_score_threshold\".\n",
      "        search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      "            search function. Can include things like:\n",
      "                k: Amount of documents to return (Default: 4)\n",
      "                score_threshold: Minimum relevance threshold\n",
      "                    for similarity_score_threshold\n",
      "                fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      "                lambda_mult: Diversity of results returned by MMR;\n",
      "                    1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      "                filter: Filter by document metadata\n",
      "    \n",
      "    Returns:\n",
      "        VectorStoreRetriever: Retriever class for VectorStore.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "        # Retrieve more documents with higher diversity\n",
      "        # Useful if your dataset has many similar documents\n",
      "        docsearch.as_retriever(\n",
      "            search_type=\"mmr\",\n",
      "            search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      "        )\n",
      "    \n",
      "        # Fetch more documents for the MMR algorithm to consider\n",
      "        # But only return the top 5\n",
      "        docsearch.as_retriever(\n",
      "            search_type=\"mmr\",\n",
      "            search_kwargs={'k': 5, 'fetch_k': 50}\n",
      "        )\n",
      "    \n",
      "        # Only retrieve documents that have a relevance score\n",
      "        # Above a certain threshold\n",
      "        docsearch.as_retriever(\n",
      "            search_type=\"similarity_score_threshold\",\n",
      "            search_kwargs={'score_threshold': 0.8}\n",
      "        )\n",
      "    \n",
      "        # Only get the single most similar document from the dataset\n",
      "        docsearch.as_retriever(search_kwargs={'k': 1})\n",
      "    \n",
      "        # Use a filter to only retrieve documents from a specific paper\n",
      "        docsearch.as_retriever(\n",
      "            search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectorstore_FAISS_1.as_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25c2fb-0b24-4eb9-b946-38e489880397",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RAG evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "8b1a3065-1914-4022-84be-d0ea87ae971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(retrive_docs, test_set, llm):\n",
    "    \n",
    "    relevant_docs = list(test_set[test_set[llm]==1].reddit_id) # ground truth based off a given LLM\n",
    "    retrived_instances = list(set([docs.metadata['reddit_id'] for docs in retrive_docs]))\n",
    "    \n",
    "    recall = len([item for item in retrived_instances if item in relevant_docs])/len(relevant_docs)\n",
    "    prec = len([item for item in retrived_instances if item in relevant_docs])/len(retrived_instances)\n",
    "    f1_score = 2*prec*recall/(prec+recall)\n",
    "\n",
    "    print('prec : ', prec, '  recall : ', recall, ' f1_score : ', f1_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "id": "acd9b8ad-7bc4-49c7-9863-3fa2413145e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What do Best Buy employees think of the company?',\n",
       "       'What are the most common reasons for employees to leave Best Buy?',\n",
       "       'Do employees feel understaffed?'], dtype=object)"
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333e9cf-a360-4abb-846a-d4028f4e852b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
