{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate llama2 llm with langchain Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\", temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n",
      "\n",
      "I hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke('Tell me a joke'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Class to use the invoke method to assess whether statement is relevant to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Old prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_prompt0(self, question: str, statement: str) -> str:\n",
    "        return f'''\n",
    "            The statement below is from a reddit thread for employees of the company for which a question is asked. \n",
    "            Does the statement provide information relevant to the question listed below (delimeted by ```)? \n",
    "            Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
    "            Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
    "            Reason should be a string explaining why and how the statement is or is not relevant.\n",
    "            Two examples and accompanying outputs are provided here (delimited by ~~~):\n",
    "            ~~~\n",
    "            Example 1:\n",
    "            Question: \"What team has won the most Super Bowls?\"\n",
    "            Statement: \"The New England Patriots have won the Super Bowl a record six times\"\n",
    "            Output: {{\n",
    "                \"relevant\": true,\n",
    "                \"reason\": \"The statement provides about the team that has won the most Super Bowls\"\n",
    "            }}\n",
    "            ~~~\n",
    "            Example 2:\n",
    "            Question: \"What team has won the most Super Bowls?\"\n",
    "            Statement: \"I do not find that interesting\"\n",
    "            Output: {{\n",
    "                \"relevant\": false,\n",
    "                \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
    "            }}\n",
    "            ~~~\n",
    "            ```\n",
    "            Question: {question}\n",
    "            ```\n",
    "            Statement:{statement}\n",
    "            ```\n",
    "            '''.replace('            ','')\n",
    "\n",
    "def prompt_template(question: str, statement: str) -> str:\n",
    "    return f'''\n",
    "        Your role is to label chunks of texts to determine whether they are relevant to a given question.\n",
    "        The statement provided is taken from employees of the company for which a question is asked. \n",
    "        A statement should be labeled as relevant (true) if it provides information that helps answer the question.\n",
    "        It does not need to answer the question directly, but it should provide at least something relevant to the question.\n",
    "        For example, if the question is \"What do employees have to say about promotions?\", a statement like \n",
    "        \"I have never gotten a raise\" would be labeled as relevant.\n",
    "        Does the statement provide information relevant to the question listed below (delimeted by ```)? \n",
    "        Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
    "        Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
    "        Reason should be a string explaining why and how the statement is or is not relevant.\n",
    "        An examples and accompanying output is provided here (delimited by ~~~):\n",
    "        ~~~\n",
    "        Example:\n",
    "        Question: \"What team has won the most Super Bowls?\"\n",
    "        Statement: \"I do not find that interesting\"\n",
    "        Output: {{\n",
    "            \"relevant\": false,\n",
    "            \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
    "        }}\n",
    "        ~~~\n",
    "        ```\n",
    "        Question: {question}\n",
    "        ```\n",
    "        Statement:{statement}\n",
    "        ```\n",
    "        '''.replace('        ','')\n",
    "\n",
    "def _generate_prompt(self, question: str, statement: str) -> str:\n",
    "        return f'''\n",
    "            Your role is to label chunks of text (statements) to determine whether they are relevant to a given question for a retrieval system.\n",
    "            The statement provided is taken from employees of the company for which a question is asked. \n",
    "            A statement should be labeled as relevant (true) if it provides information that helps answer the question.\n",
    "            It does not need to answer the question directly, but it should provide at least something relevant to the question.\n",
    "            For example, if the question is \"What do employees have to say about promotions?\", a statement like \n",
    "            \"I have never gotten a raise\" would be labeled as relevant.\n",
    "            Does the statement provide information relevant to the question listed below (delimeted by ```)? \n",
    "            Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
    "            Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
    "            Reason should be a string explaining why and how the statement is or is not relevant.\n",
    "            An examples and accompanying output is provided here (delimited by ~~~):\n",
    "            ~~~\n",
    "            Example:\n",
    "            Question: \"What team has won the most Super Bowls?\"\n",
    "            Statement: \"I do not find that interesting\"\n",
    "            Output: {{\n",
    "                \"relevant\": false,\n",
    "                \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
    "            }}\n",
    "            ~~~\n",
    "            ```\n",
    "            Question: {question}\n",
    "            ```\n",
    "            Statement:{statement}\n",
    "            ```\n",
    "        '''.replace('            ','')\n",
    "\n",
    "\n",
    "    def _generate_prompt(self, question: str, statement: str) -> str:\n",
    "        return f'''\n",
    "            Your role is to label chunks of text (statements) to determine whether they are relevant to a given question for a retrieval system.\n",
    "            The statement and question are listed below (delimeted by ```).\n",
    "            The statement provided is taken from employees of the company for which a question is asked. \n",
    "            A statement should be labeled as relevant (true) if it provides information that helps answer the question.\n",
    "            It does not need to answer the question directly, but it should provide at least something relevant to the question.\n",
    "            For example, if the question is \"What do employees have to say about promotions?\", a statement like \n",
    "            \"I have never gotten a raise\" would be labeled as relevant.\n",
    "            Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
    "            Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
    "            Reason should be a string explaining why and how the statement is or is not relevant.\n",
    "            An example and accompanying output is provided here (delimited by ~~~):\n",
    "            ~~~\n",
    "            Example:\n",
    "            Question: \"What team has won the most Super Bowls?\"\n",
    "            Statement: \"I do not find that interesting\"\n",
    "            Output: {{\n",
    "                \"relevant\": false,\n",
    "                \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
    "            }}\n",
    "            ~~~\n",
    "            ```\n",
    "            Question: {question}\n",
    "            ```\n",
    "            Statement:{statement}\n",
    "            ```\n",
    "        '''.replace('            ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, llm: Ollama):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def _generate_prompt(self, question: str, statement: str) -> str:\n",
    "        return f'''\n",
    "            <s>[INST] <<SYS>>\n",
    "            Your role is to label chunks of text (statements) to determine whether they are relevant to a given question for a retrieval system.\n",
    "            The statement and question are listed below (delimeted by ```).\n",
    "            The statement provided is taken from employees of the company for which a question is asked. \n",
    "            A statement should be labeled as relevant (true) if it provides information that helps answer the question.\n",
    "            It does not need to answer the question directly, but it should provide at least something relevant to the question.\n",
    "            A rewording of the question without adding any personal perspective should be considered unrelevant (false).\n",
    "            For example, if the question is \"What do employees have to say about promotions?\", a statement like \n",
    "            \"I have never gotten a raise\" would be labeled as relevant.\n",
    "            Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
    "            Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
    "            Reason should be a string explaining why and how the statement is or is not relevant.\n",
    "            An example and accompanying output is provided here (delimited by ~~~):\n",
    "            ~~~\n",
    "            Example:\n",
    "            Question: \"What team has won the most Super Bowls?\"\n",
    "            Statement: \"I do not find that interesting\"\n",
    "            Output: {{\n",
    "                \"relevant\": false,\n",
    "                \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
    "            }}\n",
    "            ~~~\n",
    "            <</SYS>>\n",
    "            ```\n",
    "            Question: {question}\n",
    "            ```\n",
    "            Statement:{statement}\n",
    "            ```\n",
    "            [/INST]\n",
    "        '''.replace('            ','')\n",
    "    \n",
    "    def _format_output(self, output: str) -> dict:\n",
    "        out_dict = {\"output\": output}\n",
    "        output = output.replace('\\n', '').replace('```','').strip()\n",
    "        output = output[output.index('{'):output.rindex('}')+1]\n",
    "        return json.loads(output) | out_dict\n",
    "\n",
    "    def _invoke_llm(self, query: str) -> str:\n",
    "        return self.llm.invoke(query)\n",
    "\n",
    "    def evaluate_one(self, question, statement) -> dict:\n",
    "        query = self._generate_prompt(question, statement).replace('\\n', '')\n",
    "        output = self._invoke_llm(query)\n",
    "        try:\n",
    "            return self._format_output(output)\n",
    "        except:\n",
    "            return {\"relevant\": None, \"reason\": \"Error parsing output\", \"output\": output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] <<SYS>>\n",
      "Your role is to label chunks of text (statements) to determine whether they are relevant to a given question for a retrieval system.\n",
      "The statement and question are listed below (delimeted by ```).\n",
      "The statement provided is taken from employees of the company for which a question is asked. \n",
      "A statement should be labeled as relevant (true) if it provides information that helps answer the question.\n",
      "It does not need to answer the question directly, but it should provide at least something relevant to the question.\n",
      "For example, if the question is \"What do employees have to say about promotions?\", a statement like \n",
      "\"I have never gotten a raise\" would be labeled as relevant.\n",
      "Please format the output as a dictionary with the following keys: \"relevant\", \"reason\". \n",
      "Relevant should be a boolean value indicating whether the statement is relevant to the question.\n",
      "Reason should be a string explaining why and how the statement is or is not relevant.\n",
      "An example and accompanying output is provided here (delimited by ~~~):\n",
      "~~~\n",
      "Example:\n",
      "Question: \"What team has won the most Super Bowls?\"\n",
      "Statement: \"I do not find that interesting\"\n",
      "Output: {\n",
      "    \"relevant\": false,\n",
      "    \"reason\": \"The statement provided does not have the needed context to help answer the question\"\n",
      "}\n",
      "~~~\n",
      "<</SYS>>\n",
      "```\n",
      "Question: What do Best Buy employees think of the company?\n",
      "```\n",
      "Statement:I hate working here. The pay is terrible and the hours are long.\n",
      "```\n",
      "[/INST]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(llm)\n",
    "question = 'What do Best Buy employees think of the company?'\n",
    "statement = 'I hate working here. The pay is terrible and the hours are long.'\n",
    "print(evaluator._generate_prompt(question, statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on two cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What do Best Buy employees think of the company?\n",
      "\n",
      "Text 1: I hate working here. The pay is terrible and the hours are long.\n",
      "Output: {'relevant': True, 'reason': 'The statement provides some negative information about the company, which could be relevant to an inquiry into employee satisfaction or work environment.', 'output': '\\n{\\n\"relevant\": true,\\n\"reason\": \"The statement provides some negative information about the company, which could be relevant to an inquiry into employee satisfaction or work environment.\"\\n}'}\n",
      "\n",
      "Text 2: Why did you do that?\n",
      "Output: {'relevant': False, 'reason': 'The statement provided does not relate to Best Buy or its employees, and therefore is not relevant to the question.', 'output': '\\n{\\n\"relevant\": false,\\n\"reason\": \"The statement provided does not relate to Best Buy or its employees, and therefore is not relevant to the question.\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(llm)\n",
    "question = 'What do Best Buy employees think of the company?'\n",
    "\n",
    "context1 = 'I hate working here. The pay is terrible and the hours are long.'\n",
    "out1 = evaluator.evaluate_one(question, context1)\n",
    "\n",
    "context2 = 'Why did you do that?'\n",
    "out2 = evaluator.evaluate_one(question, context2)\n",
    "print(f'Question: {question}\\n')\n",
    "print(f'Text 1: {context1}\\nOutput: {out1}')\n",
    "print(f'\\nText 2: {context2}\\nOutput: {out2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>The statement provides some negative informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>The statement provided does not relate to Best...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevant                                             reason\n",
       "0      True  The statement provides some negative informati...\n",
       "1     False  The statement provided does not relate to Best..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([out1, out2])[['relevant','reason']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test reproducibility\n",
    "1. With low temperature, we expect near identical outputs\n",
    "2. With high temperature variance in the output is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': True, 'reason': 'The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.', 'output': '\\n{\\n\"relevant\": true,\\n\"reason\": \"The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.\"\\n}'}\n",
      "{'relevant': True, 'reason': 'The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.', 'output': '\\n{\\n\"relevant\": true,\\n\"reason\": \"The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.\"\\n}'}\n",
      "{'relevant': True, 'reason': 'The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.', 'output': '\\n{\\n\"relevant\": true,\\n\"reason\": \"The statement provides some negative information about the company, which could be relevant to an inquiry regarding employee opinions of Best Buy.\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(llm)\n",
    "question = 'What do Best Buy employees think of the company?'\n",
    "statement = 'I hate working here. The pay is terrible and the hours are long.'\n",
    "\n",
    "outputs = []\n",
    "for i in range(3):\n",
    "    out = evaluator.evaluate_one(question, statement)\n",
    "    outputs.append(out)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on samples of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.read_csv('../../data/best_buy/sample_questions_statements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do Best Buy employees think fo the company?</td>\n",
       "      <td>What is your opinion on Best Buy currently.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do Best Buy employees think fo the company?</td>\n",
       "      <td>Why are you at Best Buy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do Best Buy employees think fo the company?</td>\n",
       "      <td>Don’t listen to this guy, I work there and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do Best Buy employees think fo the company?</td>\n",
       "      <td>Being a veteran and Best Buy employee, I can s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do Best Buy employees think fo the company?</td>\n",
       "      <td>Life as a Best Buy worker 💀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>Still insane to me they don’t do an annual 75%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>Hope you all can help - being denied refund; H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>New hire that still needs to receive paperwork...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>Love this!!!  My team was the same although si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Do employees feel understaffed?</td>\n",
       "      <td>Always remember, HR is here to protect the com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0    What do Best Buy employees think fo the company?   \n",
       "1    What do Best Buy employees think fo the company?   \n",
       "2    What do Best Buy employees think fo the company?   \n",
       "3    What do Best Buy employees think fo the company?   \n",
       "4    What do Best Buy employees think fo the company?   \n",
       "..                                                ...   \n",
       "114                   Do employees feel understaffed?   \n",
       "115                   Do employees feel understaffed?   \n",
       "116                   Do employees feel understaffed?   \n",
       "117                   Do employees feel understaffed?   \n",
       "118                   Do employees feel understaffed?   \n",
       "\n",
       "                                             statement  \n",
       "0          What is your opinion on Best Buy currently.  \n",
       "1                             Why are you at Best Buy?  \n",
       "2    Don’t listen to this guy, I work there and the...  \n",
       "3    Being a veteran and Best Buy employee, I can s...  \n",
       "4                          Life as a Best Buy worker 💀  \n",
       "..                                                 ...  \n",
       "114  Still insane to me they don’t do an annual 75%...  \n",
       "115  Hope you all can help - being denied refund; H...  \n",
       "116  New hire that still needs to receive paperwork...  \n",
       "117  Love this!!!  My team was the same although si...  \n",
       "118  Always remember, HR is here to protect the com...  \n",
       "\n",
       "[119 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(llm)\n",
    "sample_data = []\n",
    "for i, row in samples_df.iterrows():\n",
    "    question = row['question']\n",
    "    statement = row['statement']\n",
    "    out = evaluator.evaluate_one(question, statement)\n",
    "    sample_data.append((out | row.to_dict()))\n",
    "    print(f'\\nQuestion: {question}')\n",
    "    print(f'Statement: {statement}')\n",
    "    print(f'Output: {out}')\n",
    "    pd.DataFrame(sample_data).to_csv('../../data/best_buy/labelled_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ragas evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas.metrics import context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [03:03<00:00, 91.95s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on January 15, 1967</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who won the most super bowls?</td>\n",
       "      <td>The most super bowls have been won by The New ...</td>\n",
       "      <td>[The Green Bay Packers...Green Bay, Wisconsin....</td>\n",
       "      <td>The New England Patriots have won the Super Bo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0  When was the first super bowl?   \n",
       "1   Who won the most super bowls?   \n",
       "\n",
       "                                              answer  \\\n",
       "0       The first superbowl was held on Jan 15, 1967   \n",
       "1  The most super bowls have been won by The New ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "1  [The Green Bay Packers...Green Bay, Wisconsin....   \n",
       "\n",
       "                                        ground_truth  context_precision  \n",
       "0   The first superbowl was held on January 15, 1967                1.0  \n",
       "1  The New England Patriots have won the Super Bo...                0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = evaluate(dataset,metrics=[context_precision], llm=llm)\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
